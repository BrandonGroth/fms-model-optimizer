01/31/2025 03:23:51 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 03:24:38 - INFO - __main__ - Sample 35347 of the training set: {'input_ids': [101, 2054, 2948, 2640, 3444, 24840, 4888, 1998, 7233, 5402, 23951, 12322, 4014, 3723, 1029, 102, 2144, 1996, 2220, 4856, 2006, 7511, 11363, 2009, 2038, 2042, 1996, 3218, 2000, 8980, 2948, 2012, 2019, 6466, 2000, 3417, 1997, 1996, 26819, 2240, 1997, 1996, 2911, 1012, 1996, 3078, 3853, 1997, 2023, 18756, 5877, 2003, 2000, 3499, 2948, 2008, 3335, 1996, 28427, 14666, 1010, 3615, 2000, 2004, 1037, 10053, 2121, 1010, 2000, 2468, 10519, 2153, 2302, 1996, 3891, 1997, 7294, 2948, 9083, 2830, 1012, 1996, 18756, 5877, 4473, 1996, 8272, 1997, 2028, 2030, 2048, 1000, 5808, 1000, 4937, 9331, 11314, 2015, 1999, 2804, 2000, 1996, 2048, 6812, 8870, 1012, 2019, 18756, 5877, 2036, 24840, 4888, 1998, 7233, 5402, 16991, 2007, 1996, 5724, 1997, 17424, 12106, 1998, 7233, 1997, 2948, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 106, 'end_positions': 108}.
01/31/2025 03:24:38 - INFO - __main__ - Sample 50325 of the training set: {'input_ids': [101, 2073, 2001, 16732, 2160, 2008, 3848, 2985, 4234, 2012, 2284, 1029, 102, 2206, 1037, 7661, 2016, 5224, 2802, 2014, 7794, 9021, 1010, 3848, 2985, 1996, 4234, 1997, 5141, 2012, 16732, 2160, 2006, 1996, 8842, 1997, 20945, 1012, 1054, 5369, 12248, 17456, 1999, 2014, 3456, 2018, 10155, 2014, 20342, 1010, 1998, 2014, 2159, 18743, 2001, 26761, 2011, 4937, 5400, 16649, 1012, 2083, 2220, 2254, 1010, 2016, 2371, 1000, 5410, 1998, 4895, 4381, 1000, 1010, 1998, 2011, 3054, 1011, 2254, 2016, 2001, 1000, 2852, 15568, 2100, 1012, 1012, 1012, 19720, 1010, 1031, 1998, 1033, 5457, 1000, 1012, 2016, 2351, 2006, 9857, 1010, 2570, 2254, 5775, 1010, 2012, 2431, 2627, 2416, 1999, 1996, 3944, 1010, 2012, 1996, 2287, 1997, 6282, 1012, 2014, 2365, 1998, 6332, 2332, 3487, 8890, 1010, 1998, 2014, 7310, 7631, 1010, 3750, 9070, 2462, 1997, 2762, 1010, 2020, 2012, 2014, 2331, 8270, 1012, 2014, 8837, 9004, 17717, 1010, 10722, 18752, 1010, 2001, 4201, 2588, 2014, 2331, 8270, 2004, 1037, 2197, 5227, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 32, 'end_positions': 36}.
01/31/2025 03:24:38 - INFO - __main__ - Sample 43649 of the training set: {'input_ids': [101, 2054, 2003, 1037, 5410, 3120, 1997, 8249, 2008, 2003, 8534, 2011, 22595, 14247, 1029, 102, 22595, 14247, 2003, 2036, 2109, 2004, 1037, 25553, 3430, 1999, 2070, 16143, 2109, 2000, 3573, 1998, 3665, 17669, 4475, 1012, 2096, 1996, 3384, 2993, 2003, 17669, 1010, 2049, 2152, 4304, 3084, 2009, 2062, 4621, 2084, 2599, 1999, 9190, 2075, 8249, 2013, 2844, 4216, 2107, 2004, 10958, 12811, 1012, 2060, 3594, 1997, 22595, 14247, 2421, 4675, 11179, 2015, 2005, 2948, 2491, 9972, 1010, 2004, 28030, 2005, 7421, 2128, 1011, 4443, 4683, 1998, 2004, 1037, 25553, 3430, 1012, 2349, 2000, 2049, 2152, 4304, 1010, 2023, 3430, 2003, 2179, 1999, 1999, 8743, 4818, 8606, 3001, 1998, 1999, 1043, 12541, 2891, 26461, 16681, 2229, 1012, 22595, 14247, 2003, 6871, 2058, 6660, 9742, 11970, 2349, 2000, 2049, 3754, 2000, 2022, 4089, 3698, 2094, 1998, 3459, 2004, 2092, 2004, 2049, 4659, 2659, 3465, 1012, 1996, 2364, 3891, 1997, 7524, 2000, 22595, 14247, 2003, 5072, 16149, 2011, 14247, 15772, 2738, 2084, 2557, 18908, 7730, 1006, 14247, 2108, 2069, 1037, 5410, 6541, 12495, 12079, 1007, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 03:24:40 - INFO - __main__ - ***** Running training *****
01/31/2025 03:24:40 - INFO - __main__ - Num examples = 131754
01/31/2025 03:24:40 - INFO - __main__ - Num Epochs = 3
01/31/2025 03:24:40 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 03:24:40 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 03:24:40 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 03:24:40 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 03:24:41 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 03:24:41 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 03:25:44 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 03:27:00 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 03:27:04 - INFO - __main__ - Sample 77840 of the training set: {'input_ids': [101, 2054, 3303, 1037, 8536, 1999, 1996, 2537, 1997, 2152, 3737, 2636, 1029, 102, 2927, 1999, 3912, 1010, 2852, 1012, 2848, 2751, 10665, 1998, 2010, 3095, 2012, 3996, 2636, 1998, 2012, 6568, 12030, 12543, 4073, 2000, 4769, 3471, 1997, 3405, 1998, 2652, 2067, 4867, 25880, 1998, 4975, 2019, 23766, 1010, 10539, 7325, 18245, 2291, 1012, 2009, 2165, 2055, 2809, 2086, 1997, 2817, 1010, 3272, 2043, 2009, 2001, 6731, 2138, 1997, 2088, 2162, 2462, 1012, 2633, 1010, 1996, 2260, 1011, 4960, 1006, 2382, 4642, 1007, 2146, 2377, 1006, 6948, 1007, 3943, 1015, 30070, 2509, 11575, 12702, 16523, 9541, 3726, 2501, 2201, 2001, 3107, 2011, 1996, 3996, 2501, 2194, 2012, 1037, 2047, 2259, 2811, 3034, 2006, 2238, 2324, 1010, 3882, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 70, 'end_positions': 72}.
01/31/2025 03:27:04 - INFO - __main__ - Sample 90292 of the training set: {'input_ids': [101, 2129, 2172, 6451, 2003, 10964, 2011, 10692, 6604, 1029, 102, 1999, 2289, 1010, 2174, 1010, 1037, 2312, 2783, 4070, 15074, 1998, 4803, 14200, 2404, 3778, 2006, 10692, 1005, 1055, 9598, 1010, 2029, 2001, 25039, 5999, 2000, 1996, 9944, 1010, 20655, 1996, 2342, 2005, 3930, 1999, 9167, 1011, 11717, 6088, 1012, 10692, 14338, 3701, 10394, 1998, 3941, 1010, 3536, 1998, 3259, 1010, 18762, 1010, 2833, 3688, 1010, 7390, 1010, 1998, 11970, 1998, 5072, 3688, 1012, 10692, 2036, 14338, 1015, 1012, 5179, 2475, 4551, 11382, 8261, 19321, 2847, 1997, 6451, 6604, 1012, 2012, 1996, 2168, 2051, 10692, 17589, 10394, 1998, 3941, 1010, 5072, 3688, 1010, 18762, 1010, 2833, 3688, 1998, 5193, 3941, 1012, 10692, 17589, 3263, 2454, 11382, 8261, 19321, 2847, 1997, 6451, 6604, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 114, 'end_positions': 119}.
01/31/2025 03:27:04 - INFO - __main__ - Sample 50665 of the training set: {'input_ids': [101, 2054, 2024, 2087, 1997, 1996, 7475, 8163, 2011, 3424, 1011, 21152, 7486, 8857, 2006, 1029, 102, 2116, 5499, 3424, 1011, 21152, 9918, 2024, 4876, 5079, 2000, 2119, 3424, 3366, 22930, 2964, 1998, 3424, 1011, 19999, 2964, 1010, 2295, 2060, 19628, 2024, 2081, 2107, 2004, 11383, 2489, 9335, 2239, 2854, 2000, 2632, 1011, 16137, 19190, 4748, 1011, 4830, 3501, 25787, 1006, 1996, 6270, 22112, 1007, 1012, 2070, 5152, 3424, 1011, 6701, 2015, 7475, 2008, 2489, 9335, 2239, 2854, 14067, 1996, 5426, 1997, 1996, 5181, 2105, 1996, 2088, 1998, 2008, 2028, 1997, 2049, 8704, 2003, 2000, 6033, 1996, 2632, 1011, 1037, 4160, 3736, 8806, 1999, 2344, 2000, 14591, 1996, 3379, 1997, 9168, 1999, 6744, 1012, 1999, 3720, 2654, 1997, 2049, 16077, 1010, 22129, 2163, 2008, 2489, 9335, 2239, 2854, 1010, 16933, 1010, 1998, 2060, 2714, 2967, 1000, 2147, 1999, 1996, 3037, 1997, 19999, 2964, 1998, 2429, 2000, 2049, 8128, 1012, 1012, 1012, 1000, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 28, 'end_positions': 36}.
01/31/2025 03:27:05 - INFO - __main__ - ***** Running training *****
01/31/2025 03:27:05 - INFO - __main__ - Num examples = 131754
01/31/2025 03:27:05 - INFO - __main__ - Num Epochs = 3
01/31/2025 03:27:05 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 03:27:05 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 03:27:05 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 03:27:05 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 03:27:07 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 03:27:07 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 03:27:07 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 03:27:25 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 03:27:25 - INFO - __main__ - Num examples = 12134
01/31/2025 03:27:25 - INFO - __main__ - Batch size = 8
01/31/2025 03:28:47 - INFO - __main__ - Evaluation metrics: {'exact': 69.42642971447823, 'f1': 72.98527966482256, 'total': 11873, 'HasAns_exact': 63.02294197031039, 'HasAns_f1': 70.15084774973616, 'HasAns_total': 5928, 'NoAns_exact': 75.81160639192599, 'NoAns_f1': 75.81160639192599, 'NoAns_total': 5945, 'best_exact': 69.42642971447823, 'best_exact_thresh': 0.0, 'best_f1': 72.98527966482251, 'best_f1_thresh': 0.0}
01/31/2025 03:41:49 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 03:41:53 - INFO - __main__ - Sample 110370 of the training set: {'input_ids': [101, 2054, 2003, 1996, 3988, 11588, 2303, 3255, 8742, 5281, 2011, 2111, 2007, 16492, 11601, 4053, 1029, 102, 11498, 10814, 10440, 1010, 1996, 3279, 1997, 8742, 1998, 10758, 5013, 2491, 2044, 3809, 16492, 11601, 4053, 1010, 2089, 2022, 5642, 2011, 21025, 4103, 2571, 3255, 2012, 1996, 2504, 1997, 1996, 16492, 11601, 4053, 1010, 25292, 19357, 2140, 3255, 23408, 23461, 2011, 1037, 8110, 24176, 2030, 6812, 2884, 1010, 2030, 1010, 1999, 2274, 2000, 2702, 2566, 9358, 1997, 11498, 10814, 12863, 2015, 1010, 11588, 2303, 3255, 1999, 2752, 1997, 3143, 16792, 3279, 1012, 2023, 11588, 2303, 3255, 2003, 3322, 2649, 2004, 5255, 2030, 23690, 2021, 2089, 19852, 2046, 5729, 14527, 2030, 18392, 2075, 3255, 1010, 2030, 1996, 8742, 1997, 2543, 2770, 2091, 1996, 3456, 2030, 1997, 1037, 5442, 12814, 1999, 1996, 5771, 1012, 14447, 2089, 2022, 6234, 2030, 2089, 2025, 5258, 2127, 2086, 2044, 1996, 4487, 3736, 9709, 4544, 1012, 11707, 3949, 6524, 3640, 9879, 4335, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 99, 'end_positions': 101}.
01/31/2025 03:41:53 - INFO - __main__ - Sample 21469 of the training set: {'input_ids': [101, 2054, 2134, 1005, 1056, 1996, 6408, 2393, 4762, 1029, 102, 1996, 10039, 1997, 1996, 4186, 1997, 9434, 2106, 2025, 7949, 2000, 4346, 9978, 2030, 11428, 2015, 2007, 2152, 2548, 7264, 1025, 1999, 2591, 4761, 1996, 21139, 2015, 1997, 9434, 2020, 2004, 10754, 2004, 2087, 1997, 1996, 2344, 1012, 1996, 11428, 2815, 2935, 1997, 1996, 6952, 1997, 9434, 2004, 1037, 2237, 1997, 2048, 2000, 2093, 4595, 5381, 3473, 2105, 2009, 1024, 2004, 1037, 7325, 1998, 11194, 2006, 1037, 2882, 4094, 1996, 6408, 3271, 4762, 1996, 2237, 4610, 1010, 1998, 4262, 2007, 1996, 2237, 2815, 12890, 11601, 4818, 1010, 2021, 2053, 4372, 27843, 12680, 9355, 6111, 2001, 3843, 2076, 1996, 2690, 5535, 1012, 1996, 6103, 2328, 7340, 1998, 16707, 2006, 1996, 2225, 2217, 1010, 4372, 26775, 10441, 8450, 2588, 1996, 8493, 1012, 1031, 11091, 2734, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 03:41:53 - INFO - __main__ - Sample 130696 of the training set: {'input_ids': [101, 2054, 2001, 6923, 2077, 14491, 2427, 5839, 1029, 102, 7297, 1010, 3350, 6526, 2008, 5933, 2089, 2031, 2042, 2028, 1997, 1996, 3674, 4483, 5876, 2877, 2000, 14446, 2015, 1997, 1996, 7570, 4135, 17968, 13164, 7011, 9521, 1998, 2037, 6110, 2011, 3760, 12810, 20984, 6072, 1012, 2167, 2137, 13164, 7011, 9521, 14446, 2001, 19680, 15758, 2007, 1996, 3920, 4318, 3022, 4254, 2724, 1010, 4298, 2437, 5933, 1037, 2625, 4187, 5387, 1999, 14491, 2427, 3279, 2084, 2018, 2042, 3130, 2245, 1012, 2174, 1010, 1999, 2060, 5269, 2107, 2004, 2660, 1010, 4286, 2024, 2245, 2000, 2031, 2209, 1037, 2200, 3278, 2535, 1999, 1996, 14446, 1997, 1996, 2827, 13164, 7011, 9521, 2008, 2001, 6923, 3188, 2000, 2529, 6139, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 03:41:54 - INFO - __main__ - ***** Running training *****
01/31/2025 03:41:54 - INFO - __main__ - Num examples = 131754
01/31/2025 03:41:54 - INFO - __main__ - Num Epochs = 3
01/31/2025 03:41:54 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 03:41:54 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 03:41:54 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 03:41:54 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 03:41:55 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 03:41:55 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 03:41:56 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 03:46:00 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 03:46:00 - INFO - __main__ - Num examples = 12134
01/31/2025 03:46:00 - INFO - __main__ - Batch size = 8
01/31/2025 03:47:23 - INFO - __main__ - Evaluation metrics: {'exact': 69.42642971447823, 'f1': 72.98527966482256, 'total': 11873, 'HasAns_exact': 63.02294197031039, 'HasAns_f1': 70.15084774973616, 'HasAns_total': 5928, 'NoAns_exact': 75.81160639192599, 'NoAns_f1': 75.81160639192599, 'NoAns_total': 5945, 'best_exact': 69.42642971447823, 'best_exact_thresh': 0.0, 'best_f1': 72.98527966482251, 'best_f1_thresh': 0.0}
01/31/2025 03:52:26 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 03:53:12 - INFO - __main__ - Sample 88762 of the training set: {'input_ids': [101, 2339, 2003, 2859, 1005, 1055, 2231, 2025, 2714, 2000, 8499, 3252, 1029, 102, 2859, 2003, 1996, 2922, 22127, 2110, 1999, 1996, 2088, 2011, 2119, 2313, 1998, 2455, 2181, 1012, 2348, 2859, 2038, 2018, 2146, 6993, 1997, 2430, 3627, 2005, 4693, 1010, 2009, 2003, 2411, 5275, 2008, 1996, 22127, 3252, 1997, 1996, 2822, 2231, 2003, 2521, 2205, 4895, 9148, 14273, 2100, 2000, 6464, 1998, 1041, 15549, 2696, 6321, 6133, 1996, 2406, 1005, 1055, 3821, 1012, 2006, 1996, 2060, 2192, 1010, 2822, 17934, 2024, 10027, 1997, 11519, 7941, 3989, 2004, 1037, 2433, 1997, 22965, 2964, 1998, 1037, 2067, 23835, 2005, 2120, 4487, 19729, 3012, 1025, 2145, 2500, 7475, 2008, 1996, 3014, 1997, 12645, 2445, 2000, 4992, 1011, 2504, 4584, 1999, 1996, 2111, 1005, 1055, 3072, 1997, 2859, 8310, 2000, 1037, 2139, 13743, 2976, 2964, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 03:53:12 - INFO - __main__ - Sample 21994 of the training set: {'input_ids': [101, 2043, 2001, 1996, 2715, 3690, 1029, 102, 4556, 2189, 2003, 2396, 2189, 2550, 2030, 15685, 1999, 1996, 7443, 1997, 2530, 2189, 1010, 2164, 2119, 19246, 1006, 3412, 1007, 1998, 10644, 2189, 1012, 2096, 1037, 2714, 2744, 2003, 2036, 2109, 2000, 6523, 2000, 1996, 2558, 2013, 18171, 2000, 11102, 1006, 1996, 4556, 2558, 1007, 1010, 2023, 3720, 2003, 2055, 1996, 5041, 8487, 1997, 2051, 2013, 5560, 1996, 6252, 2301, 2000, 1996, 2556, 2154, 1010, 2029, 2950, 1996, 4556, 2558, 1998, 2536, 2060, 6993, 1012, 1996, 2430, 17606, 1997, 2023, 4535, 2150, 19429, 7810, 2090, 26245, 1998, 5141, 1010, 2029, 2003, 2124, 2004, 1996, 2691, 3218, 2558, 1012, 1996, 2350, 2051, 5908, 1997, 4556, 2189, 2024, 2004, 4076, 1024, 1996, 2220, 2189, 2558, 1010, 2029, 2950, 1996, 5781, 1006, 3156, 1516, 20652, 1007, 1998, 1996, 8028, 1006, 20652, 1516, 14883, 1007, 28500, 1025, 1996, 2691, 3218, 2558, 1010, 2029, 2950, 1996, 10456, 1006, 14883, 1516, 18171, 1007, 1010, 4556, 1006, 18171, 1516, 11102, 1007, 1010, 1998, 6298, 28500, 1006, 13140, 1516, 4976, 1007, 1025, 1998, 1996, 3983, 2301, 1006, 5775, 1516, 2456, 1007, 2029, 2950, 1996, 2715, 1006, 6193, 1516, 4479, 1007, 2008, 17702, 2015, 2013, 1996, 2397, 3708, 1011, 2301, 1010, 1996, 2152, 2715, 1006, 3054, 3983, 1011, 2301, 1007, 1010, 1998, 3824, 2030, 2695, 5302, 25888, 1006, 3339, 1516, 2325, 1007, 28500, 1012, 1031, 11091, 2734, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 187, 'end_positions': 189}.
01/31/2025 03:53:12 - INFO - __main__ - Sample 112641 of the training set: {'input_ids': [101, 2129, 2116, 5908, 2106, 1996, 4239, 2223, 2707, 2007, 2044, 1996, 3975, 1029, 102, 1999, 2826, 1010, 1996, 2034, 2407, 4184, 5295, 2013, 1996, 2374, 2223, 4372, 3742, 2063, 1998, 2006, 2676, 2089, 2826, 1996, 6904, 4239, 2223, 2001, 2719, 2004, 1037, 3132, 2194, 2551, 2041, 1997, 2019, 2436, 2012, 1996, 2374, 2523, 1005, 1055, 2059, 4075, 1999, 10237, 4796, 1012, 2023, 3214, 1037, 3338, 1011, 2039, 1997, 1996, 9645, 1011, 2095, 1011, 2214, 2374, 2223, 2008, 2018, 3498, 2127, 2059, 2007, 2176, 5908, 1025, 1996, 4239, 2223, 2052, 5452, 2007, 1037, 2309, 2407, 1998, 1996, 2374, 2223, 2007, 2093, 1012, 2045, 2001, 2053, 2689, 1999, 2971, 4289, 1025, 1996, 2168, 2193, 1997, 2780, 3879, 1999, 1996, 2327, 3462, 1010, 1998, 4712, 1998, 9591, 2090, 1996, 4239, 2223, 1998, 1996, 2047, 2034, 2407, 2815, 1996, 2168, 2004, 1996, 2214, 2034, 1998, 2117, 5908, 2007, 2093, 2780, 7049, 2013, 1996, 2223, 1998, 2093, 3755, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 92, 'end_positions': 94}.
01/31/2025 03:53:14 - INFO - __main__ - ***** Running training *****
01/31/2025 03:53:14 - INFO - __main__ - Num examples = 131754
01/31/2025 03:53:14 - INFO - __main__ - Num Epochs = 3
01/31/2025 03:53:14 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 03:53:14 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 03:53:14 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 03:53:14 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 03:53:15 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 03:53:15 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 03:53:15 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 03:54:01 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 03:54:01 - INFO - __main__ - Num examples = 12134
01/31/2025 03:54:01 - INFO - __main__ - Batch size = 8
01/31/2025 03:55:24 - INFO - __main__ - Evaluation metrics: {'exact': 66.25958056093658, 'f1': 69.53020053717972, 'total': 11873, 'HasAns_exact': 49.61201079622132, 'HasAns_f1': 56.16263005700618, 'HasAns_total': 5928, 'NoAns_exact': 82.85954583683768, 'NoAns_f1': 82.85954583683768, 'NoAns_total': 5945, 'best_exact': 66.26800303208961, 'best_exact_thresh': 0.0, 'best_f1': 69.5386230083329, 'best_f1_thresh': 0.0}
01/31/2025 04:04:10 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 04:04:52 - INFO - __main__ - Sample 75999 of the training set: {'input_ids': [101, 2054, 3257, 2106, 1996, 2167, 3792, 4296, 7949, 1029, 102, 2006, 2255, 2423, 1010, 10081, 1010, 2810, 2211, 2006, 1996, 17025, 1998, 15842, 4296, 2000, 7532, 1996, 3417, 2103, 1997, 17025, 2007, 1996, 2110, 3007, 1997, 15842, 1012, 1999, 9037, 1996, 2167, 3792, 4296, 2001, 2580, 2011, 2552, 1997, 1996, 6372, 2000, 7949, 2008, 4296, 2225, 2000, 27905, 1010, 2152, 2391, 1010, 1998, 5904, 1012, 2076, 1996, 2942, 2162, 1010, 1996, 17025, 1011, 2000, 1011, 15842, 7683, 1997, 1996, 4296, 2052, 2022, 8995, 2000, 1996, 8055, 2162, 3947, 1025, 6067, 12057, 2046, 17025, 2052, 2022, 2333, 2011, 4334, 2083, 15842, 2000, 1996, 8055, 3007, 1997, 6713, 1010, 3448, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 56, 'end_positions': 56}.
01/31/2025 04:04:52 - INFO - __main__ - Sample 21902 of the training set: {'input_ids': [101, 2043, 2967, 2008, 2024, 17379, 2594, 2000, 2169, 2060, 2433, 1037, 6056, 2005, 5023, 5114, 1010, 2009, 2003, 2170, 2054, 1029, 102, 2019, 4895, 14854, 2100, 4707, 2003, 1037, 6056, 2426, 9428, 17379, 2594, 2967, 2005, 4748, 21929, 2030, 5023, 5114, 1010, 3227, 2070, 6383, 2512, 1011, 10605, 2177, 5716, 7208, 2007, 2576, 4243, 1010, 17731, 4804, 1999, 3863, 2005, 1996, 11119, 3949, 1012, 2066, 15694, 1010, 4895, 14854, 2100, 21277, 2024, 2025, 9352, 6206, 1010, 2021, 4406, 15694, 1010, 2011, 2049, 11703, 22048, 3267, 1998, 2411, 2307, 3361, 4219, 1010, 2019, 4895, 14854, 2100, 4707, 2064, 2022, 2172, 2062, 4795, 2000, 1996, 2270, 3037, 1012, 2019, 2220, 2224, 1997, 1996, 2744, 2001, 2011, 2280, 2149, 2343, 10117, 1000, 11389, 1000, 8573, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 24, 'end_positions': 27}.
01/31/2025 04:04:52 - INFO - __main__ - Sample 4476 of the training set: {'input_ids': [101, 2429, 2000, 1996, 2137, 3075, 2523, 1010, 2054, 4635, 2106, 1996, 2338, 2031, 2426, 1996, 2087, 4703, 8315, 2808, 2013, 2456, 2000, 2268, 1029, 102, 2000, 3102, 1037, 19545, 9001, 2038, 2042, 1037, 3120, 1997, 3278, 6704, 2144, 2049, 2108, 1996, 3395, 1997, 9823, 2817, 2004, 2220, 2004, 3699, 1012, 1996, 2338, 1005, 1055, 5762, 22889, 9236, 1010, 11268, 7088, 3723, 1010, 1998, 3581, 6594, 1997, 9040, 2031, 2419, 2111, 2000, 4119, 2049, 6413, 2791, 1999, 8860, 1998, 12463, 2408, 1996, 2142, 2163, 1012, 1996, 2137, 3075, 2523, 2988, 2008, 2000, 3102, 1037, 19545, 9001, 2001, 2193, 2538, 1997, 1996, 2531, 2087, 4703, 8315, 2808, 1997, 2456, 1516, 2268, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 98, 'end_positions': 98}.
01/31/2025 04:04:54 - INFO - __main__ - ***** Running training *****
01/31/2025 04:04:54 - INFO - __main__ - Num examples = 131754
01/31/2025 04:04:54 - INFO - __main__ - Num Epochs = 3
01/31/2025 04:04:54 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 04:04:54 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 04:04:54 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 04:04:54 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 04:04:55 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 04:04:55 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 04:04:55 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 04:05:03 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 04:05:03 - INFO - __main__ - Num examples = 1000
01/31/2025 04:05:03 - INFO - __main__ - Batch size = 8
01/31/2025 04:05:10 - INFO - __main__ - Evaluation metrics: {'exact': 65.8, 'f1': 68.45528013277854, 'total': 1000, 'HasAns_exact': 49.59839357429719, 'HasAns_f1': 54.930281391121675, 'HasAns_total': 498, 'NoAns_exact': 81.87250996015936, 'NoAns_f1': 81.87250996015936, 'NoAns_total': 502, 'best_exact': 65.8, 'best_exact_thresh': 0.0, 'best_f1': 68.45528013277848, 'best_f1_thresh': 0.0}
01/31/2025 04:18:41 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 04:19:24 - INFO - __main__ - Sample 48694 of the training set: {'input_ids': [101, 2339, 2001, 2033, 12044, 29336, 2378, 2196, 11625, 1029, 102, 1999, 3411, 1010, 21616, 2203, 2080, 1010, 1037, 2887, 16012, 24229, 2551, 2005, 1996, 13859, 2194, 7569, 7677, 1010, 4453, 2033, 12044, 29336, 2378, 1006, 19875, 1011, 23593, 2497, 1007, 1010, 1037, 13922, 2550, 2011, 1996, 16622, 7279, 28775, 6894, 2819, 25022, 18886, 19172, 1010, 2004, 2019, 24054, 1997, 20287, 2290, 1011, 28155, 2417, 14194, 18260, 1010, 1037, 4187, 9007, 2109, 2011, 1996, 2303, 2000, 3965, 16480, 4244, 27833, 1012, 4111, 7012, 3662, 2200, 2204, 24054, 2100, 3466, 2004, 1999, 6612, 7012, 1010, 2174, 1037, 2146, 2744, 2817, 1999, 6077, 2179, 11704, 3896, 2012, 3020, 21656, 1998, 2004, 1037, 2765, 2033, 12044, 29336, 2378, 2001, 3373, 2000, 2022, 2205, 11704, 2005, 2529, 2224, 1012, 2033, 12044, 29336, 2378, 2001, 2196, 11625, 1010, 2138, 1997, 2049, 15316, 3896, 1997, 21434, 1010, 6740, 26118, 1010, 1998, 2823, 2331, 1999, 5911, 6077, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 136, 'end_positions': 146}.
01/31/2025 04:19:24 - INFO - __main__ - Sample 104635 of the training set: {'input_ids': [101, 2054, 2003, 1996, 17011, 1997, 13035, 1999, 1996, 13970, 15457, 2099, 5583, 1029, 102, 2045, 2079, 4839, 20347, 2306, 2122, 4064, 4655, 2073, 5200, 2064, 5788, 2005, 1996, 2287, 1997, 1996, 5943, 2291, 1012, 2122, 17011, 2015, 5258, 2043, 21167, 1005, 1055, 13943, 2558, 2003, 1037, 10480, 12884, 1997, 2008, 1997, 1996, 4874, 1010, 2107, 2004, 1015, 1024, 1016, 1010, 2030, 1017, 1024, 1018, 1012, 2065, 1010, 2360, 1010, 2019, 4874, 20347, 1996, 3103, 2320, 2005, 2296, 2048, 21167, 20347, 1010, 2009, 2097, 2069, 3143, 2431, 2019, 8753, 2011, 1996, 2051, 21167, 5651, 2000, 2049, 2434, 2597, 1012, 1996, 2087, 4600, 10357, 17011, 1999, 1996, 13970, 15457, 2099, 5583, 1010, 2007, 2058, 3263, 2124, 5200, 1010, 2003, 1996, 1016, 1024, 1017, 17011, 1012, 5200, 1999, 2023, 17011, 3143, 1016, 20347, 2005, 2296, 1017, 1997, 21167, 1010, 1998, 2024, 2124, 2004, 20228, 21823, 15460, 2138, 1996, 2922, 1997, 1996, 2124, 13970, 15457, 2099, 5583, 5200, 1010, 26930, 1010, 2003, 2426, 2068, 1012, 2348, 26930, 7821, 21167, 1005, 1055, 8753, 5570, 1010, 1996, 1016, 1024, 1017, 17011, 21312, 2027, 2064, 2196, 8902, 24198, 1012, 1996, 1017, 1024, 1018, 1010, 1017, 1024, 1019, 1010, 1018, 1024, 1021, 1998, 1016, 1024, 1019, 17011, 2015, 2024, 2625, 10357, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 04:19:24 - INFO - __main__ - Sample 54084 of the training set: {'input_ids': [101, 1996, 12469, 2018, 1037, 2465, 1037, 3576, 2223, 12912, 2006, 2048, 6642, 2007, 2040, 1029, 102, 2077, 6608, 1037, 13908, 3820, 2007, 1996, 8472, 2221, 26317, 1999, 2262, 1010, 1996, 12469, 2018, 1037, 2465, 1037, 3576, 2223, 12912, 2006, 2048, 6642, 2007, 1996, 21877, 11069, 9058, 1006, 3106, 1516, 2786, 1998, 2432, 1516, 2262, 1007, 1012, 29431, 2638, 5472, 4059, 3266, 1996, 9058, 2013, 2294, 2000, 2230, 1012, 1999, 1996, 2558, 2090, 2216, 8924, 2007, 1996, 9058, 1996, 2252, 2018, 12912, 2015, 2007, 1996, 14700, 8626, 1998, 22304, 11320, 16206, 16446, 1012, 1996, 11320, 16206, 16446, 2020, 2411, 28734, 3615, 2000, 2011, 9090, 14418, 2100, 2004, 1000, 3889, 2962, 1005, 1055, 5440, 2136, 1012, 1000, 1996, 2289, 13908, 3206, 2007, 1996, 5298, 15488, 23212, 2229, 2001, 11677, 2011, 3313, 1037, 12912, 2015, 2007, 1996, 10108, 12469, 1998, 2225, 2702, 2078, 6323, 13118, 2595, 1012, 2006, 2244, 2385, 1010, 2297, 1996, 12469, 2623, 1037, 2693, 1997, 2037, 2327, 2465, 1037, 8727, 2013, 18226, 1999, 1996, 3516, 2110, 2223, 2000, 21381, 3509, 1999, 1996, 3792, 2223, 2005, 1996, 2325, 2161, 1012, 2048, 2420, 2101, 1010, 2006, 1996, 4985, 1010, 1996, 12469, 2772, 1037, 1018, 1011, 2095, 2447, 2458, 3206, 2007, 1996, 2148, 8815, 3165, 12505, 1997, 1996, 13608, 2223, 1010, 4566, 2037, 4766, 3276, 2007, 1996, 8472, 2221, 26317, 1998, 3859, 6920, 24944, 1996, 3165, 12505, 1996, 2148, 8815, 12469, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 44, 'end_positions': 46}.
01/31/2025 04:19:26 - INFO - __main__ - ***** Running training *****
01/31/2025 04:19:26 - INFO - __main__ - Num examples = 131754
01/31/2025 04:19:26 - INFO - __main__ - Num Epochs = 3
01/31/2025 04:19:26 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 04:19:26 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 04:19:26 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 04:19:26 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 04:19:27 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 04:19:27 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 04:19:27 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 04:20:49 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 04:20:49 - INFO - __main__ - Num examples = 1000
01/31/2025 04:20:49 - INFO - __main__ - Batch size = 8
01/31/2025 04:20:57 - INFO - __main__ - Evaluation metrics: {'exact': 65.8, 'f1': 68.45528013277854, 'total': 1000, 'HasAns_exact': 49.59839357429719, 'HasAns_f1': 54.930281391121675, 'HasAns_total': 498, 'NoAns_exact': 81.87250996015936, 'NoAns_f1': 81.87250996015936, 'NoAns_total': 502, 'best_exact': 65.8, 'best_exact_thresh': 0.0, 'best_f1': 68.45528013277848, 'best_f1_thresh': 0.0}
01/31/2025 04:22:20 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 04:22:24 - INFO - __main__ - Sample 75999 of the training set: {'input_ids': [101, 2054, 3257, 2106, 1996, 2167, 3792, 4296, 7949, 1029, 102, 2006, 2255, 2423, 1010, 10081, 1010, 2810, 2211, 2006, 1996, 17025, 1998, 15842, 4296, 2000, 7532, 1996, 3417, 2103, 1997, 17025, 2007, 1996, 2110, 3007, 1997, 15842, 1012, 1999, 9037, 1996, 2167, 3792, 4296, 2001, 2580, 2011, 2552, 1997, 1996, 6372, 2000, 7949, 2008, 4296, 2225, 2000, 27905, 1010, 2152, 2391, 1010, 1998, 5904, 1012, 2076, 1996, 2942, 2162, 1010, 1996, 17025, 1011, 2000, 1011, 15842, 7683, 1997, 1996, 4296, 2052, 2022, 8995, 2000, 1996, 8055, 2162, 3947, 1025, 6067, 12057, 2046, 17025, 2052, 2022, 2333, 2011, 4334, 2083, 15842, 2000, 1996, 8055, 3007, 1997, 6713, 1010, 3448, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 56, 'end_positions': 56}.
01/31/2025 04:22:24 - INFO - __main__ - Sample 21902 of the training set: {'input_ids': [101, 2043, 2967, 2008, 2024, 17379, 2594, 2000, 2169, 2060, 2433, 1037, 6056, 2005, 5023, 5114, 1010, 2009, 2003, 2170, 2054, 1029, 102, 2019, 4895, 14854, 2100, 4707, 2003, 1037, 6056, 2426, 9428, 17379, 2594, 2967, 2005, 4748, 21929, 2030, 5023, 5114, 1010, 3227, 2070, 6383, 2512, 1011, 10605, 2177, 5716, 7208, 2007, 2576, 4243, 1010, 17731, 4804, 1999, 3863, 2005, 1996, 11119, 3949, 1012, 2066, 15694, 1010, 4895, 14854, 2100, 21277, 2024, 2025, 9352, 6206, 1010, 2021, 4406, 15694, 1010, 2011, 2049, 11703, 22048, 3267, 1998, 2411, 2307, 3361, 4219, 1010, 2019, 4895, 14854, 2100, 4707, 2064, 2022, 2172, 2062, 4795, 2000, 1996, 2270, 3037, 1012, 2019, 2220, 2224, 1997, 1996, 2744, 2001, 2011, 2280, 2149, 2343, 10117, 1000, 11389, 1000, 8573, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 24, 'end_positions': 27}.
01/31/2025 04:22:24 - INFO - __main__ - Sample 4476 of the training set: {'input_ids': [101, 2429, 2000, 1996, 2137, 3075, 2523, 1010, 2054, 4635, 2106, 1996, 2338, 2031, 2426, 1996, 2087, 4703, 8315, 2808, 2013, 2456, 2000, 2268, 1029, 102, 2000, 3102, 1037, 19545, 9001, 2038, 2042, 1037, 3120, 1997, 3278, 6704, 2144, 2049, 2108, 1996, 3395, 1997, 9823, 2817, 2004, 2220, 2004, 3699, 1012, 1996, 2338, 1005, 1055, 5762, 22889, 9236, 1010, 11268, 7088, 3723, 1010, 1998, 3581, 6594, 1997, 9040, 2031, 2419, 2111, 2000, 4119, 2049, 6413, 2791, 1999, 8860, 1998, 12463, 2408, 1996, 2142, 2163, 1012, 1996, 2137, 3075, 2523, 2988, 2008, 2000, 3102, 1037, 19545, 9001, 2001, 2193, 2538, 1997, 1996, 2531, 2087, 4703, 8315, 2808, 1997, 2456, 1516, 2268, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 98, 'end_positions': 98}.
01/31/2025 04:22:26 - INFO - __main__ - ***** Running training *****
01/31/2025 04:22:26 - INFO - __main__ - Num examples = 131754
01/31/2025 04:22:26 - INFO - __main__ - Num Epochs = 3
01/31/2025 04:22:26 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 04:22:26 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 04:22:26 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 04:22:26 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 04:22:27 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 04:22:27 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 04:22:27 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 04:23:14 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 04:23:14 - INFO - __main__ - Num examples = 1000
01/31/2025 04:23:14 - INFO - __main__ - Batch size = 8
01/31/2025 04:23:20 - INFO - __main__ - Evaluation metrics: {'exact': 65.7, 'f1': 68.35528013277855, 'total': 1000, 'HasAns_exact': 49.59839357429719, 'HasAns_f1': 54.930281391121675, 'HasAns_total': 498, 'NoAns_exact': 81.67330677290836, 'NoAns_f1': 81.67330677290836, 'NoAns_total': 502, 'best_exact': 65.7, 'best_exact_thresh': 0.0, 'best_f1': 68.35528013277849, 'best_f1_thresh': 0.0}
01/31/2025 04:25:38 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 04:26:20 - INFO - __main__ - Sample 84890 of the training set: {'input_ids': [101, 2054, 3029, 3253, 2825, 7300, 5424, 1011, 5611, 4736, 1010, 2029, 26099, 3569, 1029, 102, 26099, 19872, 10287, 2090, 1996, 4013, 1011, 2530, 1010, 4013, 1011, 3354, 1010, 1998, 8699, 2923, 3034, 13815, 2058, 1996, 5512, 1997, 1996, 1000, 2345, 4012, 23041, 7413, 1000, 12786, 5336, 2964, 1999, 3088, 1998, 4021, 1998, 1996, 6469, 2075, 1997, 3795, 3521, 13463, 1996, 3147, 2162, 2090, 1996, 2225, 1998, 1996, 3354, 2586, 1012, 2012, 2316, 5575, 26099, 4912, 1037, 16413, 2005, 1996, 24685, 1997, 2248, 3639, 21277, 1010, 2490, 2005, 1996, 4336, 1997, 13437, 1010, 11337, 1010, 1998, 9835, 2013, 2413, 3627, 1010, 2490, 2005, 1996, 9302, 2157, 1997, 2709, 1010, 1998, 1996, 7375, 1997, 4895, 18853, 4953, 1996, 5424, 1516, 5611, 4736, 1012, 2002, 4594, 1999, 19670, 1996, 19973, 2000, 3413, 18853, 2006, 2169, 1997, 2122, 3314, 1010, 5546, 12329, 1996, 2844, 2490, 1997, 2859, 1998, 2634, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 113, 'end_positions': 113}.
01/31/2025 04:26:20 - INFO - __main__ - Sample 39544 of the training set: {'input_ids': [101, 2040, 3024, 23564, 20224, 3148, 2007, 16581, 1000, 8910, 1000, 1029, 102, 2016, 2003, 1996, 2069, 2450, 3495, 2315, 1999, 1996, 23183, 1005, 2019, 1025, 4161, 1006, 20640, 2247, 2007, 4441, 1007, 2000, 2022, 1037, 3696, 1997, 2643, 2000, 8438, 1025, 2004, 2028, 2040, 1000, 13802, 2014, 15775, 16643, 3723, 1000, 1025, 2019, 15578, 10265, 3372, 2028, 1025, 4217, 1997, 2014, 2388, 1998, 4056, 2000, 16455, 5819, 2145, 1999, 1996, 26578, 1025, 20640, 1006, 5921, 2308, 1007, 3970, 2046, 2326, 2011, 2643, 1025, 8725, 2005, 2011, 1006, 2028, 1997, 1996, 23172, 2004, 2566, 7025, 1007, 23564, 20224, 3148, 1006, 12397, 10980, 2015, 1007, 1025, 2008, 1999, 2014, 5593, 2016, 12427, 1999, 1996, 3379, 1998, 20640, 2018, 3229, 2000, 2632, 1011, 2771, 13492, 2497, 1006, 5319, 2000, 2022, 1996, 4151, 1997, 7570, 11983, 1007, 1010, 1998, 2001, 3024, 2007, 16581, 1000, 8910, 1000, 2011, 2643, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 04:26:20 - INFO - __main__ - Sample 103500 of the training set: {'input_ids': [101, 2054, 2828, 1997, 2396, 2003, 2108, 3755, 2011, 2396, 4896, 1999, 9719, 1029, 102, 2028, 1997, 1996, 2838, 1997, 18543, 2396, 2003, 1037, 11765, 2875, 29407, 4169, 2348, 17158, 2396, 2003, 2108, 20001, 2135, 3755, 2011, 1037, 2193, 1997, 2396, 1000, 4896, 1000, 1998, 2087, 5546, 1996, 19332, 8464, 4546, 2396, 2803, 1012, 4546, 2396, 11726, 4839, 1999, 2035, 1996, 2364, 4865, 1998, 2045, 2003, 1037, 2312, 1998, 17133, 3293, 2396, 3496, 1012, 9719, 2001, 2349, 2000, 3677, 1996, 2248, 2396, 2782, 19676, 2050, 1999, 2294, 2021, 2023, 2001, 8014, 2012, 1996, 2197, 3371, 2206, 1037, 7593, 2090, 1996, 3803, 18829, 1997, 19676, 2050, 1998, 1996, 9719, 3757, 1997, 2495, 1998, 3226, 2058, 1996, 3295, 1997, 2070, 1997, 1996, 19676, 2050, 2824, 1999, 1996, 5037, 4753, 1997, 1996, 3007, 19332, 8464, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 29, 'end_positions': 30}.
01/31/2025 04:26:22 - INFO - __main__ - ***** Running training *****
01/31/2025 04:26:22 - INFO - __main__ - Num examples = 131754
01/31/2025 04:26:22 - INFO - __main__ - Num Epochs = 3
01/31/2025 04:26:22 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 04:26:22 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 04:26:22 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 04:26:22 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 04:26:23 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 04:26:23 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 04:26:23 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 04:30:01 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 04:30:01 - INFO - __main__ - Num examples = 1000
01/31/2025 04:30:01 - INFO - __main__ - Batch size = 8
01/31/2025 04:30:07 - INFO - __main__ - Evaluation metrics: {'exact': 65.7, 'f1': 68.35528013277855, 'total': 1000, 'HasAns_exact': 49.59839357429719, 'HasAns_f1': 54.930281391121675, 'HasAns_total': 498, 'NoAns_exact': 81.67330677290836, 'NoAns_f1': 81.67330677290836, 'NoAns_total': 502, 'best_exact': 65.7, 'best_exact_thresh': 0.0, 'best_f1': 68.35528013277849, 'best_f1_thresh': 0.0}
01/31/2025 04:31:51 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 04:32:33 - INFO - __main__ - Sample 60118 of the training set: {'input_ids': [101, 2073, 2079, 2087, 5544, 19464, 3471, 2272, 2013, 1029, 102, 2349, 2000, 2116, 4693, 1997, 11806, 4093, 1010, 2087, 1997, 1996, 2181, 2003, 5044, 2011, 2529, 3747, 1012, 1996, 2434, 3019, 10072, 1997, 16215, 12228, 2401, 2003, 3224, 2007, 21760, 2004, 2049, 21047, 2427, 1010, 2004, 2064, 2145, 2022, 2179, 1999, 1996, 15030, 8713, 2232, 4020, 2651, 1012, 1999, 1996, 25770, 2015, 1010, 1037, 8150, 1997, 21760, 1998, 19893, 2052, 2022, 3019, 1012, 2174, 1010, 2087, 1997, 1996, 8575, 2031, 2042, 5985, 1998, 2024, 1999, 11806, 4910, 2224, 2096, 2087, 1997, 1996, 6138, 2024, 8461, 2007, 19893, 1998, 7222, 1012, 2144, 2901, 1010, 16215, 12228, 2401, 1005, 1055, 6138, 2031, 2042, 3266, 13659, 2005, 1037, 2062, 3019, 1998, 7823, 10072, 2062, 24501, 18622, 4765, 2000, 4785, 2689, 2004, 2092, 2004, 7870, 1998, 2310, 27512, 1012, 1999, 7831, 2000, 1996, 3224, 1010, 5237, 2003, 2145, 3243, 7511, 1998, 6817, 2011, 2312, 5090, 1998, 18847, 14561, 2015, 1012, 3471, 2182, 2024, 3303, 2926, 2011, 6233, 15330, 4318, 6993, 2076, 1996, 2621, 2706, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 04:32:33 - INFO - __main__ - Sample 116875 of the training set: {'input_ids': [101, 2043, 2001, 9130, 2490, 2357, 2006, 1029, 102, 3645, 1022, 3640, 11907, 8346, 2007, 3784, 2578, 2013, 7513, 1998, 2500, 1012, 1037, 5310, 2064, 2085, 8833, 1999, 2000, 3645, 2007, 1037, 7513, 4070, 1010, 2029, 2064, 2022, 2109, 2000, 3229, 2578, 1998, 26351, 8093, 10698, 4371, 5097, 1998, 10906, 2090, 5733, 1012, 3645, 1022, 2036, 3719, 2007, 1037, 7396, 10439, 2005, 7513, 1005, 1055, 3712, 23663, 6112, 5527, 2326, 1010, 2029, 2036, 4473, 18726, 2000, 3828, 6764, 3495, 2000, 3712, 23663, 1012, 1037, 3712, 23663, 7396, 2005, 1996, 15363, 1998, 5371, 10566, 2003, 2025, 2443, 1999, 3645, 1022, 1010, 1998, 2442, 2022, 22817, 10329, 1012, 24378, 14959, 18726, 2024, 3024, 2104, 1996, 12202, 4435, 1010, 2164, 12202, 2189, 1010, 12202, 2678, 1010, 1998, 1996, 12202, 6047, 15621, 7452, 2005, 2224, 2007, 2019, 12202, 9475, 10122, 1012, 2399, 2064, 17409, 2046, 2019, 12202, 2444, 9594, 10439, 1010, 2029, 2036, 4473, 5198, 2000, 3193, 2037, 6337, 1998, 27911, 9363, 2890, 1012, 2060, 24378, 18726, 3073, 1996, 3754, 2000, 4957, 17312, 2099, 1998, 9130, 1012, 2349, 2000, 9130, 7532, 2326, 3431, 1010, 9130, 2490, 2003, 9776, 1999, 2035, 24378, 18726, 4621, 2238, 1022, 1010, 2325, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 04:32:33 - INFO - __main__ - Sample 116810 of the training set: {'input_ids': [101, 2054, 2003, 1996, 2171, 1997, 1996, 2839, 7513, 2109, 2000, 2191, 3645, 1023, 4025, 2062, 16115, 3468, 1029, 102, 7513, 2211, 2019, 6475, 3049, 8857, 2105, 3645, 1022, 1998, 2049, 3302, 13855, 1999, 2255, 2262, 1010, 3225, 2007, 2049, 2034, 2547, 15147, 4239, 2075, 2006, 2255, 2403, 1010, 2262, 1012, 7513, 1005, 1055, 6475, 5166, 1997, 2149, 1002, 1015, 1012, 1019, 1516, 1015, 1012, 1022, 4551, 2001, 6022, 3469, 2084, 1996, 2149, 1002, 3263, 2454, 3049, 2109, 2000, 5326, 3645, 5345, 1012, 2004, 2112, 1997, 2049, 3049, 1010, 7513, 2275, 2039, 4090, 3769, 1011, 2039, 5324, 2503, 25943, 1006, 3952, 7995, 2006, 3302, 1007, 1010, 3024, 2731, 2005, 7027, 5126, 1999, 5386, 2007, 13420, 1010, 1998, 8678, 2007, 1996, 8139, 3573, 4677, 2190, 4965, 2000, 2640, 4423, 7258, 2000, 13398, 5733, 1012, 1999, 2019, 3947, 2000, 2191, 7027, 8834, 1997, 3645, 1022, 5733, 2062, 1000, 3167, 1000, 1010, 7513, 2036, 2764, 1037, 2839, 2124, 1999, 2394, 1011, 4092, 6089, 2004, 1000, 10786, 2829, 1000, 1010, 3005, 7214, 6337, 1006, 2164, 3167, 7760, 1010, 10402, 1010, 1998, 22028, 1007, 2003, 2036, 2956, 2006, 10467, 3197, 1997, 3645, 1022, 5733, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 04:32:34 - INFO - __main__ - ***** Running training *****
01/31/2025 04:32:34 - INFO - __main__ - Num examples = 131754
01/31/2025 04:32:34 - INFO - __main__ - Num Epochs = 3
01/31/2025 04:32:34 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 04:32:34 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 04:32:34 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 04:32:34 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 04:32:35 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 04:32:35 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 04:32:35 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 04:33:07 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 04:33:07 - INFO - __main__ - Num examples = 1000
01/31/2025 04:33:07 - INFO - __main__ - Batch size = 8
01/31/2025 04:33:14 - INFO - __main__ - Evaluation metrics: {'exact': 65.7, 'f1': 68.35528013277855, 'total': 1000, 'HasAns_exact': 49.59839357429719, 'HasAns_f1': 54.930281391121675, 'HasAns_total': 498, 'NoAns_exact': 81.67330677290836, 'NoAns_f1': 81.67330677290836, 'NoAns_total': 502, 'best_exact': 65.7, 'best_exact_thresh': 0.0, 'best_f1': 68.35528013277849, 'best_f1_thresh': 0.0}
01/31/2025 04:35:51 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 04:35:54 - INFO - __main__ - Sample 26036 of the training set: {'input_ids': [101, 2054, 4247, 1999, 2070, 7925, 2012, 1996, 19258, 1997, 1996, 12854, 4034, 2030, 3834, 2533, 1029, 102, 3525, 2007, 1996, 29591, 1997, 1000, 2379, 2264, 1000, 1999, 8041, 1998, 2510, 7925, 1010, 1000, 2690, 2264, 1000, 19914, 1012, 2174, 1010, 1000, 2379, 2264, 1000, 4247, 1999, 2070, 7925, 2012, 1996, 19258, 1997, 1996, 12854, 4034, 2030, 3834, 2533, 1012, 2027, 2024, 2025, 3227, 2641, 5664, 4655, 2004, 2027, 2020, 2012, 2037, 2434, 6210, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 42, 'end_positions': 43}.
01/31/2025 04:35:54 - INFO - __main__ - Sample 46158 of the training set: {'input_ids': [101, 3183, 1010, 2247, 2007, 4557, 11611, 1998, 2198, 18343, 1010, 2106, 2726, 7625, 5136, 2000, 2022, 2028, 1997, 1996, 2093, 4602, 2273, 2008, 2412, 2973, 1029, 102, 2122, 5328, 2006, 3412, 13986, 1998, 1996, 5197, 1997, 3265, 13454, 1010, 2247, 2007, 1996, 2591, 3206, 1010, 2150, 3391, 6383, 1999, 1996, 2137, 8355, 1998, 1996, 21168, 1997, 1996, 2142, 2163, 4552, 1012, 2726, 7625, 2170, 2005, 1037, 1000, 2813, 1997, 8745, 2090, 2277, 1998, 2110, 1000, 2012, 1996, 2976, 2504, 1012, 2002, 3130, 2018, 3569, 3144, 4073, 2000, 4487, 8583, 2696, 16558, 4509, 1996, 2277, 1997, 2563, 1999, 3448, 1010, 1998, 8786, 1996, 3448, 11671, 2005, 3412, 4071, 1012, 7625, 1005, 1055, 2576, 15084, 2020, 6551, 5105, 2011, 1996, 7896, 1997, 2198, 18343, 1010, 4557, 11611, 1010, 1998, 7527, 8446, 3183, 2002, 2641, 1996, 2093, 4602, 2273, 2008, 2412, 2973, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 128, 'end_positions': 129}.
01/31/2025 04:35:54 - INFO - __main__ - Sample 121019 of the training set: {'input_ids': [101, 2129, 2146, 2001, 16073, 9048, 3750, 1999, 2373, 1029, 102, 1996, 8442, 1011, 2028, 2095, 5853, 1997, 1996, 16073, 9048, 3750, 2001, 1996, 6493, 1997, 2151, 2822, 3750, 1012, 16073, 9048, 1005, 1055, 5853, 2003, 2036, 6334, 2004, 1996, 2927, 1997, 2019, 3690, 2124, 2004, 1996, 1000, 2152, 13282, 1000, 1010, 2076, 2029, 1996, 5321, 2584, 1996, 28672, 1997, 2049, 2591, 1010, 3171, 1998, 2510, 2373, 1012, 16073, 9048, 1005, 1055, 2146, 5853, 2318, 2043, 2002, 2001, 2809, 2086, 2214, 2588, 1996, 4895, 7292, 2135, 13614, 1997, 2010, 2269, 1012, 2000, 4652, 1037, 9377, 1997, 2079, 18581, 2078, 1005, 1055, 21237, 4818, 18847, 18155, 6026, 1997, 2373, 2076, 1996, 15647, 1010, 1996, 18454, 14191, 4048, 3750, 1010, 2006, 2010, 2331, 8270, 1010, 15789, 2805, 2176, 3026, 5239, 7767, 2000, 21208, 2006, 6852, 1997, 2010, 2402, 2365, 1012, 1996, 2176, 7767, 1517, 2365, 2378, 1010, 1041, 14454, 4609, 1010, 10514, 5705, 23278, 1010, 1998, 27885, 10448, 1517, 2020, 4217, 2005, 2037, 2146, 2326, 1010, 2021, 2036, 2000, 4675, 18908, 2169, 2060, 1005, 1055, 8092, 1012, 2087, 2590, 1010, 1996, 2176, 2020, 2025, 4876, 3141, 2000, 1996, 4461, 2155, 1998, 4201, 2053, 4366, 2000, 1996, 6106, 1012, 2174, 1010, 2004, 2051, 2979, 1010, 2083, 3382, 1998, 24532, 12758, 1010, 27885, 10448, 1010, 1996, 2087, 3502, 1997, 1996, 2176, 1010, 4719, 2107, 2576, 13811, 2004, 2000, 2022, 1037, 4022, 5081, 1012, 2130, 2295, 27885, 10448, 1005, 1055, 9721, 2001, 2196, 2019, 3277, 1010, 2010, 3167, 24416, 1998, 2576, 9530, 8043, 20203, 6491, 2419, 2032, 2046, 2019, 9686, 25015, 3436, 4736, 2007, 1996, 2402, 3750, 1012, 1999, 18610, 2683, 16073, 9048, 1010, 2083, 7577, 7301, 1010, 4487, 10286, 7583, 1998, 8580, 27885, 10448, 1517, 1037, 3278, 3377, 2005, 1037, 5417, 1011, 2095, 1011, 2214, 3750, 2058, 1037, 19863, 2100, 3761, 1998, 5281, 3474, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 12, 'end_positions': 15}.
01/31/2025 04:35:56 - INFO - __main__ - ***** Running training *****
01/31/2025 04:35:56 - INFO - __main__ - Num examples = 131754
01/31/2025 04:35:56 - INFO - __main__ - Num Epochs = 3
01/31/2025 04:35:56 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 04:35:56 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 04:35:56 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 04:35:56 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 04:35:57 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 04:35:57 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 04:35:57 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 04:36:17 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 04:36:17 - INFO - __main__ - Num examples = 12134
01/31/2025 04:36:17 - INFO - __main__ - Batch size = 8
01/31/2025 04:37:37 - INFO - __main__ - Evaluation metrics: {'exact': 66.25958056093658, 'f1': 69.53356952564094, 'total': 11873, 'HasAns_exact': 49.61201079622132, 'HasAns_f1': 56.16937769533277, 'HasAns_total': 5928, 'NoAns_exact': 82.85954583683768, 'NoAns_f1': 82.85954583683768, 'NoAns_total': 5945, 'best_exact': 66.26800303208961, 'best_exact_thresh': 0.0, 'best_f1': 69.54199199679411, 'best_f1_thresh': 0.0}
01/31/2025 04:39:29 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 04:39:32 - INFO - __main__ - Sample 39995 of the training set: {'input_ids': [101, 1999, 2029, 2752, 2515, 4940, 1005, 1055, 24908, 4610, 2031, 20828, 1029, 102, 4940, 2038, 1037, 3811, 24908, 4610, 2007, 3327, 20828, 1999, 5446, 1010, 5814, 1010, 2470, 1010, 2009, 1010, 2495, 1010, 12708, 1010, 5193, 1998, 6813, 1012, 4940, 3506, 1996, 4075, 2005, 2116, 1997, 2660, 1005, 1055, 2922, 11578, 1010, 2164, 2274, 1997, 1996, 2702, 2922, 1999, 1996, 2406, 1006, 2241, 2006, 6599, 1007, 1010, 1998, 2176, 1997, 1996, 2922, 2416, 1999, 1996, 2406, 1006, 2241, 2006, 3006, 3007, 6648, 1007, 1006, 2019, 2480, 1010, 22245, 3021, 9956, 2078, 1006, 1996, 2088, 1005, 1055, 2922, 5471, 2194, 1007, 1010, 1996, 2120, 2660, 2924, 1998, 10093, 20528, 1007, 1010, 2004, 2092, 2004, 2107, 4387, 4230, 1998, 2228, 7286, 2004, 1996, 2449, 2473, 1997, 2660, 1998, 1996, 2827, 2473, 1997, 3119, 9209, 1012, 4940, 1005, 1055, 9435, 2036, 2031, 1996, 2132, 4822, 1997, 14008, 14971, 16862, 3316, 5624, 2015, 1006, 2164, 13207, 3122, 1007, 1010, 21122, 5582, 2015, 1010, 4539, 1010, 1047, 1011, 20481, 1004, 2436, 9316, 1012, 1996, 2103, 2003, 2188, 2000, 2660, 1005, 1055, 2922, 1998, 20530, 2712, 6442, 2029, 16024, 2062, 2084, 1002, 4293, 4551, 1999, 3119, 2296, 2095, 1998, 4464, 1003, 1997, 1996, 3842, 1005, 1055, 11661, 3119, 1012, 4940, 3199, 3640, 2019, 4443, 2391, 2005, 2120, 1998, 2248, 5731, 1010, 1998, 2003, 2660, 1005, 1055, 2117, 20530, 3199, 1012, 1031, 11091, 2734, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 24, 'end_positions': 38}.
01/31/2025 04:39:32 - INFO - __main__ - Sample 50042 of the training set: {'input_ids': [101, 2054, 2001, 3848, 1005, 1055, 2388, 1005, 1055, 2171, 1029, 102, 3848, 2001, 1996, 2684, 1997, 3159, 3487, 1010, 3804, 1997, 5982, 1998, 2358, 27362, 14644, 2078, 1010, 1996, 2959, 2365, 1997, 2332, 2577, 3523, 1012, 2119, 1996, 3804, 1997, 5982, 1998, 2332, 2577, 3523, 2351, 1999, 11102, 1010, 1998, 3848, 2001, 2992, 2104, 2485, 10429, 2011, 2014, 2446, 1011, 2141, 2388, 4615, 3848, 1997, 24937, 1011, 2522, 4645, 1011, 7842, 2389, 8151, 1012, 2016, 7900, 1996, 6106, 4793, 2324, 1010, 2044, 2014, 2269, 1005, 1055, 2093, 6422, 3428, 2018, 2035, 2351, 1010, 2975, 2053, 6405, 11476, 2336, 1012, 1996, 2142, 2983, 2001, 2525, 2019, 2511, 6543, 12078, 1010, 1999, 2029, 1996, 11074, 2218, 4659, 2210, 3622, 2576, 2373, 1012, 9139, 1010, 3848, 4692, 2000, 3747, 2231, 3343, 1998, 18645, 14651, 1025, 7271, 1010, 2016, 2150, 1037, 2120, 12696, 2040, 2001, 4453, 2007, 9384, 4781, 1997, 3167, 16561, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 63, 'end_positions': 73}.
01/31/2025 04:39:32 - INFO - __main__ - Sample 91386 of the training set: {'input_ids': [101, 2040, 2020, 3862, 2715, 3060, 17687, 8712, 1029, 102, 3365, 2715, 17687, 11153, 3857, 5693, 2008, 4321, 28024, 1996, 9406, 1042, 1011, 1019, 3063, 4275, 2328, 1999, 1996, 2220, 6641, 2104, 1996, 10429, 1997, 9406, 6490, 2937, 6746, 8840, 2906, 1012, 2434, 8840, 2906, 1011, 2772, 5693, 2024, 4912, 2044, 1998, 5186, 7070, 1012, 2060, 11153, 2013, 1996, 8840, 2906, 2558, 1998, 3041, 2421, 10241, 1998, 25706, 1010, 15942, 1998, 21213, 3428, 1012, 2070, 3862, 2715, 2137, 7844, 17687, 8712, 2421, 1010, 1999, 2804, 2000, 10905, 1010, 9406, 1010, 13351, 1010, 10125, 2571, 5643, 1998, 22180, 5620, 1012, 17687, 2015, 2013, 2060, 3032, 2421, 1996, 8840, 2906, 1006, 2859, 1007, 1010, 4203, 1010, 9508, 1006, 2859, 1007, 1010, 2745, 5163, 1006, 4420, 1007, 1010, 24252, 1006, 2859, 1007, 1010, 5612, 1006, 2859, 1007, 1010, 2002, 5178, 2078, 1006, 2710, 1007, 1010, 13097, 26654, 1006, 2660, 1007, 1998, 5253, 9747, 1006, 2859, 1007, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 04:39:33 - INFO - __main__ - ***** Running training *****
01/31/2025 04:39:33 - INFO - __main__ - Num examples = 131754
01/31/2025 04:39:33 - INFO - __main__ - Num Epochs = 3
01/31/2025 04:39:33 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 04:39:33 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 04:39:33 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 04:39:33 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 04:39:34 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 04:39:34 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 04:39:34 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 04:39:47 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 04:39:47 - INFO - __main__ - Num examples = 12134
01/31/2025 04:39:47 - INFO - __main__ - Batch size = 8
01/31/2025 04:41:08 - INFO - __main__ - Evaluation metrics: {'exact': 69.46011959909038, 'f1': 73.01531981193507, 'total': 11873, 'HasAns_exact': 63.07354925775979, 'HasAns_f1': 70.19414509566509, 'HasAns_total': 5928, 'NoAns_exact': 75.82842724978974, 'NoAns_f1': 75.82842724978974, 'NoAns_total': 5945, 'best_exact': 69.46011959909038, 'best_exact_thresh': 0.0, 'best_f1': 73.01531981193502, 'best_f1_thresh': 0.0}
01/31/2025 04:42:21 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 04:42:24 - INFO - __main__ - Sample 114880 of the training set: {'input_ids': [101, 2054, 2024, 1996, 2069, 8915, 2080, 3470, 7495, 2000, 1996, 2329, 14195, 102, 1996, 2329, 14195, 2024, 1037, 2177, 1997, 3470, 2125, 1996, 2167, 1011, 2530, 3023, 1997, 6803, 2885, 2008, 8676, 1997, 1996, 3470, 1997, 2307, 3725, 1010, 3163, 1998, 2058, 2416, 4595, 3760, 14195, 1012, 4350, 1999, 1996, 2167, 4448, 1010, 1996, 3470, 2031, 1037, 2561, 2181, 1997, 3155, 22904, 1010, 18914, 2463, 2475, 1010, 1998, 1037, 4117, 2313, 1997, 2074, 2104, 3963, 2454, 1012, 2048, 11074, 2163, 2024, 2284, 2006, 1996, 3470, 1024, 3163, 1006, 2029, 4472, 5560, 2274, 1011, 4369, 2015, 1997, 1996, 2479, 2007, 1996, 2168, 2171, 1007, 1998, 1996, 2142, 2983, 1997, 2307, 3725, 1998, 2642, 3163, 1012, 1996, 2329, 14195, 2036, 2421, 2093, 4410, 12530, 15266, 1024, 1996, 8842, 1997, 2158, 1998, 1010, 2011, 4535, 1010, 1996, 15358, 2072, 7184, 1997, 3933, 1998, 1996, 15358, 2072, 7184, 1997, 24640, 1999, 1996, 3149, 3470, 1010, 2348, 1996, 3732, 2024, 2025, 8186, 1037, 2112, 1997, 1996, 13888, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 04:42:24 - INFO - __main__ - Sample 31938 of the training set: {'input_ids': [101, 2054, 2003, 1996, 7526, 2005, 6967, 1005, 1055, 3977, 2005, 5992, 6204, 7730, 1029, 102, 1996, 27012, 1997, 6967, 6576, 7607, 2049, 2152, 5992, 6204, 7730, 1006, 5354, 1012, 1020, 26306, 10790, 2575, 1055, 1013, 1049, 1007, 1998, 2947, 2036, 2152, 9829, 6204, 7730, 1010, 2029, 2024, 1996, 2117, 3284, 1006, 2000, 3165, 1007, 2426, 5760, 11970, 2012, 2282, 4860, 1012, 2023, 2003, 2138, 1996, 9507, 7730, 2000, 10496, 3665, 1999, 11970, 2012, 2282, 4860, 3262, 16896, 2013, 17501, 1997, 15057, 2006, 9829, 22755, 1997, 1996, 17779, 1010, 2029, 2024, 4659, 5410, 2005, 1037, 3730, 3384, 1012, 1996, 4555, 2566, 26770, 2783, 4304, 1997, 6967, 1999, 2330, 2250, 2003, 3155, 1017, 1012, 1015, 26306, 10790, 2575, 1037, 1013, 25525, 1997, 2892, 1011, 27197, 2181, 1010, 2682, 2029, 2009, 4269, 2000, 3684, 11664, 2135, 1012, 2004, 2007, 2060, 11970, 1010, 2065, 6967, 2003, 2872, 2114, 2178, 3384, 1010, 14891, 27760, 2278, 24625, 2097, 5258, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 16, 'end_positions': 17}.
01/31/2025 04:42:24 - INFO - __main__ - Sample 102062 of the training set: {'input_ids': [101, 2054, 4249, 2031, 4423, 1999, 10019, 1029, 102, 1996, 3010, 2374, 2492, 2003, 5018, 4210, 1006, 14989, 1049, 1007, 2146, 1998, 3515, 4210, 1006, 5354, 1049, 1007, 2898, 2007, 2203, 10019, 2322, 4210, 1006, 2324, 1049, 1007, 2784, 1010, 1998, 3125, 3210, 7287, 4210, 1006, 7886, 1049, 1007, 4237, 1012, 2012, 2169, 3125, 2240, 2003, 1037, 2275, 1997, 2871, 1011, 3329, 1011, 2152, 1006, 2260, 1049, 1007, 3125, 19894, 2015, 1010, 2029, 8676, 1997, 2048, 10051, 2015, 2587, 2011, 2019, 2324, 1015, 30070, 2475, 1011, 3329, 1011, 2146, 1006, 1019, 1012, 1020, 1049, 1007, 2892, 8237, 2029, 2003, 2184, 2519, 1006, 1017, 1049, 1007, 2682, 1996, 3125, 2240, 1012, 1996, 3125, 19894, 2015, 2089, 2022, 1044, 1011, 5044, 1006, 2119, 8466, 4964, 1999, 1996, 2598, 1007, 2348, 1999, 1996, 3020, 1011, 10250, 12322, 2890, 6479, 1996, 17372, 1011, 9292, 2640, 1006, 3569, 2011, 1037, 2309, 9203, 2695, 2369, 1996, 3125, 2240, 1010, 2061, 2008, 2169, 2695, 4627, 2184, 2519, 1006, 1017, 1049, 1007, 2682, 1996, 2598, 1007, 2003, 6871, 1012, 1996, 3903, 1997, 1996, 2492, 2024, 4417, 2011, 2317, 2217, 12735, 1010, 1996, 3125, 2240, 2003, 4417, 1999, 2317, 1010, 1998, 2317, 3210, 2024, 4567, 11457, 2135, 2408, 1996, 2492, 2296, 1019, 4210, 1006, 1018, 1012, 1020, 1049, 1007, 2013, 1996, 3125, 2240, 1012, 2122, 11457, 3210, 2024, 2170, 1000, 4220, 3210, 1000, 1998, 2411, 4417, 2007, 1996, 3292, 1999, 4210, 2013, 1998, 2019, 8612, 4197, 2646, 1996, 7205, 3125, 2240, 1012, 1999, 3025, 5109, 1010, 12563, 2020, 2025, 2109, 1998, 2296, 4220, 2240, 2001, 2788, 4417, 2007, 1996, 3292, 2000, 1996, 3125, 2240, 1010, 2164, 1996, 3125, 2240, 2993, 2029, 2001, 4417, 2007, 1037, 1000, 1014, 1000, 1025, 1999, 2087, 28244, 2651, 1010, 1996, 2184, 1011, 1010, 2322, 1011, 1010, 2382, 1011, 1010, 2871, 1011, 1010, 1998, 2753, 1011, 4220, 3210, 2024, 4417, 2007, 3616, 1010, 2007, 1996, 3125, 2240, 2823, 2108, 4417, 2007, 1037, 1000, 1043, 1000, 1012, 1996, 2803, 1006, 4583, 1011, 4220, 1007, 2240, 2788, 2003, 4417, 2007, 1037, 1000, 1039, 1000, 1012, 1000, 23325, 6017, 1000, 2024, 4993, 1999, 2317, 1010, 5903, 2000, 1996, 4220, 4270, 3210, 1010, 2012, 1015, 4220, 1006, 1014, 1012, 1023, 1049, 1007, 14025, 1010, 2484, 4210, 1006, 2538, 1012, 1023, 1049, 1007, 2013, 1996, 2217, 12735, 1012, 2006, 4249, 2008, 2031, 1037, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 04:42:25 - INFO - __main__ - ***** Running training *****
01/31/2025 04:42:25 - INFO - __main__ - Num examples = 131754
01/31/2025 04:42:25 - INFO - __main__ - Num Epochs = 3
01/31/2025 04:42:25 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 04:42:25 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 04:42:25 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 04:42:25 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 04:42:26 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 04:42:26 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 04:42:26 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 04:42:50 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 04:42:50 - INFO - __main__ - Num examples = 12134
01/31/2025 04:42:50 - INFO - __main__ - Batch size = 8
01/31/2025 04:44:11 - INFO - __main__ - Evaluation metrics: {'exact': 69.46011959909038, 'f1': 73.01531981193507, 'total': 11873, 'HasAns_exact': 63.07354925775979, 'HasAns_f1': 70.19414509566509, 'HasAns_total': 5928, 'NoAns_exact': 75.82842724978974, 'NoAns_f1': 75.82842724978974, 'NoAns_total': 5945, 'best_exact': 69.46011959909038, 'best_exact_thresh': 0.0, 'best_f1': 73.01531981193502, 'best_f1_thresh': 0.0}
01/31/2025 05:00:41 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 05:01:22 - INFO - __main__ - Sample 121481 of the training set: {'input_ids': [101, 2054, 2003, 1996, 2922, 6284, 2177, 1999, 15448, 1029, 102, 2055, 1019, 1003, 1997, 1996, 15448, 2078, 2313, 2024, 6284, 1012, 1996, 2922, 6284, 2177, 1999, 15448, 2003, 1996, 28616, 23615, 2080, 2111, 1012, 2037, 3700, 3668, 2013, 4880, 11503, 10464, 2078, 1010, 14373, 1010, 2000, 5673, 9026, 1010, 15448, 2247, 1996, 22529, 3023, 1012, 2045, 2003, 1037, 3128, 28616, 23615, 2080, 2653, 1010, 2021, 2312, 2967, 3713, 28616, 23615, 2080, 3023, 21414, 1010, 3009, 1010, 14115, 1998, 2060, 4155, 1012, 1996, 21414, 2394, 2234, 2055, 2083, 6976, 3967, 2007, 1996, 2329, 2040, 16844, 3550, 1996, 2181, 1012, 2116, 2024, 8135, 1012, 3151, 28616, 23615, 2080, 2554, 2001, 3811, 14336, 2007, 1037, 4225, 2576, 3252, 1012, 2045, 2001, 1037, 2332, 1010, 2021, 2002, 2106, 2025, 2031, 2561, 2373, 1012, 2612, 1010, 1996, 2373, 2001, 3975, 2090, 2370, 1010, 1037, 3099, 1010, 1037, 2236, 1010, 1998, 2011, 1996, 18171, 2015, 1010, 2019, 5902, 1012, 3439, 2592, 2006, 5465, 2003, 2411, 23649, 2011, 1996, 2755, 2008, 2116, 1997, 1996, 5465, 2020, 4100, 1011, 19336, 1012, 2178, 2350, 2177, 2003, 1996, 9815, 3070, 2532, 1006, 2030, 7680, 2226, 1007, 2111, 1010, 10320, 2070, 2184, 1010, 2199, 2111, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 30, 'end_positions': 32}.
01/31/2025 05:01:22 - INFO - __main__ - Sample 18408 of the training set: {'input_ids': [101, 2612, 1997, 2108, 2580, 2013, 2498, 9779, 13309, 1005, 1055, 4654, 9152, 19466, 2080, 3408, 2024, 2245, 2000, 2941, 4298, 2031, 2042, 5105, 2011, 2054, 1029, 102, 2116, 1997, 1996, 25396, 2015, 2008, 2031, 2042, 2641, 1006, 2411, 2011, 9779, 13309, 2370, 1007, 2004, 2616, 9530, 3597, 10985, 4654, 9152, 19466, 2080, 2071, 2092, 2031, 2042, 5105, 2011, 3097, 16105, 9289, 5167, 1010, 2005, 2742, 2616, 2013, 2845, 1010, 2446, 1010, 2413, 1010, 6983, 1010, 2394, 1998, 4467, 1012, 9779, 13309, 2018, 1037, 5041, 4556, 2495, 1998, 2354, 3418, 3306, 1010, 3763, 1998, 2413, 1012, 5136, 25223, 2213, 1520, 4126, 1521, 6431, 2394, 4126, 2030, 19982, 3490, 2863, 1520, 2000, 28887, 1010, 4487, 3736, 9397, 17597, 1521, 6431, 6983, 10722, 20936, 2696, 1520, 2000, 28887, 1010, 2000, 3648, 1521, 1006, 2122, 9779, 13309, 22556, 3711, 1999, 9779, 13309, 1521, 1055, 4885, 9206, 1007, 1012, 2122, 2616, 2453, 2022, 2488, 5240, 2004, 1037, 14099, 24491, 1997, 22822, 8458, 2080, 1011, 3042, 7712, 6789, 1997, 1037, 3097, 16105, 9289, 8875, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 59, 'end_positions': 62}.
01/31/2025 05:01:22 - INFO - __main__ - Sample 109582 of the training set: {'input_ids': [101, 2054, 2024, 14163, 7377, 7895, 2015, 1029, 102, 1999, 1996, 2627, 1010, 24209, 2102, 2497, 7890, 2072, 11117, 1998, 9152, 20722, 2015, 6296, 3324, 1010, 8160, 1998, 2273, 1997, 4144, 2013, 2367, 3033, 1997, 1996, 2088, 2083, 15694, 1012, 1996, 4525, 5636, 4666, 2759, 5084, 3451, 2824, 2107, 2004, 14163, 7377, 7895, 2015, 1006, 13805, 25353, 8737, 20049, 2050, 1007, 1012, 1996, 24209, 2102, 2497, 7890, 2072, 5321, 3391, 6628, 1996, 3930, 1997, 21079, 3490, 12454, 3906, 2877, 2000, 2573, 2107, 2004, 1996, 21079, 3490, 16137, 2532, 5737, 1998, 4487, 7447, 4623, 1010, 2029, 2024, 2426, 1996, 5700, 2800, 10485, 1999, 12454, 1012, 2474, 20715, 2102, 4895, 9152, 3736, 1010, 1037, 2338, 9227, 1999, 1996, 6286, 2301, 2012, 24209, 2102, 2497, 7890, 2072, 5434, 1010, 3397, 14253, 5265, 2007, 26309, 2005, 3595, 20233, 1998, 2358, 5714, 7068, 7666, 1999, 1996, 2789, 2433, 1997, 3418, 4424, 2840, 1012, 1996, 5853, 1997, 1996, 9152, 20722, 2015, 2387, 2116, 4706, 8818, 1998, 1996, 4955, 1997, 12454, 2004, 1037, 2653, 1997, 2457, 1010, 3447, 1998, 2495, 1012, 1999, 11617, 1010, 1037, 3074, 1997, 12454, 1043, 3270, 16739, 4623, 1010, 2315, 19739, 23858, 2906, 1011, 1041, 1011, 5003, 7317, 20784, 2050, 1010, 8786, 2011, 5003, 2232, 2474, 19062, 21790, 1517, 1996, 2034, 2931, 12454, 4802, 2000, 3965, 1037, 4487, 7447, 1517, 2001, 2405, 1999, 13624, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 55, 'end_positions': 59}.
01/31/2025 05:01:23 - INFO - __main__ - ***** Running training *****
01/31/2025 05:01:23 - INFO - __main__ - Num examples = 131754
01/31/2025 05:01:23 - INFO - __main__ - Num Epochs = 3
01/31/2025 05:01:23 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 05:01:23 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 05:01:23 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 05:01:23 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 05:01:24 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 05:01:24 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 05:01:24 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 05:01:36 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 05:01:36 - INFO - __main__ - Num examples = 1000
01/31/2025 05:01:36 - INFO - __main__ - Batch size = 8
01/31/2025 05:01:42 - INFO - __main__ - Evaluation metrics: {'exact': 68.5, 'f1': 71.53895271395267, 'total': 1000, 'HasAns_exact': 62.65060240963855, 'HasAns_f1': 68.75291709629059, 'HasAns_total': 498, 'NoAns_exact': 74.30278884462152, 'NoAns_f1': 74.30278884462152, 'NoAns_total': 502, 'best_exact': 68.5, 'best_exact_thresh': 0.0, 'best_f1': 71.53895271395265, 'best_f1_thresh': 0.0}
01/31/2025 05:03:14 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 05:04:19 - INFO - __main__ - Sample 75999 of the training set: {'input_ids': [101, 2054, 3257, 2106, 1996, 2167, 3792, 4296, 7949, 1029, 102, 2006, 2255, 2423, 1010, 10081, 1010, 2810, 2211, 2006, 1996, 17025, 1998, 15842, 4296, 2000, 7532, 1996, 3417, 2103, 1997, 17025, 2007, 1996, 2110, 3007, 1997, 15842, 1012, 1999, 9037, 1996, 2167, 3792, 4296, 2001, 2580, 2011, 2552, 1997, 1996, 6372, 2000, 7949, 2008, 4296, 2225, 2000, 27905, 1010, 2152, 2391, 1010, 1998, 5904, 1012, 2076, 1996, 2942, 2162, 1010, 1996, 17025, 1011, 2000, 1011, 15842, 7683, 1997, 1996, 4296, 2052, 2022, 8995, 2000, 1996, 8055, 2162, 3947, 1025, 6067, 12057, 2046, 17025, 2052, 2022, 2333, 2011, 4334, 2083, 15842, 2000, 1996, 8055, 3007, 1997, 6713, 1010, 3448, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 56, 'end_positions': 56}.
01/31/2025 05:04:19 - INFO - __main__ - Sample 21902 of the training set: {'input_ids': [101, 2043, 2967, 2008, 2024, 17379, 2594, 2000, 2169, 2060, 2433, 1037, 6056, 2005, 5023, 5114, 1010, 2009, 2003, 2170, 2054, 1029, 102, 2019, 4895, 14854, 2100, 4707, 2003, 1037, 6056, 2426, 9428, 17379, 2594, 2967, 2005, 4748, 21929, 2030, 5023, 5114, 1010, 3227, 2070, 6383, 2512, 1011, 10605, 2177, 5716, 7208, 2007, 2576, 4243, 1010, 17731, 4804, 1999, 3863, 2005, 1996, 11119, 3949, 1012, 2066, 15694, 1010, 4895, 14854, 2100, 21277, 2024, 2025, 9352, 6206, 1010, 2021, 4406, 15694, 1010, 2011, 2049, 11703, 22048, 3267, 1998, 2411, 2307, 3361, 4219, 1010, 2019, 4895, 14854, 2100, 4707, 2064, 2022, 2172, 2062, 4795, 2000, 1996, 2270, 3037, 1012, 2019, 2220, 2224, 1997, 1996, 2744, 2001, 2011, 2280, 2149, 2343, 10117, 1000, 11389, 1000, 8573, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 24, 'end_positions': 27}.
01/31/2025 05:04:19 - INFO - __main__ - Sample 4476 of the training set: {'input_ids': [101, 2429, 2000, 1996, 2137, 3075, 2523, 1010, 2054, 4635, 2106, 1996, 2338, 2031, 2426, 1996, 2087, 4703, 8315, 2808, 2013, 2456, 2000, 2268, 1029, 102, 2000, 3102, 1037, 19545, 9001, 2038, 2042, 1037, 3120, 1997, 3278, 6704, 2144, 2049, 2108, 1996, 3395, 1997, 9823, 2817, 2004, 2220, 2004, 3699, 1012, 1996, 2338, 1005, 1055, 5762, 22889, 9236, 1010, 11268, 7088, 3723, 1010, 1998, 3581, 6594, 1997, 9040, 2031, 2419, 2111, 2000, 4119, 2049, 6413, 2791, 1999, 8860, 1998, 12463, 2408, 1996, 2142, 2163, 1012, 1996, 2137, 3075, 2523, 2988, 2008, 2000, 3102, 1037, 19545, 9001, 2001, 2193, 2538, 1997, 1996, 2531, 2087, 4703, 8315, 2808, 1997, 2456, 1516, 2268, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 98, 'end_positions': 98}.
01/31/2025 05:04:20 - INFO - __main__ - ***** Running training *****
01/31/2025 05:04:20 - INFO - __main__ - Num examples = 131754
01/31/2025 05:04:20 - INFO - __main__ - Num Epochs = 3
01/31/2025 05:04:20 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 05:04:20 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 05:04:20 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 05:04:20 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 05:04:21 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 05:04:21 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 05:04:21 - INFO - __main__ - 
    fp16 without torch.compile
01/31/2025 05:04:28 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 05:04:28 - INFO - __main__ - Num examples = 1000
01/31/2025 05:04:28 - INFO - __main__ - Batch size = 8
01/31/2025 05:04:35 - INFO - __main__ - Evaluation metrics: {'exact': 68.5, 'f1': 71.53895271395267, 'total': 1000, 'HasAns_exact': 62.65060240963855, 'HasAns_f1': 68.75291709629059, 'HasAns_total': 498, 'NoAns_exact': 74.30278884462152, 'NoAns_f1': 74.30278884462152, 'NoAns_total': 502, 'best_exact': 68.5, 'best_exact_thresh': 0.0, 'best_f1': 71.53895271395265, 'best_f1_thresh': 0.0}
01/31/2025 06:26:13 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 06:26:58 - INFO - __main__ - Sample 129309 of the training set: {'input_ids': [101, 2040, 2109, 1996, 2744, 12795, 2000, 6235, 1996, 3019, 2381, 1997, 2158, 1029, 102, 24590, 2224, 1997, 1996, 2744, 2005, 2070, 1997, 1996, 3395, 3043, 4158, 3525, 1010, 2107, 2004, 1996, 2224, 2011, 16821, 14262, 6072, 1999, 9931, 2000, 6235, 1996, 3019, 2381, 1010, 2030, 5122, 12162, 6779, 1010, 1997, 2158, 1010, 2241, 2006, 12596, 13336, 1010, 1998, 1996, 4325, 1997, 1037, 3242, 1999, 12795, 1998, 3802, 7295, 9888, 1999, 7973, 2012, 1996, 2120, 2688, 1997, 3019, 2381, 1006, 2605, 1007, 2011, 3744, 3434, 20371, 2139, 24209, 4017, 2890, 7011, 8449, 2139, 7987, 10207, 1012, 2536, 2460, 1011, 2973, 4411, 1997, 21571, 2015, 2018, 2525, 2042, 2719, 1012, 1996, 18341, 3802, 7295, 12898, 5856, 4226, 2139, 3000, 1010, 1996, 2034, 2000, 2224, 3802, 7295, 6779, 1010, 2001, 2719, 1999, 10011, 1012, 2049, 2372, 2020, 3952, 3424, 1011, 8864, 10134, 1012, 2043, 8864, 2001, 8961, 1999, 2605, 1999, 7993, 1996, 18341, 2001, 4704, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 34, 'end_positions': 36}.
01/31/2025 06:26:58 - INFO - __main__ - Sample 128009 of the training set: {'input_ids': [101, 2040, 3569, 5978, 1999, 13040, 1029, 102, 1999, 16734, 2575, 1010, 1996, 2034, 2610, 14064, 1997, 5673, 2139, 11497, 2001, 8733, 1012, 2011, 1996, 5550, 2301, 1010, 2087, 2952, 9243, 2525, 2018, 2334, 3197, 2007, 2375, 7285, 4972, 1012, 2006, 2251, 1023, 1010, 14276, 1037, 5945, 3483, 2001, 2580, 1999, 1996, 2110, 1997, 21750, 29220, 2005, 8498, 2375, 1998, 2344, 1012, 1999, 13040, 1010, 1996, 5077, 2548, 2155, 7448, 2000, 4380, 1010, 2138, 1997, 1996, 2413, 5274, 1997, 5978, 1012, 2332, 16304, 6819, 2511, 1996, 1000, 13566, 27742, 16216, 7941, 2139, 14955, 24108, 1000, 1006, 2236, 2610, 13566, 11656, 1007, 2005, 9751, 1012, 2002, 2036, 2580, 1037, 2548, 2610, 3457, 2005, 5673, 2139, 11497, 1999, 12861, 1012, 1999, 10937, 1010, 2044, 4336, 1010, 2169, 2874, 2318, 10863, 2049, 2334, 1000, 2510, 2610, 1000, 1010, 2007, 2344, 6032, 8518, 1012, 1996, 2976, 4296, 2610, 2001, 2580, 1999, 8784, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 06:26:58 - INFO - __main__ - Sample 121079 of the training set: {'input_ids': [101, 2129, 2116, 2111, 2973, 1999, 2859, 2012, 1996, 2203, 1997, 1996, 3983, 2301, 1029, 102, 2859, 2036, 2211, 6114, 2013, 15986, 2058, 16340, 9513, 2076, 2023, 2558, 1012, 2313, 3930, 2001, 2358, 8490, 16885, 2005, 1996, 2034, 2431, 1997, 1996, 5550, 2301, 2349, 2000, 2942, 5233, 1998, 16311, 2015, 1010, 2021, 14165, 1998, 4722, 9211, 6360, 11674, 2023, 9874, 1012, 1996, 4955, 1997, 2047, 8765, 2013, 1996, 10925, 2107, 2004, 1996, 14557, 1998, 21443, 3039, 2019, 5301, 2833, 4425, 2004, 2092, 1010, 2061, 2008, 1996, 2561, 2313, 1997, 2859, 2076, 1996, 4985, 2301, 13212, 2098, 2013, 2531, 2454, 2000, 3998, 2454, 2111, 1012, 2574, 2035, 2800, 16439, 2001, 2109, 2039, 1010, 6932, 13193, 2000, 2147, 2412, 1011, 3760, 1998, 2062, 20531, 2499, 14811, 1012, 1996, 18816, 2319, 10052, 3750, 2320, 2022, 5302, 7231, 2094, 1996, 2406, 1005, 1055, 3663, 2011, 17674, 2075, 1000, 1996, 2313, 4247, 2000, 4982, 1010, 2021, 1996, 2455, 2515, 2025, 1012, 1000, 1996, 2069, 3588, 2112, 1997, 1996, 3400, 2008, 2018, 5424, 2571, 16439, 2001, 26650, 4360, 1010, 2073, 1996, 6941, 1997, 10147, 4115, 1998, 2002, 22360, 3070, 21786, 2018, 2042, 17692, 2125, 2004, 1037, 26650, 10759, 1012, 1996, 3750, 28447, 2005, 1996, 2034, 2051, 2008, 7658, 2822, 9272, 2020, 10386, 2000, 7392, 1012, 22235, 2020, 10386, 2011, 1996, 13282, 2013, 5153, 1996, 6645, 1997, 2037, 23562, 1010, 2130, 2046, 2060, 17450, 23562, 1998, 2013, 5153, 2046, 11265, 28173, 1006, 1996, 7658, 2822, 2324, 6941, 1007, 1998, 2020, 2445, 3809, 29115, 2065, 2027, 2106, 1999, 2344, 2000, 2562, 1996, 22235, 4055, 2114, 2169, 2060, 2000, 5770, 1996, 13282, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 100, 'end_positions': 101}.
01/31/2025 06:26:59 - INFO - __main__ - ***** Running training *****
01/31/2025 06:26:59 - INFO - __main__ - Num examples = 131754
01/31/2025 06:26:59 - INFO - __main__ - Num Epochs = 3
01/31/2025 06:26:59 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 06:26:59 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 06:26:59 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 06:26:59 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 06:27:01 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 06:27:01 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 06:30:19 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 06:31:04 - INFO - __main__ - Sample 57702 of the training set: {'input_ids': [101, 1996, 4151, 3142, 3400, 2349, 2000, 2049, 2116, 2367, 9187, 2015, 2001, 2464, 2004, 2054, 2828, 1997, 2554, 1029, 102, 2012, 1996, 2168, 2051, 1010, 3987, 15463, 2419, 2000, 1037, 2446, 17882, 1997, 3119, 1999, 1996, 11275, 2712, 1998, 3033, 1997, 2789, 2885, 2083, 1996, 7003, 5243, 4588, 2223, 1012, 2247, 1996, 3119, 5847, 1010, 7003, 5243, 4588, 3119, 3703, 2150, 6401, 1997, 1996, 2446, 3226, 1012, 2446, 2237, 2375, 1006, 2358, 18727, 28109, 1007, 2001, 3755, 2011, 1996, 3739, 1997, 2312, 1010, 4659, 7272, 2446, 7080, 1010, 2037, 3747, 1998, 2576, 2373, 1012, 2947, 2111, 2040, 2052, 2022, 2641, 1000, 7074, 1000, 1010, 2007, 1037, 2691, 3226, 1010, 2653, 1010, 1998, 2088, 8584, 2367, 2013, 2008, 1997, 1996, 4193, 3541, 7243, 1010, 16844, 3550, 6202, 4865, 2004, 2521, 2167, 1997, 2556, 1011, 2154, 2762, 2004, 12674, 1006, 1999, 5120, 1007, 1010, 8947, 1006, 1999, 4701, 1007, 1010, 1998, 1058, 2100, 11755, 1006, 2085, 1999, 3607, 1007, 1012, 1996, 7003, 5243, 4588, 2223, 2001, 2025, 7580, 2446, 1999, 2151, 5636, 3168, 1024, 2116, 4865, 2040, 2587, 1996, 2223, 2020, 2648, 1996, 4151, 3142, 3400, 1998, 1037, 2193, 1997, 2068, 2089, 2069, 11853, 2022, 7356, 2004, 2446, 1012, 1996, 3400, 2993, 2001, 2025, 4498, 2446, 2593, 1012, 2009, 2018, 1037, 4800, 1011, 5636, 1998, 4800, 1011, 17002, 8787, 3252, 1010, 2070, 1997, 1996, 3760, 5636, 6447, 1998, 4155, 2109, 2012, 2367, 2335, 2020, 3803, 1010, 3059, 1010, 2413, 1010, 5569, 1998, 3907, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 210, 'end_positions': 217}.
01/31/2025 06:31:04 - INFO - __main__ - Sample 16646 of the training set: {'input_ids': [101, 2129, 2116, 14549, 2097, 2022, 2109, 2000, 3231, 1037, 2047, 2291, 1997, 9163, 14828, 1029, 102, 1996, 2093, 6745, 14549, 2097, 10776, 13595, 5604, 1997, 1037, 2047, 2291, 1997, 9163, 14828, 1998, 6970, 1011, 5871, 6971, 1010, 1998, 2707, 4346, 9163, 2578, 2043, 3201, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 18, 'end_positions': 18}.
01/31/2025 06:31:04 - INFO - __main__ - Sample 14300 of the training set: {'input_ids': [101, 2129, 2116, 2519, 2106, 1996, 2034, 6302, 9864, 4471, 3604, 1029, 102, 2006, 2238, 2538, 1010, 6756, 1010, 4330, 1005, 1055, 3353, 11860, 1037, 9949, 2376, 7026, 4471, 1037, 6196, 3292, 1010, 2013, 1996, 4412, 1997, 1996, 5951, 2082, 1999, 2899, 1010, 1040, 1012, 1039, 1012, 1010, 2000, 4330, 2012, 1996, 3332, 1997, 2010, 5911, 1010, 2070, 19883, 3620, 1006, 6352, 3027, 1007, 2185, 1010, 2539, 2086, 2077, 1996, 2034, 2376, 2557, 21670, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 61, 'end_positions': 61}.
01/31/2025 06:31:05 - INFO - __main__ - ***** Running training *****
01/31/2025 06:31:05 - INFO - __main__ - Num examples = 131754
01/31/2025 06:31:05 - INFO - __main__ - Num Epochs = 3
01/31/2025 06:31:05 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 06:31:05 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 06:31:05 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 06:31:05 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 06:31:06 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 06:31:06 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 06:37:54 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 06:37:57 - INFO - __main__ - Sample 6755 of the training set: {'input_ids': [101, 2070, 3469, 15910, 2071, 2202, 2129, 2146, 2000, 2468, 12581, 9677, 1029, 102, 1999, 4968, 6077, 1010, 4424, 16736, 4269, 2000, 4148, 2105, 2287, 2416, 2000, 4376, 2706, 2005, 2119, 3767, 1998, 3801, 1010, 2348, 2023, 2064, 2022, 8394, 2127, 2039, 2000, 2048, 2086, 2214, 2005, 2070, 2312, 15910, 1012, 2023, 2003, 1996, 2051, 2012, 2029, 2931, 6077, 2097, 2031, 2037, 2034, 9765, 13288, 5402, 1012, 2027, 2097, 3325, 4745, 9765, 13288, 12709, 12170, 11639, 28488, 1010, 2076, 2029, 1996, 2303, 20776, 2005, 10032, 1012, 2012, 1996, 4672, 1997, 1996, 5402, 1010, 3801, 2097, 2272, 2046, 9765, 7946, 1010, 2108, 10597, 1998, 8186, 28667, 22048, 2000, 8872, 9513, 1012, 2138, 1996, 1051, 3567, 5788, 1998, 2024, 5214, 1997, 2108, 10768, 28228, 28931, 2005, 1037, 2733, 2044, 1051, 19722, 13490, 1010, 2009, 2003, 2825, 2005, 1037, 2931, 2000, 6775, 2007, 2062, 2084, 2028, 3287, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 43, 'end_positions': 44}.
01/31/2025 06:37:57 - INFO - __main__ - Sample 86512 of the training set: {'input_ids': [101, 2054, 2106, 9162, 26572, 2050, 2228, 2055, 3854, 16371, 17545, 1029, 102, 2843, 8167, 13865, 13926, 8049, 2649, 3854, 22860, 2004, 1996, 1000, 7915, 2568, 1045, 2412, 2777, 1000, 1010, 1998, 6213, 22953, 19779, 5488, 2626, 1000, 2002, 2001, 1996, 12266, 4355, 2158, 1045, 2412, 2354, 1010, 2302, 6453, 1012, 2002, 2001, 1037, 11067, 1012, 1000, 2577, 26572, 2050, 1010, 3005, 8921, 2012, 3802, 2232, 10204, 3854, 22860, 3230, 2004, 1037, 3076, 1010, 2056, 1000, 5206, 2001, 1996, 2069, 3076, 1045, 2001, 2412, 4452, 1997, 1012, 2065, 1999, 1996, 2607, 1997, 1037, 8835, 1045, 3090, 2019, 4895, 19454, 7178, 3291, 1010, 1996, 9592, 2020, 2002, 1005, 1040, 2272, 2000, 2033, 2012, 1996, 2203, 1997, 1996, 8835, 2007, 1996, 3143, 5576, 8040, 3089, 12820, 2006, 1037, 7540, 1997, 3259, 1012, 1000, 11085, 15530, 23412, 1037, 2466, 2409, 2011, 6141, 18236, 1010, 7175, 1996, 3177, 1997, 3854, 22860, 1005, 1055, 16268, 1010, 2043, 8307, 2356, 3854, 22860, 2000, 9611, 1996, 3297, 4875, 11989, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 76, 'end_positions': 86}.
01/31/2025 06:37:57 - INFO - __main__ - Sample 461 of the training set: {'input_ids': [101, 2019, 2742, 1997, 1037, 2299, 6461, 2875, 1037, 3287, 4378, 2003, 2054, 1029, 102, 2016, 2038, 2363, 2522, 1011, 3015, 6495, 2005, 2087, 1997, 1996, 2774, 2680, 2007, 10461, 1005, 1055, 2775, 1998, 2014, 3948, 4073, 1012, 2014, 2220, 2774, 2020, 7714, 5533, 1998, 2931, 1011, 23011, 11773, 9265, 2066, 1000, 2981, 2308, 1000, 1998, 1000, 12084, 1000, 1010, 2021, 2044, 1996, 2707, 1997, 2014, 3276, 2007, 6108, 1062, 2016, 23946, 2000, 2062, 2158, 1011, 25069, 11971, 2015, 2107, 2004, 1000, 23488, 1016, 1057, 1000, 1012, 20773, 2038, 2036, 2363, 2522, 1011, 5155, 6495, 2005, 2087, 1997, 1996, 2636, 1999, 2029, 2016, 2038, 2042, 2920, 1010, 2926, 2076, 2014, 3948, 4073, 1012, 2174, 1010, 2016, 2515, 2025, 5675, 2618, 10299, 2841, 1010, 2021, 4050, 3310, 2039, 2007, 16106, 1998, 4784, 2076, 2537, 1010, 6631, 2068, 2007, 6443, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 82, 'end_positions': 84}.
01/31/2025 06:37:58 - INFO - __main__ - ***** Running training *****
01/31/2025 06:37:58 - INFO - __main__ - Num examples = 131754
01/31/2025 06:37:58 - INFO - __main__ - Num Epochs = 3
01/31/2025 06:37:58 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 06:37:58 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 06:37:58 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 06:37:58 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 06:37:59 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 06:37:59 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 06:48:47 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 06:49:45 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 06:52:14 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 06:52:18 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 06:57:52 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 06:57:55 - INFO - __main__ - Sample 24592 of the training set: {'input_ids': [101, 1996, 2744, 2632, 2361, 1010, 2632, 2213, 1010, 2632, 4783, 2030, 2632, 5051, 5218, 2000, 2054, 1999, 2715, 4155, 1029, 102, 1999, 2715, 4155, 1996, 2744, 2632, 2361, 1010, 2632, 2213, 1010, 2632, 4783, 2030, 2632, 5051, 5218, 2000, 1037, 15400, 24813, 1999, 1996, 10348, 4655, 2917, 1996, 21193, 1010, 2025, 1996, 11373, 1012, 2019, 2632, 2361, 5218, 2000, 1037, 2152, 3137, 20787, 2073, 17188, 2024, 2579, 2000, 2022, 26627, 2076, 1996, 2621, 2706, 1998, 2073, 10974, 25684, 2064, 2022, 2179, 1010, 1998, 1996, 2744, 1000, 1996, 13698, 1000, 1010, 7727, 2000, 1996, 4020, 1010, 2003, 1037, 28616, 3630, 5017, 1012, 1996, 2744, 2005, 1996, 3137, 11373, 9783, 2011, 3842, 1998, 2653, 1024, 2616, 2107, 2004, 7109, 1010, 12849, 12439, 1010, 21025, 14376, 2884, 1010, 13183, 2480, 1010, 1998, 15214, 2024, 2109, 1999, 2446, 4092, 4655, 1024, 18318, 1010, 27263, 1010, 21418, 1998, 9932, 25698, 6216, 1999, 2413, 4092, 4655, 1025, 1998, 10125, 1010, 27263, 3597, 2030, 25022, 2863, 1999, 3059, 4092, 4655, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 40, 'end_positions': 53}.
01/31/2025 06:57:55 - INFO - __main__ - Sample 79383 of the training set: {'input_ids': [101, 2040, 2106, 4789, 10045, 8970, 2000, 10104, 2344, 2493, 1029, 102, 2178, 2040, 5201, 6022, 2000, 1996, 21244, 1997, 1996, 2344, 2003, 4789, 2271, 10045, 1010, 1996, 2069, 2711, 1997, 1996, 2558, 2000, 2022, 2445, 1996, 10439, 8411, 3508, 1000, 2307, 1000, 1012, 2010, 3747, 2006, 1996, 12865, 2566, 4168, 4383, 3053, 2296, 7814, 1997, 10104, 2166, 1012, 4789, 2001, 1037, 7155, 1010, 9667, 1010, 28625, 21197, 2121, 1010, 17200, 1010, 6259, 3213, 1010, 14925, 27417, 2923, 1010, 1998, 11125, 1012, 2104, 1996, 20153, 1997, 14910, 8296, 1997, 10900, 1010, 4789, 27992, 1996, 8882, 1997, 2913, 2005, 2035, 10104, 2493, 1010, 3107, 17484, 2000, 1996, 9823, 1998, 15113, 2094, 1996, 2147, 1997, 9253, 24759, 22436, 5130, 1010, 2107, 2004, 5436, 13429, 1012, 5262, 1010, 2009, 2001, 1996, 4228, 2086, 1997, 2147, 2589, 2011, 2726, 1037, 12519, 3022, 1998, 2370, 1006, 13412, 2629, 1516, 13029, 2549, 1007, 2008, 3039, 2005, 1996, 10502, 1997, 10488, 16033, 9834, 2937, 2817, 1999, 1996, 8882, 1997, 10104, 2816, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 103, 'end_positions': 103}.
01/31/2025 06:57:55 - INFO - __main__ - Sample 116169 of the training set: {'input_ids': [101, 2054, 2003, 1996, 2236, 2512, 1011, 3430, 6553, 3193, 1997, 2019, 4874, 1029, 102, 1037, 2117, 3291, 2007, 3430, 2964, 2003, 2008, 2009, 14485, 2015, 1996, 5197, 1997, 4262, 1012, 2009, 5927, 2296, 4874, 2004, 5664, 1998, 16246, 2013, 2035, 2060, 5200, 1012, 2169, 4874, 2003, 3432, 2019, 1999, 8743, 18856, 24237, 1997, 3043, 2008, 2003, 2069, 27223, 3141, 2000, 2060, 2477, 1012, 1996, 2801, 1997, 3043, 2004, 3078, 3084, 2111, 2228, 1997, 5200, 2004, 2108, 24670, 3584, 1999, 2051, 1998, 2686, 1010, 1998, 2025, 9352, 3141, 2000, 2505, 1012, 2021, 1999, 23257, 1005, 1055, 3193, 1010, 4262, 2202, 1037, 3078, 2535, 1010, 3383, 2130, 2062, 2590, 2084, 1996, 2128, 20051, 2050, 3209, 1012, 1037, 3076, 2635, 3964, 1999, 2028, 1997, 23257, 1005, 1055, 2991, 4814, 4280, 2626, 2008, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 06:57:56 - INFO - __main__ - ***** Running training *****
01/31/2025 06:57:56 - INFO - __main__ - Num examples = 131754
01/31/2025 06:57:56 - INFO - __main__ - Num Epochs = 3
01/31/2025 06:57:56 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 06:57:56 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 06:57:56 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 06:57:56 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 06:57:57 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 06:57:57 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'mapping', 'tb_writer', 'force_calib_once'}
01/31/2025 06:58:24 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 06:58:34 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 06:58:40 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 06:58:43 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 06:59:01 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 06:59:01 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:08:47 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 07:08:49 - INFO - __main__ - Sample 40937 of the training set: {'input_ids': [101, 2247, 2007, 2060, 5876, 1010, 2129, 2146, 2442, 1037, 4539, 2022, 5117, 2000, 2031, 4621, 5894, 1029, 102, 1996, 2329, 4233, 1000, 4621, 5894, 1000, 1010, 3574, 1996, 7998, 2012, 2029, 1037, 3282, 2071, 8116, 1037, 2186, 1997, 10986, 2114, 1037, 3048, 4539, 1025, 2023, 2071, 2022, 27570, 2011, 4555, 19976, 2770, 2051, 2004, 2092, 2004, 1996, 3282, 1005, 1055, 10673, 1012, 2011, 1996, 2397, 5687, 1996, 2329, 6210, 2001, 1000, 2008, 4578, 2012, 2029, 1037, 3495, 8455, 4539, 2012, 4278, 5601, 1006, 1027, 4185, 2509, 1012, 1020, 2463, 1013, 1044, 1007, 2064, 2022, 5117, 2005, 2322, 3823, 2077, 1996, 3282, 6561, 3963, 5445, 6678, 1000, 1012, 2174, 1010, 4621, 5894, 2005, 3082, 9779, 4409, 2001, 5360, 2011, 2512, 1011, 19630, 5876, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 96, 'end_positions': 98}.
01/31/2025 07:08:49 - INFO - __main__ - Sample 51684 of the training set: {'input_ids': [101, 2054, 2003, 1996, 3800, 1997, 1996, 2047, 19184, 102, 1999, 3054, 2249, 1010, 1996, 3192, 2623, 1999, 2049, 2047, 1000, 2300, 1010, 18723, 1010, 19548, 5656, 19184, 1000, 2008, 2049, 4804, 2085, 7679, 3952, 2006, 18723, 1010, 3391, 1999, 4942, 1011, 24505, 3088, 1998, 2148, 4021, 1010, 2138, 3229, 2000, 5301, 18723, 2003, 7290, 1999, 2216, 4655, 1012, 2037, 3946, 1011, 2437, 3579, 2038, 2042, 2144, 2249, 2006, 18723, 2671, 1998, 2974, 1006, 1000, 10938, 8082, 6786, 1000, 1007, 1010, 6959, 4275, 2012, 4094, 1010, 3923, 18723, 6089, 1010, 2311, 5157, 2005, 18723, 1010, 10903, 1998, 9312, 2004, 2092, 2004, 3343, 1010, 12288, 1998, 4806, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 31, 'end_positions': 57}.
01/31/2025 07:08:49 - INFO - __main__ - Sample 26757 of the training set: {'input_ids': [101, 2054, 2095, 2001, 18699, 2243, 2304, 1998, 2317, 2580, 1029, 102, 1996, 19364, 7570, 6499, 18712, 12352, 2072, 1006, 18699, 2243, 1010, 1996, 2900, 5062, 3840, 1007, 2211, 9283, 2470, 2000, 1000, 19829, 1996, 8050, 7337, 1997, 2678, 1998, 2614, 10266, 2007, 1996, 2274, 2529, 9456, 1000, 1999, 3546, 1010, 2044, 1996, 5522, 3783, 1012, 18699, 2243, 2275, 2041, 2000, 3443, 2019, 10751, 9189, 2291, 2008, 3092, 2039, 4577, 2172, 3020, 1999, 20714, 5852, 2084, 23961, 11020, 1005, 1055, 3130, 9188, 1000, 10751, 9189, 1000, 1012, 2023, 2047, 2291, 1010, 18699, 2243, 3609, 1010, 2580, 1999, 3285, 1010, 2443, 11176, 2629, 3210, 1010, 1037, 1019, 1024, 1017, 7814, 6463, 1998, 3438, 22100, 25416, 21898, 3446, 1012, 1996, 2554, 1997, 4367, 3861, 1998, 2547, 6145, 1006, 15488, 13876, 2063, 1007, 1010, 3753, 2011, 2798, 18353, 9695, 1010, 2150, 1996, 5604, 1998, 2817, 3691, 2005, 10751, 9189, 2974, 1999, 1996, 2248, 4258, 1012, 15488, 13876, 2063, 2052, 3231, 10751, 9189, 3001, 2013, 2367, 3316, 2013, 2296, 9530, 3401, 11444, 3468, 7339, 1010, 2021, 1996, 3291, 1997, 11566, 1996, 2367, 11630, 17808, 1996, 2974, 2005, 2116, 2086, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 07:08:50 - INFO - __main__ - ***** Running training *****
01/31/2025 07:08:50 - INFO - __main__ - Num examples = 131754
01/31/2025 07:08:50 - INFO - __main__ - Num Epochs = 3
01/31/2025 07:08:50 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 07:08:50 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 07:08:50 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 07:08:50 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 07:08:52 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 07:08:52 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'force_calib_once', 'mapping', 'tb_writer'}
01/31/2025 07:10:06 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 07:10:16 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 07:10:18 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:10:22 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 07:10:39 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 07:10:39 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:14:25 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 07:14:27 - INFO - __main__ - Sample 83897 of the training set: {'input_ids': [101, 2054, 2001, 1996, 3007, 1997, 2605, 1999, 4529, 1029, 102, 1999, 4529, 1010, 2206, 1996, 5036, 1997, 18346, 1010, 1996, 2103, 2001, 2717, 4183, 12926, 2000, 2605, 1999, 10388, 2007, 1057, 1012, 1055, 1012, 2343, 23954, 4267, 1005, 1055, 1000, 7426, 2685, 1000, 2302, 1037, 9782, 1012, 1996, 3058, 1997, 1996, 8775, 2001, 22307, 19620, 2135, 2511, 2006, 19125, 2154, 1012, 2009, 2003, 21888, 3251, 1037, 9782, 1999, 18104, 2052, 2031, 3092, 1999, 2605, 1005, 1055, 7927, 2144, 1996, 2576, 4243, 2358, 3089, 6455, 2005, 2019, 8392, 24922, 2030, 1037, 4434, 2000, 2605, 14729, 2069, 2005, 1037, 2235, 10817, 1997, 4494, 1999, 1996, 2197, 14365, 9153, 2290, 2004, 2092, 2004, 1999, 1996, 2334, 3864, 1012, 1996, 25520, 10450, 2319, 8285, 3630, 23738, 2015, 2040, 2020, 4013, 2413, 2018, 2180, 2116, 4494, 1999, 1996, 2062, 3541, 3033, 1997, 1996, 2555, 1998, 2060, 4865, 2144, 1996, 18985, 1997, 1996, 2555, 2011, 2762, 1999, 7428, 1012, 1996, 2929, 2318, 2007, 1996, 2034, 2602, 2005, 1996, 14365, 9153, 2290, 1025, 2216, 2700, 2020, 2170, 1000, 4649, 2139, 18780, 2229, 6186, 6790, 7442, 2015, 1000, 1010, 1998, 2127, 1996, 2991, 1997, 22029, 1999, 6193, 1010, 2027, 2020, 1996, 2069, 11964, 2700, 2011, 1996, 25520, 10450, 6962, 2000, 1996, 2446, 3323, 9694, 1996, 2709, 1997, 2216, 6500, 2000, 2605, 1012, 2012, 1996, 2197, 14365, 9153, 2290, 2602, 1999, 18104, 1998, 2049, 23275, 1010, 1996, 3154, 4791, 2020, 1996, 2591, 8037, 1025, 1996, 2103, 2001, 1996, 3831, 3007, 1997, 1996, 2555, 1010, 2001, 9613, 2011, 2116, 7074, 2805, 2011, 1996, 2430, 2231, 1999, 4068, 1998, 2049, 29571, 4610, 6296, 2116, 7074, 1012, 2023, 2071, 4863, 1996, 4489, 2090, 1996, 3541, 3789, 1998, 1996, 2028, 1999, 18104, 1012, 2044, 1996, 2162, 1010, 2116, 7074, 2187, 18104, 1998, 2253, 2067, 2000, 2762, 1025, 2070, 1997, 2068, 2020, 17787, 2011, 1996, 10575, 2030, 10016, 2011, 1996, 4397, 2805, 4614, 1012, 1996, 3828, 12119, 6771, 2001, 14954, 1999, 1996, 3638, 2426, 1996, 25520, 10450, 6962, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 07:14:27 - INFO - __main__ - Sample 18324 of the training set: {'input_ids': [101, 2040, 2003, 3625, 2005, 1996, 4325, 1997, 1996, 10947, 2030, 2705, 9888, 1029, 102, 2715, 12029, 2030, 2705, 9888, 2003, 2241, 2006, 1996, 10947, 2030, 2705, 9888, 2580, 2011, 20424, 6289, 7389, 2015, 1999, 1996, 2117, 2431, 1997, 1996, 3708, 2301, 2241, 2006, 6983, 2030, 2705, 9888, 1012, 1996, 3080, 2030, 2705, 9888, 2009, 2999, 2001, 2580, 1999, 1996, 5550, 2301, 2011, 3841, 13512, 26142, 2005, 11246, 4173, 1998, 8968, 7109, 5575, 2241, 2006, 3115, 2446, 2030, 2705, 9888, 1012, 3041, 3015, 1999, 12029, 2018, 2011, 1998, 2312, 2109, 2019, 4748, 21929, 2030, 2705, 9888, 2241, 2006, 3763, 1998, 2690, 2659, 2446, 2030, 2705, 9888, 1012, 2070, 8092, 1997, 1996, 3115, 2446, 2030, 2705, 9888, 1517, 2005, 2742, 1010, 3015, 1005, 1059, 1005, 1013, 1005, 1059, 1005, 2612, 1997, 1005, 1058, 1005, 1013, 1005, 1058, 1005, 19035, 2092, 2046, 1996, 5687, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 30, 'end_positions': 33}.
01/31/2025 07:14:27 - INFO - __main__ - Sample 54976 of the training set: {'input_ids': [101, 2029, 3212, 1005, 1055, 3719, 3832, 2000, 2886, 26018, 1029, 102, 2044, 2086, 1997, 19905, 1010, 2093, 2307, 4204, 1010, 3607, 1010, 1996, 2142, 2983, 1998, 2605, 1010, 2787, 2000, 18793, 1999, 1996, 4736, 1998, 2169, 3842, 2741, 1037, 3212, 2000, 5483, 1012, 2206, 2739, 2008, 4117, 6188, 1516, 6811, 25515, 2020, 2183, 2000, 2886, 1996, 3306, 2479, 1997, 26018, 1010, 1996, 6035, 4170, 16618, 1996, 6188, 1516, 6811, 4170, 2012, 6583, 10755, 5740, 1012, 2044, 1037, 2733, 1011, 2146, 3233, 7245, 1010, 1037, 2645, 2211, 2029, 4504, 1999, 1996, 6215, 1997, 1996, 6188, 1516, 6811, 4170, 1012, 1037, 2413, 15372, 2486, 2001, 14501, 2000, 28589, 1996, 13982, 1997, 1996, 6811, 2390, 2013, 1996, 21877, 4135, 26029, 14183, 1010, 2096, 1996, 13176, 8979, 2000, 1996, 4110, 2112, 1997, 2430, 5483, 2011, 11517, 1012, 2004, 1037, 2765, 1997, 2086, 1997, 19905, 1010, 1996, 17235, 13013, 3306, 2110, 2001, 2633, 3858, 2104, 1996, 2414, 8778, 1999, 9500, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 67, 'end_positions': 69}.
01/31/2025 07:14:28 - INFO - __main__ - ***** Running training *****
01/31/2025 07:14:28 - INFO - __main__ - Num examples = 131754
01/31/2025 07:14:28 - INFO - __main__ - Num Epochs = 3
01/31/2025 07:14:28 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 07:14:28 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 07:14:28 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 07:14:28 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 07:14:30 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 07:14:30 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'tb_writer', 'force_calib_once', 'mapping'}
01/31/2025 07:14:35 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 07:14:45 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 07:14:47 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:14:52 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 07:15:09 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 07:15:09 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:17:12 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 07:17:12 - INFO - __main__ - Num examples = 12134
01/31/2025 07:17:12 - INFO - __main__ - Batch size = 8
01/31/2025 07:22:52 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 07:22:55 - INFO - __main__ - Sample 44540 of the training set: {'input_ids': [101, 2043, 2106, 3870, 2707, 2000, 4562, 1037, 5435, 1997, 2548, 2608, 1029, 102, 2013, 2538, 2258, 3646, 2127, 2014, 16993, 1010, 3870, 1005, 1055, 2608, 5031, 1997, 1037, 8840, 10431, 3351, 7682, 1996, 2548, 5435, 1997, 2608, 1997, 1996, 2142, 2983, 4489, 2094, 2007, 1037, 3830, 1997, 2093, 2685, 23157, 1010, 1996, 2803, 2391, 7682, 1037, 15588, 3123, 1998, 1996, 2034, 1998, 2353, 1037, 2892, 1997, 2358, 2577, 1012, 2588, 2014, 16993, 1010, 2016, 7900, 1996, 2536, 2608, 2014, 2269, 2218, 2004, 11074, 1012, 1996, 3035, 2036, 14882, 2548, 4781, 1998, 3167, 9245, 2005, 2224, 1999, 1996, 2142, 2983, 1010, 2710, 1010, 2660, 1010, 2047, 3414, 1010, 9156, 1010, 16893, 1010, 1998, 6974, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 15, 'end_positions': 17}.
01/31/2025 07:22:55 - INFO - __main__ - Sample 21873 of the training set: {'input_ids': [101, 2054, 2048, 4127, 1997, 27748, 2003, 1996, 2248, 2451, 2667, 2000, 2131, 21651, 2004, 3584, 1029, 102, 1999, 3522, 2086, 1010, 1031, 2043, 1029, 1033, 1996, 2248, 2451, 2038, 2081, 4073, 2000, 8627, 3032, 2000, 4487, 24137, 7405, 2618, 3161, 1998, 13135, 27748, 1998, 2000, 4297, 20026, 14776, 2068, 2004, 3584, 18421, 1012, 1031, 11091, 2734, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 40, 'end_positions': 43}.
01/31/2025 07:22:55 - INFO - __main__ - Sample 95806 of the training set: {'input_ids': [101, 2043, 2001, 1996, 2236, 2695, 2436, 2328, 1029, 102, 2464, 1999, 2049, 7289, 2591, 6123, 1010, 3306, 6308, 4294, 5015, 1037, 2047, 3602, 1997, 17540, 27840, 1998, 19355, 1999, 2270, 3121, 1999, 3725, 2105, 9807, 2004, 2019, 23617, 1997, 14594, 16742, 2006, 1996, 2552, 1997, 2586, 1010, 1996, 18813, 5233, 1010, 1998, 1996, 18856, 22591, 3126, 2005, 2576, 5290, 1012, 2009, 2001, 2000, 2022, 2520, 22745, 1005, 1055, 3045, 2640, 2005, 1996, 2270, 2971, 2005, 22501, 2267, 1010, 4729, 2008, 2623, 1996, 3306, 2806, 2001, 2000, 2022, 1996, 7444, 8909, 18994, 1999, 4294, 1012, 22745, 1998, 2728, 15081, 2063, 2253, 2006, 2000, 3857, 2070, 1997, 1996, 2087, 2590, 3121, 1997, 1996, 3690, 1010, 2164, 1996, 3004, 2548, 1010, 29456, 3871, 1006, 13040, 1516, 5641, 1007, 1010, 1996, 2236, 2695, 2436, 1006, 11617, 1516, 2756, 1007, 1998, 1996, 2329, 2688, 1006, 12522, 1516, 4466, 1007, 1010, 22745, 2118, 2267, 2414, 1006, 11931, 1516, 2382, 1007, 1998, 1996, 2120, 3916, 1006, 10212, 1516, 4229, 1007, 1012, 1999, 3885, 1010, 2726, 5226, 1006, 16496, 1516, 8517, 1007, 1010, 1999, 5792, 2007, 1996, 3324, 4080, 4267, 1006, 15051, 1516, 7993, 1007, 1998, 6621, 2520, 3766, 1006, 19916, 1516, 11523, 1007, 2580, 10490, 1998, 3121, 1997, 2248, 7784, 1025, 1996, 7641, 6104, 2012, 3499, 4710, 1006, 12094, 1007, 1998, 1996, 1006, 2548, 1007, 2152, 2082, 1999, 5928, 1006, 12522, 1516, 2756, 1007, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 132, 'end_positions': 134}.
01/31/2025 07:22:56 - INFO - __main__ - ***** Running training *****
01/31/2025 07:22:56 - INFO - __main__ - Num examples = 131754
01/31/2025 07:22:56 - INFO - __main__ - Num Epochs = 3
01/31/2025 07:22:56 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 07:22:56 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 07:22:56 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 07:22:56 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 07:22:57 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 07:22:57 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'tb_writer', 'mapping', 'force_calib_once'}
01/31/2025 07:23:09 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 07:23:20 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 07:23:22 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:23:26 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 07:23:43 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 07:23:43 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:25:00 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 07:25:00 - INFO - __main__ - Num examples = 12134
01/31/2025 07:25:00 - INFO - __main__ - Batch size = 8
01/31/2025 07:33:04 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 07:33:06 - INFO - __main__ - Sample 101875 of the training set: {'input_ids': [101, 2029, 2406, 18966, 27891, 1997, 14815, 1011, 14535, 5005, 1029, 102, 1999, 2762, 1010, 2967, 2107, 2004, 16417, 3367, 3126, 10431, 3207, 11265, 19761, 10421, 2078, 2764, 1037, 4310, 2806, 1997, 3919, 2189, 1010, 16911, 14815, 1011, 14535, 5005, 1010, 25628, 5693, 1998, 2179, 5200, 1012, 2372, 1997, 2008, 2177, 2052, 2101, 2175, 2006, 2000, 20880, 2007, 2372, 1997, 1996, 5798, 2283, 1012, 1999, 4380, 1010, 1996, 2695, 1011, 7196, 3496, 3473, 2044, 1996, 4245, 1997, 21133, 2401, 2600, 2007, 4996, 2107, 2004, 4190, 2401, 2080, 27929, 1010, 3007, 1999, 24108, 2140, 1998, 20228, 15878, 2063, 12726, 1998, 2059, 1996, 3098, 1997, 1996, 2189, 2252, 10602, 2938, 2050, 1999, 7509, 9094, 1010, 2007, 4490, 2066, 6644, 2063, 1039, 1010, 14841, 10230, 1010, 6986, 29323, 2316, 1010, 3062, 5498, 1998, 21442, 27524, 10980, 2015, 1010, 2004, 8832, 2006, 22754, 2066, 1996, 4424, 2166, 1997, 1996, 9576, 2015, 1998, 1996, 6583, 2080, 4400, 1013, 6583, 2080, 7509, 9094, 2186, 1010, 2207, 1999, 1996, 2866, 1010, 2762, 1998, 4380, 1010, 4414, 1012, 1031, 11091, 2734, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 07:33:06 - INFO - __main__ - Sample 121746 of the training set: {'input_ids': [101, 2054, 2003, 1996, 2561, 2193, 1997, 2825, 5034, 18259, 6087, 1029, 102, 2061, 2008, 1996, 4555, 2193, 1997, 6087, 2064, 2022, 14125, 22296, 2006, 2115, 3274, 3898, 1010, 2169, 3609, 2038, 2042, 2445, 1037, 3642, 2193, 1010, 2030, 5034, 18259, 1010, 2029, 4136, 2115, 3274, 1996, 8015, 1997, 1996, 2417, 1010, 2665, 1998, 2630, 6177, 1997, 2008, 3609, 1012, 1996, 8015, 1997, 2169, 6922, 2003, 7594, 2006, 1037, 4094, 1997, 5717, 2000, 20637, 1010, 2029, 2965, 1996, 3143, 2862, 2950, 2385, 1010, 6255, 2581, 1010, 20294, 5664, 6087, 1998, 13178, 1012, 1996, 5034, 18259, 2193, 1997, 5760, 2417, 1010, 2005, 2742, 1010, 2003, 20637, 1010, 4002, 1010, 4002, 1010, 2029, 2965, 1996, 2417, 6922, 2003, 2012, 2049, 4555, 8015, 1010, 1998, 2045, 2003, 2053, 2665, 2030, 2630, 1012, 1996, 5034, 18259, 2193, 2005, 11466, 2003, 10545, 1010, 2322, 1010, 3438, 1010, 2029, 2965, 2008, 1996, 2417, 2003, 3621, 2625, 6387, 1998, 3568, 9904, 1010, 2045, 2003, 2070, 2665, 1010, 2029, 12671, 2009, 2646, 4589, 1025, 1998, 2045, 2003, 1037, 3469, 3815, 1997, 2630, 1010, 2029, 3084, 2009, 3621, 2630, 1011, 8766, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 81, 'end_positions': 86}.
01/31/2025 07:33:06 - INFO - __main__ - Sample 52212 of the training set: {'input_ids': [101, 2054, 2515, 1037, 4678, 8738, 2467, 8676, 1997, 2005, 24209, 12502, 2015, 1029, 102, 1996, 24209, 12502, 2003, 1037, 2235, 2000, 5396, 1011, 7451, 1010, 26483, 3973, 11401, 4743, 1012, 1999, 2049, 3019, 4044, 1010, 2009, 2003, 2179, 1999, 5747, 2100, 3182, 1010, 1999, 5931, 20331, 1010, 2426, 4910, 8765, 1010, 1998, 1999, 2060, 3182, 2007, 9742, 3104, 1012, 2009, 14172, 2006, 8079, 1010, 9728, 1010, 1998, 2060, 2235, 25700, 1012, 2108, 1037, 4321, 2598, 1011, 13160, 1010, 6754, 16843, 2271, 4743, 1010, 4968, 3370, 1997, 1996, 24209, 12502, 2001, 2025, 3697, 1010, 2348, 2116, 1997, 2049, 3748, 16160, 2024, 6025, 1999, 16187, 1012, 2009, 2001, 2124, 2000, 1996, 23437, 2146, 2077, 1996, 5508, 1997, 21868, 1998, 2001, 8212, 1999, 7632, 10624, 25643, 18757, 2013, 24368, 2629, 4647, 1012, 2009, 13447, 2408, 5279, 1999, 6565, 19311, 2015, 1998, 1996, 5055, 2071, 2823, 2022, 3856, 2039, 2125, 1996, 2598, 2011, 2192, 1012, 2122, 2020, 1996, 2691, 24209, 12502, 1006, 26046, 14287, 7646, 26046, 14287, 7646, 1007, 1010, 2021, 2715, 4968, 4383, 19311, 2015, 2024, 3262, 1997, 2887, 24209, 12502, 1006, 26046, 14287, 7646, 14855, 26029, 5555, 1007, 2029, 2001, 2763, 4968, 4383, 2004, 2220, 2004, 1996, 6252, 2301, 4748, 1999, 2900, 1012, 2027, 2020, 2761, 2921, 2004, 2299, 12887, 1010, 1998, 2027, 2024, 2245, 2000, 2031, 2042, 5570, 2109, 1999, 2299, 15795, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 07:33:07 - INFO - __main__ - ***** Running training *****
01/31/2025 07:33:07 - INFO - __main__ - Num examples = 131754
01/31/2025 07:33:07 - INFO - __main__ - Num Epochs = 3
01/31/2025 07:33:07 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 07:33:07 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 07:33:07 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 07:33:07 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 07:33:08 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 07:33:08 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'force_calib_once', 'tb_writer', 'mapping'}
01/31/2025 07:33:09 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 07:33:19 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 07:33:21 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:33:24 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 07:33:42 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 07:33:42 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:33:42 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 07:33:43 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 07:33:43 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 07:33:47 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 07:33:47 - INFO - __main__ - Num examples = 12134
01/31/2025 07:33:47 - INFO - __main__ - Batch size = 8
01/31/2025 07:41:07 - INFO - __main__ - Evaluation metrics: {'exact': 1.3223279710266993, 'f1': 4.682662806759511, 'total': 11873, 'HasAns_exact': 0.20242914979757085, 'HasAns_f1': 6.932735409017512, 'HasAns_total': 5928, 'NoAns_exact': 2.4390243902439024, 'NoAns_f1': 2.4390243902439024, 'NoAns_total': 5945, 'best_exact': 50.07159100480081, 'best_exact_thresh': 0.0, 'best_f1': 50.076392220240706, 'best_f1_thresh': 0.0}
01/31/2025 07:42:18 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 07:42:21 - INFO - __main__ - Sample 6810 of the training set: {'input_ids': [101, 2054, 2003, 1996, 3469, 4175, 2005, 3616, 1997, 6077, 2641, 2000, 3769, 9869, 1996, 4774, 1029, 102, 1996, 3795, 3899, 2313, 2003, 4358, 2012, 25621, 2454, 1024, 14993, 2241, 2006, 1037, 13338, 16134, 1010, 2004, 4941, 2000, 2060, 10035, 2073, 1996, 16134, 2038, 2025, 2042, 2081, 2800, 1516, 2035, 3899, 2313, 10035, 2024, 2241, 2006, 3164, 2529, 2313, 7939, 24279, 1998, 2455, 3594, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 25, 'end_positions': 26}.
01/31/2025 07:42:21 - INFO - __main__ - Sample 129981 of the training set: {'input_ids': [101, 1999, 2054, 2095, 2106, 20351, 2468, 4642, 1029, 102, 1996, 2231, 1997, 9213, 2003, 1037, 4992, 2231, 1999, 1996, 2976, 3252, 1997, 4501, 1010, 2003, 2241, 1999, 15036, 1010, 1996, 3007, 1997, 1996, 9213, 2874, 1012, 1996, 2708, 2704, 1997, 9213, 1006, 4642, 1007, 2003, 2700, 2011, 1996, 4992, 3320, 1997, 1996, 9213, 2000, 3710, 2004, 1996, 2132, 1997, 1996, 4992, 2231, 1999, 9213, 1010, 4501, 1012, 1996, 2783, 2708, 2704, 2003, 7890, 3676, 2480, 20351, 1010, 2040, 2150, 1996, 2708, 2704, 1997, 9213, 2004, 2108, 5854, 2044, 3099, 1005, 1055, 3627, 3225, 2013, 2423, 2337, 2268, 2000, 2382, 2233, 2268, 1012, 6920, 2288, 2128, 1011, 2700, 2004, 1037, 2765, 1997, 2340, 2089, 2286, 3864, 1012, 1996, 4992, 3320, 1997, 1996, 9213, 2003, 1037, 4895, 5555, 28990, 6372, 1997, 2700, 4505, 1997, 1996, 2874, 1997, 9213, 1010, 2029, 2003, 2284, 1999, 15036, 1999, 2789, 4501, 1012, 1996, 3320, 2001, 2511, 2104, 3720, 10114, 1997, 1996, 4552, 1997, 4501, 2004, 2383, 1037, 2561, 1997, 4261, 2487, 4272, 1010, 2007, 5764, 4272, 9235, 2005, 2308, 1998, 2809, 9235, 2005, 2512, 1011, 7486, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 97, 'end_positions': 97}.
01/31/2025 07:42:21 - INFO - __main__ - Sample 16278 of the training set: {'input_ids': [101, 2054, 6433, 2000, 2739, 17058, 8141, 1996, 4773, 16602, 2064, 1005, 1056, 5047, 3495, 1029, 102, 2023, 2832, 4269, 2043, 1996, 5310, 20407, 1037, 6375, 7692, 8840, 11266, 2953, 1006, 24471, 2140, 1007, 1010, 2005, 2742, 8299, 1024, 1013, 1013, 4372, 1012, 16948, 1012, 8917, 1013, 1010, 2046, 1996, 16602, 1012, 1996, 17576, 1997, 1996, 24471, 2140, 1010, 1996, 6375, 7692, 8909, 4765, 18095, 2030, 24471, 2072, 1010, 16463, 2129, 1996, 24471, 2140, 2097, 2022, 10009, 1012, 1996, 2087, 4141, 2109, 2785, 1997, 24471, 2072, 4627, 2007, 8299, 1024, 1998, 14847, 1037, 7692, 2000, 2022, 5140, 2058, 1996, 23760, 18209, 4651, 8778, 1006, 8299, 1007, 1012, 2116, 16602, 2015, 2036, 2490, 1037, 3528, 1997, 2060, 17576, 2229, 1010, 2107, 2004, 16770, 1024, 2005, 16770, 1010, 3027, 2361, 1024, 2005, 1996, 5371, 4651, 8778, 1010, 1998, 5371, 1024, 2005, 2334, 6764, 1012, 17576, 2229, 2008, 1996, 4773, 16602, 3685, 3495, 5047, 2024, 2411, 4375, 2125, 2000, 2178, 4646, 4498, 1012, 2005, 2742, 1010, 5653, 3406, 1024, 24471, 2483, 2024, 2788, 2979, 2000, 1996, 5310, 1005, 1055, 12398, 1041, 1011, 5653, 4646, 1010, 1998, 2739, 1024, 24471, 2483, 2024, 2979, 2000, 1996, 5310, 1005, 1055, 12398, 2739, 17058, 8068, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 07:42:22 - INFO - __main__ - ***** Running training *****
01/31/2025 07:42:22 - INFO - __main__ - Num examples = 131754
01/31/2025 07:42:22 - INFO - __main__ - Num Epochs = 3
01/31/2025 07:42:22 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 07:42:22 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 07:42:22 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 07:42:22 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 07:42:23 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 07:42:23 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'tb_writer', 'mapping', 'force_calib_once'}
01/31/2025 07:42:23 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 07:42:33 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 07:42:35 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:42:39 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 07:42:57 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 07:42:57 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:42:57 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 07:42:57 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 07:42:57 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 07:42:58 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 07:42:58 - INFO - __main__ - Num examples = 12134
01/31/2025 07:42:58 - INFO - __main__ - Batch size = 8
01/31/2025 07:48:35 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 07:48:38 - INFO - __main__ - Sample 12263 of the training set: {'input_ids': [101, 2040, 2001, 1996, 3354, 2704, 2040, 3818, 2446, 28044, 1029, 102, 1996, 3354, 5227, 2000, 3693, 10079, 10375, 1999, 1996, 10530, 1997, 1996, 4068, 3034, 1997, 2254, 1516, 2337, 4051, 1012, 3354, 3097, 2704, 9587, 10994, 4492, 2081, 10340, 2000, 2031, 2762, 2128, 19496, 10451, 1998, 3864, 2005, 1037, 6090, 1011, 2446, 2231, 1010, 2104, 3785, 1997, 10534, 1997, 1996, 2176, 4204, 8749, 1998, 2446, 21083, 1010, 2021, 2035, 2020, 4188, 2011, 1996, 2060, 3097, 7767, 1010, 10634, 2229, 1006, 3915, 1007, 1010, 10267, 1006, 2866, 1007, 1998, 7226, 23505, 1006, 2605, 1007, 1012, 10340, 2005, 1996, 28044, 1997, 2762, 2020, 2498, 2047, 1024, 3041, 2006, 2322, 2233, 3999, 1010, 7566, 2055, 1037, 2446, 28044, 1010, 7531, 2011, 1996, 27084, 24164, 2094, 1005, 13125, 3602, 1005, 1010, 3092, 2044, 1996, 2142, 2983, 1010, 2605, 1010, 1998, 1996, 2142, 2163, 7278, 2008, 1037, 10562, 2762, 2323, 2025, 2022, 8699, 1998, 2323, 2022, 2489, 2000, 3693, 1996, 2647, 4721, 2451, 1998, 4373, 2213, 1012, 2508, 14145, 1006, 3915, 1007, 1010, 2040, 2777, 1999, 3000, 2007, 10267, 1010, 16298, 21126, 1998, 2728, 8040, 28600, 2319, 1006, 2605, 1007, 1010, 19768, 2008, 1000, 1996, 4874, 2323, 2022, 2000, 4468, 6594, 2007, 1996, 12513, 1998, 2000, 2811, 2006, 1996, 2647, 3639, 2451, 1000, 1012, 2429, 2000, 2198, 11721, 14141, 2483, 1000, 2045, 2001, 2210, 21970, 1999, 2530, 15433, 2000, 8849, 2023, 3749, 1000, 2013, 10331, 1012, 2096, 5272, 23381, 14233, 9912, 19514, 2008, 16298, 21126, 1005, 1055, 10652, 2008, 1523, 8699, 3989, 2965, 3354, 3989, 1524, 2001, 1996, 2364, 5387, 1999, 1996, 13893, 1997, 1996, 3354, 10340, 1010, 16298, 21126, 2036, 8615, 2008, 16905, 2453, 2031, 4504, 1999, 1996, 2203, 1997, 1996, 3729, 2226, 1005, 1055, 13811, 1999, 1996, 21122, 6155, 15900, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 35, 'end_positions': 37}.
01/31/2025 07:48:38 - INFO - __main__ - Sample 67193 of the training set: {'input_ids': [101, 2054, 2003, 3393, 4328, 13433, 3490, 7011, 20763, 1005, 1055, 2597, 2012, 26236, 2194, 2304, 4519, 1029, 102, 3393, 4328, 13433, 3490, 7011, 20763, 2003, 1037, 2472, 1998, 17334, 2040, 2003, 4069, 7587, 2007, 2010, 3153, 2194, 5003, 2226, 1012, 6606, 29464, 28578, 2401, 1005, 1055, 2194, 2304, 4519, 2038, 2036, 2363, 2248, 10761, 2007, 7562, 2000, 2885, 1998, 2047, 2259, 1012, 5099, 6154, 2038, 2018, 1037, 3278, 4254, 2006, 15075, 2078, 3226, 1012, 2429, 2000, 5736, 11796, 23508, 5572, 2072, 4213, 1010, 8065, 2013, 1996, 2118, 1997, 7359, 2012, 2158, 10441, 1010, 1000, 5099, 6154, 3226, 1999, 3327, 2003, 2759, 5921, 15075, 2078, 3360, 1012, 1000, 2066, 2200, 2116, 2060, 3032, 1010, 5099, 6154, 2189, 2003, 2759, 1012, 1999, 2804, 1010, 1996, 8346, 1997, 5099, 6154, 3787, 2046, 15075, 2078, 4535, 2036, 1000, 3231, 14144, 2000, 1996, 4651, 8010, 1997, 1996, 3153, 3596, 3209, 1010, 1000, 1998, 2000, 1996, 1000, 13782, 2083, 2029, 2111, 1998, 2035, 2037, 25405, 3716, 3604, 1012, 1000, 3153, 2119, 1999, 2049, 3151, 2433, 1998, 2049, 2062, 2715, 3596, 2038, 2815, 1037, 2430, 3451, 9598, 2000, 15075, 3619, 1010, 2926, 19634, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 07:48:38 - INFO - __main__ - Sample 62264 of the training set: {'input_ids': [101, 2040, 2003, 1996, 2343, 1997, 1996, 22953, 6968, 1005, 1055, 2866, 2407, 1029, 102, 1996, 6234, 3426, 1997, 1996, 12925, 2003, 10599, 1010, 2007, 2116, 15957, 8951, 1996, 4288, 1997, 2702, 14468, 7486, 2011, 5636, 10958, 10023, 3170, 2044, 1996, 9040, 1998, 4028, 1997, 1037, 10958, 10023, 3170, 2450, 2004, 1996, 2364, 3426, 1012, 2878, 4731, 2031, 2042, 1000, 11703, 20592, 1000, 1012, 2058, 3998, 3506, 1998, 1037, 2193, 1997, 2270, 3121, 2031, 2042, 10958, 5422, 1012, 2429, 2000, 27112, 1047, 10606, 1010, 1996, 2343, 1997, 1996, 14468, 20996, 12053, 3148, 5502, 2866, 1006, 22953, 6968, 1007, 1010, 2004, 1997, 2654, 2238, 2262, 1010, 13757, 20996, 12053, 16303, 2031, 2042, 2730, 1010, 1015, 1010, 3263, 2024, 4394, 1010, 1998, 2062, 2084, 3770, 1010, 2199, 2031, 2042, 12936, 1012, 2429, 2000, 1996, 12620, 4614, 1010, 1996, 4808, 1010, 2090, 5636, 10958, 10023, 3170, 7992, 2015, 1998, 20996, 12053, 3148, 7486, 1010, 2187, 6275, 2111, 2757, 1010, 6584, 5229, 1010, 1998, 5190, 1997, 5014, 3908, 1012, 2009, 12936, 2062, 2084, 4720, 1010, 2199, 2111, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 80, 'end_positions': 82}.
01/31/2025 07:48:39 - INFO - __main__ - ***** Running training *****
01/31/2025 07:48:39 - INFO - __main__ - Num examples = 131754
01/31/2025 07:48:39 - INFO - __main__ - Num Epochs = 3
01/31/2025 07:48:39 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 07:48:39 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 07:48:39 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 07:48:39 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 07:48:40 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 07:48:40 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'force_calib_once', 'tb_writer', 'mapping'}
01/31/2025 07:48:40 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 07:48:50 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 07:48:52 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:48:56 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 07:49:14 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 07:49:14 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:49:14 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 07:49:14 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 07:49:14 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 07:49:14 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 07:49:14 - INFO - __main__ - Num examples = 12134
01/31/2025 07:49:14 - INFO - __main__ - Batch size = 8
01/31/2025 07:57:14 - INFO - __main__ - Evaluation metrics: {'exact': 1.3223279710266993, 'f1': 4.682662806759511, 'total': 11873, 'HasAns_exact': 0.20242914979757085, 'HasAns_f1': 6.932735409017512, 'HasAns_total': 5928, 'NoAns_exact': 2.4390243902439024, 'NoAns_f1': 2.4390243902439024, 'NoAns_total': 5945, 'best_exact': 50.07159100480081, 'best_exact_thresh': 0.0, 'best_f1': 50.076392220240706, 'best_f1_thresh': 0.0}
01/31/2025 07:58:55 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 07:58:57 - INFO - __main__ - Sample 60136 of the training set: {'input_ids': [101, 2054, 2095, 2106, 2116, 3655, 1999, 16215, 12228, 2401, 4558, 3904, 1997, 2037, 2313, 1029, 102, 2076, 1996, 2690, 5535, 1010, 16215, 12228, 2401, 2001, 4350, 2012, 1996, 3675, 2090, 15139, 1998, 13838, 6500, 1010, 4417, 2011, 1996, 7842, 9453, 2314, 1012, 1996, 9808, 3215, 6340, 26896, 2290, 2929, 2419, 2000, 1996, 27574, 1997, 13838, 2111, 2090, 1996, 6252, 1998, 1996, 6122, 2301, 2104, 2446, 3627, 1012, 1996, 2313, 3930, 3445, 2076, 1996, 4985, 2301, 1998, 4370, 2152, 2127, 2088, 2162, 1045, 1010, 2077, 2009, 9784, 2306, 1996, 3983, 2301, 1998, 2904, 2000, 1037, 6689, 2144, 2901, 1012, 2144, 1996, 2927, 1997, 3923, 6648, 2105, 8905, 1010, 1996, 16215, 12228, 2937, 3655, 2031, 3020, 3930, 6165, 24501, 2361, 1012, 3760, 6165, 1997, 6689, 2084, 3541, 2752, 1006, 2116, 4731, 2439, 2431, 1997, 2037, 2313, 2144, 3925, 1010, 6168, 1996, 5221, 3655, 1006, 9413, 24428, 1998, 27510, 1007, 2562, 3652, 1007, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 07:58:57 - INFO - __main__ - Sample 61450 of the training set: {'input_ids': [101, 2040, 2001, 1996, 2201, 2995, 2630, 4056, 2000, 1029, 102, 1999, 2238, 3069, 1010, 11284, 2207, 2014, 2353, 2996, 2201, 1010, 2995, 2630, 1010, 2029, 2001, 4427, 2011, 1998, 4056, 2000, 5977, 9502, 1012, 5291, 2962, 2932, 2001, 3227, 7622, 2007, 1996, 3947, 1010, 3015, 2008, 1996, 2201, 1000, 2614, 1031, 1055, 1033, 2004, 2065, 2009, 3310, 2013, 1996, 2540, 1000, 1012, 2009, 4504, 1999, 2093, 3895, 2437, 2009, 2000, 2193, 1011, 2028, 2006, 1996, 4908, 2980, 2531, 1024, 1000, 2444, 2000, 2425, 1000, 1010, 1000, 13008, 2123, 1005, 1056, 25250, 1000, 1998, 1000, 2330, 2115, 2540, 1000, 1010, 1998, 2048, 2062, 2327, 1011, 2274, 3895, 1024, 1000, 2995, 2630, 1000, 1998, 1000, 2474, 25340, 14753, 6590, 1000, 1012, 1996, 2201, 9370, 1996, 6093, 1999, 2058, 2654, 3032, 4969, 1010, 2019, 15741, 6344, 2012, 1996, 2051, 1010, 1998, 2150, 2014, 2190, 1011, 4855, 2996, 2201, 1997, 2014, 2476, 2000, 2023, 3058, 2007, 4341, 1997, 2423, 2454, 1012, 1999, 1996, 2168, 2095, 1010, 11284, 5652, 1999, 1996, 11321, 6090, 7228, 2143, 8344, 4474, 1010, 2005, 2029, 2016, 2001, 3018, 1996, 3585, 20710, 2361, 9766, 2400, 2005, 1000, 5409, 3883, 1000, 1012, 2016, 2081, 2014, 8900, 2834, 1999, 1037, 2537, 1997, 2585, 10958, 4783, 1005, 1055, 13020, 1998, 3419, 1011, 3419, 1025, 1996, 2143, 1998, 2377, 2119, 2522, 1011, 5652, 9502, 1012, 1996, 2279, 2095, 1010, 11284, 2001, 2956, 1999, 1996, 2143, 2040, 1005, 1055, 2008, 2611, 1012, 2016, 5201, 2176, 2774, 2000, 2049, 6050, 1010, 2164, 1996, 2516, 2650, 1998, 1000, 4786, 1037, 23960, 1000, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 32, 'end_positions': 33}.
01/31/2025 07:58:57 - INFO - __main__ - Sample 43322 of the training set: {'input_ids': [101, 2040, 2001, 1996, 7786, 1997, 1996, 2032, 13380, 4221, 2983, 2012, 1996, 2051, 1029, 102, 1996, 2983, 2003, 3855, 1999, 1996, 2566, 11514, 7393, 1997, 1996, 9413, 22123, 13492, 11219, 2712, 2004, 2019, 2590, 3006, 2173, 2005, 11554, 1010, 2029, 2001, 15612, 2802, 1996, 3418, 2088, 1012, 17712, 17421, 2001, 2012, 1996, 2051, 5451, 2011, 1062, 2891, 12902, 2229, 1010, 2040, 2036, 9950, 1996, 3417, 1997, 4748, 15859, 2015, 1012, 1996, 17712, 17421, 4221, 11117, 19601, 3119, 2011, 12927, 2075, 2037, 2219, 17712, 17421, 4221, 9598, 1012, 1996, 2110, 2036, 2511, 2049, 2002, 3351, 22847, 2058, 1996, 13993, 2983, 1997, 13970, 4095, 1998, 5570, 3133, 1996, 4331, 1997, 1996, 12028, 2006, 1996, 13771, 6000, 1010, 2776, 8402, 2049, 3627, 2058, 1996, 2555, 2007, 1996, 9187, 1997, 1996, 2032, 13380, 4221, 2983, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 07:58:58 - INFO - __main__ - ***** Running training *****
01/31/2025 07:58:58 - INFO - __main__ - Num examples = 131754
01/31/2025 07:58:58 - INFO - __main__ - Num Epochs = 3
01/31/2025 07:58:58 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 07:58:58 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 07:58:58 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 07:58:58 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 07:59:00 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 07:59:00 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'tb_writer', 'mapping', 'force_calib_once'}
01/31/2025 07:59:00 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 07:59:10 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 07:59:12 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:59:16 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 07:59:33 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 07:59:33 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 07:59:33 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 07:59:33 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 07:59:33 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 07:59:34 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 07:59:34 - INFO - __main__ - Num examples = 12134
01/31/2025 07:59:34 - INFO - __main__ - Batch size = 8
01/31/2025 08:01:17 - INFO - __main__ - Evaluation metrics: {'exact': 72.54274404110166, 'f1': 75.95472124253871, 'total': 11873, 'HasAns_exact': 72.8744939271255, 'HasAns_f1': 79.70823301495648, 'HasAns_total': 5928, 'NoAns_exact': 72.21194280908327, 'NoAns_f1': 72.21194280908327, 'NoAns_total': 5945, 'best_exact': 72.54274404110166, 'best_exact_thresh': 0.0, 'best_f1': 75.95472124253871, 'best_f1_thresh': 0.0}
01/31/2025 08:04:56 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 08:04:59 - INFO - __main__ - Sample 72460 of the training set: {'input_ids': [101, 2040, 2106, 3312, 27177, 2918, 2010, 2147, 2125, 1997, 1029, 102, 1999, 9176, 1010, 5588, 7522, 16270, 6305, 11865, 4135, 2361, 7367, 29033, 19845, 2015, 12099, 4359, 1996, 5258, 7389, 5666, 1997, 16405, 2121, 4842, 2389, 9016, 2011, 3432, 9034, 11572, 2000, 9378, 2037, 2398, 2077, 7052, 2000, 2308, 1999, 27162, 1012, 2023, 5456, 3653, 13701, 2094, 1996, 16216, 10867, 3399, 1997, 4295, 1012, 2174, 1010, 7367, 29033, 19845, 2015, 1005, 9556, 2020, 2025, 12315, 2011, 2010, 16682, 1998, 2234, 2046, 2224, 2069, 2007, 15636, 2011, 2329, 9431, 3312, 27177, 1010, 2040, 1999, 6725, 4928, 1996, 6481, 1997, 3424, 3366, 18409, 1012, 27177, 1005, 1055, 2147, 2001, 2241, 2006, 1996, 2590, 9556, 2011, 2413, 21477, 3434, 19351, 3126, 1012, 19351, 3126, 2001, 2583, 2000, 4957, 12702, 21759, 7088, 19230, 2007, 4295, 1010, 4329, 6026, 4200, 1012, 2002, 2036, 14917, 2028, 1997, 1996, 2087, 2590, 4725, 1999, 4652, 3512, 4200, 1010, 2043, 1999, 6756, 2002, 2550, 1037, 17404, 2114, 10958, 20536, 1012, 19351, 3126, 8826, 1996, 2832, 1997, 19351, 9496, 9276, 1010, 2000, 2393, 4652, 1996, 3659, 1997, 4295, 2083, 6501, 1998, 2060, 9440, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 116, 'end_positions': 118}.
01/31/2025 08:04:59 - INFO - __main__ - Sample 86384 of the training set: {'input_ids': [101, 2129, 2146, 2106, 2009, 2202, 3854, 22860, 2000, 3275, 1037, 3433, 2000, 3399, 1997, 12958, 2791, 1029, 102, 2021, 3854, 22860, 1010, 2040, 2018, 4194, 2012, 1996, 3519, 1010, 4484, 2010, 4476, 2004, 2019, 7107, 17191, 2228, 2121, 1010, 1998, 1999, 2625, 2084, 1037, 3204, 2001, 2583, 2000, 10639, 2000, 2643, 2884, 2370, 2019, 5875, 9509, 1997, 2010, 9872, 1024, 8419, 2008, 1996, 5156, 22260, 18994, 12070, 3001, 2024, 4039, 2000, 10580, 2037, 2219, 18700, 1012, 2174, 1010, 2643, 2884, 2018, 2525, 3603, 2023, 9509, 1010, 2085, 2124, 2004, 2010, 2117, 12958, 2791, 9872, 1998, 2741, 3854, 22860, 1037, 17463, 6657, 2102, 1997, 2010, 3720, 4820, 2119, 12958, 2791, 9872, 2015, 1012, 3854, 22860, 8969, 2643, 2884, 1005, 1055, 9470, 1999, 2010, 2279, 3661, 1012, 2002, 2196, 2245, 2172, 1997, 1000, 1996, 2137, 2291, 1997, 6815, 3167, 9470, 2005, 2673, 1012, 1000, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 42, 'end_positions': 45}.
01/31/2025 08:04:59 - INFO - __main__ - Sample 65536 of the training set: {'input_ids': [101, 2040, 2001, 1996, 3478, 4944, 1997, 1996, 3506, 1997, 3323, 1029, 102, 1996, 3506, 1997, 3323, 1999, 2414, 2011, 2909, 2798, 6287, 2007, 20769, 2011, 1037, 2350, 16258, 21576, 1997, 1996, 2220, 7788, 6308, 1010, 11668, 2057, 14510, 16405, 11528, 1010, 2003, 2019, 2742, 1997, 1996, 7788, 6308, 2806, 2013, 2049, 3041, 2558, 1999, 1996, 2117, 4284, 1997, 1996, 3708, 2301, 1012, 4973, 2013, 1996, 2152, 6652, 7788, 2558, 2421, 2577, 7664, 3660, 1005, 1055, 2640, 2005, 1996, 4789, 3986, 1999, 2414, 1010, 1998, 2520, 12136, 3790, 1005, 1055, 4970, 2012, 17710, 3468, 2267, 1010, 4345, 1012, 2013, 1996, 2117, 2431, 1997, 1996, 3708, 2301, 9921, 2009, 2150, 2062, 2691, 1999, 3725, 2005, 9253, 1011, 7788, 2000, 2022, 2109, 1999, 1996, 2640, 1997, 2512, 1011, 12301, 1998, 2512, 1011, 10605, 3121, 4127, 1012, 7788, 4751, 2130, 2211, 2000, 3711, 1999, 2551, 1011, 2465, 3847, 11683, 4942, 5332, 10521, 2098, 2011, 29291, 1010, 2295, 2445, 1996, 10961, 1010, 2625, 4703, 2084, 1999, 1996, 2640, 1997, 3356, 1998, 2690, 1011, 2465, 3847, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 08:05:00 - INFO - __main__ - ***** Running training *****
01/31/2025 08:05:00 - INFO - __main__ - Num examples = 131754
01/31/2025 08:05:00 - INFO - __main__ - Num Epochs = 3
01/31/2025 08:05:00 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 08:05:00 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 08:05:00 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 08:05:00 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 08:05:01 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 08:05:01 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'mapping', 'tb_writer', 'force_calib_once'}
01/31/2025 08:05:01 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 08:05:11 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 08:05:13 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:05:17 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 08:05:35 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 08:05:35 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:05:35 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 08:05:35 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 08:05:35 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 08:05:36 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 08:05:36 - INFO - __main__ - Num examples = 12134
01/31/2025 08:05:36 - INFO - __main__ - Batch size = 8
01/31/2025 08:06:15 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 08:06:17 - INFO - __main__ - Sample 127601 of the training set: {'input_ids': [101, 2040, 9339, 1996, 2240, 1997, 22677, 2087, 5546, 1029, 102, 1996, 4748, 15489, 2229, 6958, 2036, 2018, 2116, 5097, 1012, 1996, 2128, 1011, 5456, 1997, 3418, 10485, 2716, 1037, 2062, 13769, 1998, 8321, 3716, 1997, 3418, 9569, 2816, 2107, 2004, 8680, 5397, 7088, 6491, 1010, 1998, 9253, 24759, 22436, 2964, 1010, 3005, 14318, 9866, 1996, 24464, 2015, 1010, 2066, 1996, 2277, 11397, 1997, 2214, 1010, 11121, 1010, 2012, 2560, 3322, 1010, 2000, 5136, 2004, 4315, 14966, 2013, 7746, 11449, 1998, 2947, 15581, 3085, 2000, 1037, 2166, 1997, 3017, 11870, 1012, 1996, 2240, 2013, 1037, 3689, 1997, 22677, 1010, 24004, 7680, 1010, 2529, 2072, 9152, 19466, 1037, 2033, 7344, 2819, 2404, 2080, 1006, 2030, 2007, 9152, 2140, 2005, 9152, 19466, 1007, 1010, 3574, 1000, 1045, 2572, 1037, 2529, 2108, 1010, 1045, 2228, 2498, 2529, 7344, 2000, 2033, 1000, 1010, 2124, 2144, 16433, 2083, 1996, 20380, 1997, 3002, 14060, 1010, 4227, 9100, 9598, 2004, 4958, 9956, 15630, 2075, 1996, 24464, 7729, 1012, 1996, 4861, 1010, 1999, 1037, 2377, 14440, 2030, 11780, 2013, 1037, 1006, 2085, 2439, 1007, 3306, 4038, 2011, 2273, 12243, 1010, 2089, 2031, 7940, 1999, 1037, 2422, 27693, 12818, 1516, 2004, 1037, 5021, 11581, 2063, 2005, 2019, 2214, 2158, 1005, 1055, 19960, 14423, 1516, 2021, 2009, 2855, 2150, 1037, 6011, 15185, 1998, 2802, 1996, 5535, 2001, 9339, 2007, 1037, 6748, 3574, 1010, 2011, 23080, 1998, 3002, 14060, 1010, 2000, 2171, 1037, 2261, 1010, 1998, 2087, 5546, 2011, 21224, 1012, 2957, 8670, 19042, 7009, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 238, 'end_positions': 238}.
01/31/2025 08:06:17 - INFO - __main__ - Sample 90902 of the training set: {'input_ids': [101, 2054, 2576, 13165, 6817, 6161, 4331, 1999, 1996, 5109, 2077, 1996, 2117, 2088, 2162, 1029, 102, 1996, 4945, 1997, 3537, 4243, 2000, 4652, 23779, 2013, 2635, 2058, 6161, 4331, 1999, 1996, 6641, 1998, 5687, 12603, 7315, 2098, 3769, 4842, 1012, 2002, 4265, 2013, 1996, 3622, 8465, 1997, 2023, 4945, 1010, 2144, 2824, 2044, 1996, 2019, 11624, 7393, 2015, 1010, 1996, 18985, 1997, 5118, 2011, 1996, 2446, 14365, 1999, 4260, 1010, 3140, 2032, 2046, 4568, 8340, 1012, 2010, 2087, 2590, 2573, 1999, 1996, 2492, 1997, 2591, 2671, 1517, 1996, 5635, 1997, 3181, 2964, 1006, 3646, 1007, 1998, 1996, 2330, 2554, 1998, 2049, 6716, 1006, 3386, 1007, 1517, 2020, 4427, 2011, 2010, 9185, 2006, 1996, 2824, 1997, 2010, 2051, 1998, 3421, 1010, 1999, 1037, 3168, 1010, 1037, 4668, 2000, 1996, 15157, 2561, 25691, 8909, 8780, 21615, 2008, 2059, 6817, 2430, 2647, 4331, 1012, 2010, 2808, 8047, 3537, 26505, 2004, 1037, 2591, 1998, 2576, 4695, 1012, 2027, 2036, 3421, 4866, 16218, 2015, 1997, 1996, 9569, 3653, 6342, 9397, 19234, 2015, 2104, 8091, 5582, 2035, 3596, 1997, 2561, 25691, 2964, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 24, 'end_positions': 24}.
01/31/2025 08:06:17 - INFO - __main__ - Sample 13901 of the training set: {'input_ids': [101, 1999, 2054, 2112, 1997, 1996, 12667, 10343, 2099, 2106, 27112, 7265, 3701, 4839, 1029, 102, 5560, 3963, 1003, 1997, 1996, 2181, 1999, 1996, 12667, 10343, 2099, 5031, 1997, 5041, 8575, 1010, 2007, 14897, 27112, 7265, 4655, 3701, 8279, 1999, 1996, 2264, 1012, 1996, 2181, 2003, 4138, 1999, 9754, 4219, 1010, 2164, 11540, 1010, 3019, 3806, 1010, 1998, 3707, 10848, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 40, 'end_positions': 41}.
01/31/2025 08:06:19 - INFO - __main__ - ***** Running training *****
01/31/2025 08:06:19 - INFO - __main__ - Num examples = 131754
01/31/2025 08:06:19 - INFO - __main__ - Num Epochs = 3
01/31/2025 08:06:19 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 08:06:19 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 08:06:19 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 08:06:19 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 08:06:20 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 08:06:20 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'force_calib_once', 'mapping', 'tb_writer'}
01/31/2025 08:06:20 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 08:06:30 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 08:06:32 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:06:36 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 08:06:54 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 08:06:54 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:06:54 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 08:06:54 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 08:06:54 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 08:06:54 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 08:06:54 - INFO - __main__ - Num examples = 12134
01/31/2025 08:06:54 - INFO - __main__ - Batch size = 8
01/31/2025 08:14:44 - INFO - __main__ - Evaluation metrics: {'exact': 10.29225974901036, 'f1': 11.001936176584058, 'total': 11873, 'HasAns_exact': 0.33738191632928477, 'HasAns_f1': 1.7587699434181179, 'HasAns_total': 5928, 'NoAns_exact': 20.218671152228765, 'NoAns_f1': 20.218671152228765, 'NoAns_total': 5945, 'best_exact': 50.07159100480081, 'best_exact_thresh': 0.0, 'best_f1': 50.07159100480081, 'best_f1_thresh': 0.0}
01/31/2025 08:17:14 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 08:17:17 - INFO - __main__ - Sample 4985 of the training set: {'input_ids': [101, 2054, 2106, 29270, 2689, 1996, 2516, 1997, 2010, 2279, 3740, 2201, 2013, 2061, 2393, 2033, 2643, 2000, 1029, 102, 2383, 3322, 2623, 1037, 2047, 2201, 4709, 2061, 2393, 2033, 2643, 18517, 2005, 1037, 2297, 2713, 1010, 1999, 2233, 2325, 2225, 2623, 2008, 1996, 2201, 2052, 2612, 2022, 19325, 2170, 25430, 4509, 1012, 2101, 2008, 3204, 1010, 2225, 2001, 3018, 2019, 5756, 8972, 2011, 1996, 2082, 1997, 1996, 2396, 2820, 1997, 3190, 2005, 2010, 5857, 2000, 2189, 1010, 4827, 1010, 1998, 2759, 3226, 1010, 3985, 2437, 2032, 2019, 5756, 1040, 7011, 1012, 1996, 2279, 3204, 1010, 2225, 25214, 2012, 1996, 1043, 8523, 2669, 4917, 2782, 1999, 1996, 2866, 1010, 2750, 1037, 9964, 2772, 2011, 2471, 11502, 1010, 2199, 2111, 2114, 2010, 3311, 1012, 2012, 2028, 2391, 1010, 2002, 2409, 1996, 4378, 1024, 1000, 2017, 2024, 2085, 3666, 1996, 4602, 2542, 2600, 2732, 2006, 1996, 4774, 1012, 1000, 2865, 11730, 1010, 2164, 2591, 2865, 4573, 2107, 2004, 10474, 1010, 2020, 9249, 4055, 2006, 2010, 2836, 1012, 23770, 3090, 1010, 1000, 1996, 3247, 2000, 2338, 2225, 2005, 1996, 10453, 2038, 4928, 6801, 2144, 2049, 8874, 1010, 1998, 1996, 2265, 2993, 2596, 2000, 11508, 5562, 2119, 1043, 8523, 2669, 4917, 2175, 2545, 1998, 2216, 2040, 15757, 1999, 2000, 3422, 2006, 2037, 2694, 2015, 1012, 1000, 1996, 4772, 2794, 2008, 1000, 2002, 1005, 1055, 5599, 2010, 2189, 3713, 2005, 1998, 6011, 2993, 1012, 1000, 1996, 6697, 2056, 2008, 1000, 2010, 2275, 2038, 1037, 16834, 10768, 21735, 1516, 2021, 2045, 2024, 16680, 1998, 24646, 24168, 1010, 1998, 2002, 7659, 1037, 13939, 10459, 3275, 1999, 2392, 1997, 1996, 6565, 4306, 1012, 1000, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 50, 'end_positions': 51}.
01/31/2025 08:17:17 - INFO - __main__ - Sample 11316 of the training set: {'input_ids': [101, 2054, 2001, 1996, 2034, 2276, 2000, 2265, 3454, 1999, 3609, 1029, 102, 2006, 1015, 2251, 3476, 1010, 4035, 2048, 2150, 1996, 2034, 2547, 3149, 1999, 2885, 2000, 3743, 5570, 1999, 6120, 1010, 2478, 1996, 2225, 2446, 14412, 2291, 2008, 2003, 2145, 1999, 2224, 2651, 2348, 2108, 6360, 19886, 2011, 3617, 3001, 1012, 1006, 4035, 2028, 1998, 11858, 2211, 22810, 1011, 2240, 6120, 8960, 7453, 2006, 2321, 2281, 3440, 1007, 1012, 4406, 2060, 12350, 6833, 1010, 4035, 2048, 2515, 2025, 2031, 7815, 3850, 2030, 3115, 2739, 4730, 1010, 2021, 1037, 2846, 1997, 8497, 3832, 2000, 2022, 20551, 1998, 7578, 1006, 2348, 2065, 1037, 4746, 2038, 2152, 4378, 8599, 2009, 2003, 2411, 2776, 16360, 19234, 2098, 2000, 4035, 2028, 1007, 1012, 1996, 2367, 2128, 22930, 1997, 4035, 2475, 3039, 2049, 2034, 11486, 1010, 2909, 2585, 2012, 6528, 10235, 2000, 3222, 1996, 2034, 8366, 15693, 1998, 4516, 2186, 2107, 2004, 2942, 6648, 1010, 1996, 16354, 1997, 2158, 1998, 9154, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 18, 'end_positions': 19}.
01/31/2025 08:17:17 - INFO - __main__ - Sample 29049 of the training set: {'input_ids': [101, 2054, 2785, 1997, 2374, 2208, 2003, 2956, 1999, 1996, 4104, 3179, 1997, 1996, 2335, 1029, 102, 1996, 2208, 2003, 2443, 1999, 1996, 3780, 2006, 28401, 1010, 1998, 4751, 2035, 1996, 5353, 1005, 1055, 2374, 4023, 1006, 4239, 2223, 1998, 2374, 2223, 2528, 1010, 2223, 2028, 1998, 2223, 2048, 1012, 1007, 1996, 4104, 3179, 1997, 1996, 2208, 2036, 2950, 3463, 1998, 4106, 2013, 4104, 4239, 2223, 2399, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 63, 'end_positions': 65}.
01/31/2025 08:17:18 - INFO - __main__ - ***** Running training *****
01/31/2025 08:17:18 - INFO - __main__ - Num examples = 131754
01/31/2025 08:17:18 - INFO - __main__ - Num Epochs = 3
01/31/2025 08:17:18 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 08:17:18 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 08:17:18 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 08:17:18 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 08:17:19 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 08:17:19 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'tb_writer', 'mapping', 'force_calib_once'}
01/31/2025 08:17:19 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 08:17:29 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 08:17:31 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:17:35 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 08:17:52 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 08:17:52 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:17:53 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 08:17:53 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 08:17:53 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 08:17:53 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 08:17:53 - INFO - __main__ - Num examples = 12134
01/31/2025 08:17:53 - INFO - __main__ - Batch size = 8
01/31/2025 08:22:18 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 08:22:20 - INFO - __main__ - Sample 5590 of the training set: {'input_ids': [101, 2429, 2000, 1996, 11903, 2724, 2002, 3284, 19960, 29293, 2110, 2003, 2025, 2054, 1029, 102, 11388, 6964, 12374, 2163, 1997, 19960, 29293, 16326, 1006, 28619, 1024, 1046, 15788, 1025, 15315, 2102, 1024, 28144, 16811, 1007, 1012, 1996, 2087, 3418, 8760, 3670, 1997, 10930, 12863, 4784, 2003, 2179, 1999, 1996, 2220, 20855, 1997, 1996, 11903, 1012, 2028, 3145, 9525, 4252, 1997, 1996, 11903, 2001, 2008, 19960, 29293, 16326, 2442, 2022, 4117, 2007, 5622, 5677, 5844, 26497, 1012, 1996, 4489, 2090, 1996, 11903, 1005, 1055, 4252, 1998, 1996, 13272, 3591, 1999, 2220, 11655, 26837, 8713, 6981, 2003, 8478, 1012, 19960, 29293, 2163, 2894, 2024, 2025, 2019, 2203, 1010, 2005, 2429, 2000, 1996, 11903, 1010, 2130, 1996, 3284, 19960, 29293, 2110, 2003, 2025, 5622, 5677, 5844, 1012, 2612, 1997, 26615, 1037, 3143, 8292, 11488, 3508, 1997, 2245, 1010, 2070, 4066, 1997, 5177, 4023, 2442, 2202, 2173, 1024, 1037, 5622, 5677, 5844, 26497, 1010, 2241, 2006, 1996, 3218, 1997, 2568, 3993, 7073, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 72, 'end_positions': 74}.
01/31/2025 08:22:20 - INFO - __main__ - Sample 127167 of the training set: {'input_ids': [101, 2007, 2054, 2181, 2515, 5978, 5566, 2007, 2005, 9045, 1029, 102, 3604, 1998, 6813, 3613, 2000, 2022, 5186, 2590, 2005, 5978, 1010, 2007, 10367, 3616, 19939, 2000, 3623, 6022, 1999, 1996, 2925, 1012, 1031, 11091, 2734, 1033, 2174, 1010, 1996, 4852, 2971, 2013, 2789, 2647, 14345, 4247, 2000, 4503, 1010, 2007, 1996, 3739, 1997, 2714, 13051, 2008, 2024, 2411, 16269, 1999, 3032, 2107, 2004, 8097, 1012, 8821, 1010, 2009, 2038, 2042, 4072, 2005, 1996, 2406, 2000, 3579, 2588, 2049, 18111, 13051, 1010, 2107, 2004, 2740, 1010, 3267, 1998, 3541, 6813, 1010, 2000, 2994, 3805, 1997, 2049, 10159, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 44, 'end_positions': 46}.
01/31/2025 08:22:20 - INFO - __main__ - Sample 112405 of the training set: {'input_ids': [101, 2054, 2024, 2070, 1997, 1996, 2334, 15723, 7446, 2109, 1999, 3306, 4092, 1029, 102, 2715, 3306, 2038, 1010, 1999, 2804, 2000, 3115, 2715, 3306, 2030, 11737, 4140, 17471, 1010, 1037, 2898, 3528, 1997, 11976, 1997, 9671, 3798, 1997, 8203, 13420, 3669, 5856, 8553, 1010, 2164, 18543, 1010, 21179, 2594, 1010, 6178, 15455, 10085, 2937, 1010, 24665, 12676, 1998, 24529, 20411, 11148, 1006, 1996, 2069, 6405, 4387, 1997, 3418, 2079, 7277, 3306, 1007, 1012, 6300, 27760, 2278, 2003, 1996, 2653, 1997, 1996, 23982, 12184, 2015, 1010, 1998, 13655, 1999, 2235, 4279, 1999, 5483, 1010, 2047, 2259, 1998, 3956, 1012, 1999, 2804, 2000, 3306, 1010, 2116, 13176, 1999, 5483, 1998, 1996, 18239, 2024, 17636, 1999, 2060, 4155, 2030, 11976, 2107, 2004, 2394, 1010, 12098, 27760, 3775, 2912, 1013, 9408, 1010, 23958, 11148, 1010, 13037, 13838, 1010, 2845, 1998, 5037, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 31, 'end_positions': 61}.
01/31/2025 08:22:21 - INFO - __main__ - ***** Running training *****
01/31/2025 08:22:21 - INFO - __main__ - Num examples = 131754
01/31/2025 08:22:21 - INFO - __main__ - Num Epochs = 3
01/31/2025 08:22:21 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 08:22:21 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 08:22:21 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 08:22:21 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 08:22:23 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 08:22:23 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'tb_writer', 'mapping', 'force_calib_once'}
01/31/2025 08:22:23 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 08:22:33 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 08:22:35 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:22:39 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 08:22:56 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 08:22:56 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:22:57 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 08:22:57 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 08:22:57 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 08:22:57 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 08:22:57 - INFO - __main__ - Num examples = 12134
01/31/2025 08:22:57 - INFO - __main__ - Batch size = 8
01/31/2025 08:24:34 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 08:24:36 - INFO - __main__ - Sample 27536 of the training set: {'input_ids': [101, 1999, 2054, 9183, 4664, 1997, 7938, 2064, 16831, 2111, 2022, 2179, 1029, 102, 16831, 2111, 1999, 1996, 7109, 1997, 3088, 2024, 4055, 2426, 2367, 3032, 1006, 14717, 1010, 6520, 12322, 5833, 2072, 1010, 11154, 1010, 1998, 8763, 7938, 1007, 2008, 2020, 7976, 2135, 1998, 2070, 2453, 2360, 12098, 16313, 19848, 6588, 13571, 2098, 2011, 1996, 2280, 4461, 4204, 1012, 6090, 1011, 16831, 6491, 2003, 2019, 13165, 2008, 13010, 1996, 16905, 1997, 2035, 5636, 16831, 2015, 2320, 2112, 1997, 16831, 23560, 2107, 2004, 1996, 19128, 4648, 2078, 3400, 1010, 1996, 15262, 2140, 21308, 1010, 1996, 2175, 12618, 2239, 5321, 1998, 1996, 4315, 24968, 2110, 2104, 2028, 5210, 1998, 2028, 3842, 1012, 1996, 9033, 4215, 23189, 6939, 8851, 3755, 6090, 1011, 16831, 6491, 1010, 2029, 2776, 2419, 2000, 1996, 13958, 9648, 2078, 2162, 2090, 14717, 2006, 2028, 2217, 1010, 1998, 11154, 1010, 7394, 1998, 1996, 3354, 2586, 2006, 1996, 2060, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 37, 'end_positions': 37}.
01/31/2025 08:24:36 - INFO - __main__ - Sample 126617 of the training set: {'input_ids': [101, 2429, 2000, 2087, 18288, 1010, 2054, 5325, 2165, 2173, 2044, 1996, 5325, 1999, 5647, 2613, 3776, 1029, 102, 1047, 26549, 2386, 1005, 1055, 18974, 1006, 2008, 1996, 3930, 1997, 1037, 3293, 2613, 3776, 11957, 7127, 2008, 1057, 1012, 1055, 1012, 3847, 3343, 2001, 2025, 1996, 3426, 1997, 1996, 5325, 1007, 2003, 8315, 2011, 3176, 4106, 1012, 2044, 20059, 1996, 12398, 1997, 3293, 10940, 2076, 1996, 3361, 5325, 1010, 15990, 17679, 2019, 1998, 4938, 1038, 1012, 12055, 2988, 1006, 1999, 2285, 2230, 1007, 1024, 1000, 2057, 2424, 3132, 3350, 2008, 6937, 26118, 1999, 4642, 5910, 1031, 3293, 14344, 1011, 6153, 12012, 1033, 5414, 2104, 18560, 4158, 3188, 2000, 1996, 5325, 1012, 1000, 2060, 18288, 2490, 1996, 18974, 2008, 1996, 5325, 1999, 3293, 2613, 3776, 1998, 3141, 18435, 2165, 2173, 2044, 1996, 5325, 1999, 5647, 2613, 3776, 1012, 2449, 4988, 23729, 25933, 3207, 2080, 4311, 1024, 1000, 1996, 2034, 5751, 1997, 6689, 1999, 5647, 2613, 3776, 4158, 1999, 2294, 1012, 2093, 2086, 2101, 1010, 3293, 2613, 3776, 2318, 3110, 1996, 3896, 1012, 7939, 6610, 1037, 1012, 21025, 6906, 2818, 1010, 1037, 2613, 3776, 4905, 1998, 18133, 2050, 1010, 2626, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 118, 'end_positions': 123}.
01/31/2025 08:24:36 - INFO - __main__ - Sample 58188 of the training set: {'input_ids': [101, 2054, 2079, 3943, 1003, 1997, 3287, 2493, 2079, 1029, 102, 2493, 7052, 23471, 2024, 3223, 2000, 3582, 2019, 3932, 3642, 1010, 2029, 25979, 5248, 1999, 2240, 2007, 17627, 12209, 2107, 2004, 3834, 16718, 1010, 29235, 2000, 4377, 1998, 18087, 2075, 4781, 1010, 1998, 14689, 10196, 5897, 2013, 4469, 7849, 18400, 3348, 1998, 2013, 1996, 8381, 1997, 5850, 1998, 6544, 1012, 2116, 2493, 1006, 6070, 3867, 1997, 2273, 1010, 3943, 3867, 1997, 2308, 1007, 2593, 8536, 10316, 2030, 2202, 1037, 14221, 2013, 2037, 2913, 2000, 3710, 2004, 15111, 11743, 1012, 1006, 2273, 4050, 3710, 2005, 2048, 1011, 2086, 1010, 2096, 2308, 3710, 2005, 2324, 2706, 1012, 1007, 2019, 2495, 2012, 23471, 2003, 2036, 2625, 6450, 2084, 2012, 2714, 2797, 5534, 1010, 2144, 1000, 1037, 3278, 4664, 1000, 1997, 1996, 3465, 1997, 4082, 1996, 2118, 2003, 4942, 5332, 4305, 5422, 2011, 1996, 2277, 1005, 1055, 14841, 20744, 5029, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 08:24:38 - INFO - __main__ - ***** Running training *****
01/31/2025 08:24:38 - INFO - __main__ - Num examples = 131754
01/31/2025 08:24:38 - INFO - __main__ - Num Epochs = 3
01/31/2025 08:24:38 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 08:24:38 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 08:24:38 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 08:24:38 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 08:24:39 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 08:24:39 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'mapping', 'force_calib_once', 'tb_writer'}
01/31/2025 08:24:39 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 08:24:49 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 08:24:51 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:24:55 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 08:25:12 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 08:25:12 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:25:13 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 08:25:13 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 08:25:13 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 08:25:13 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 08:25:13 - INFO - __main__ - Num examples = 12134
01/31/2025 08:25:13 - INFO - __main__ - Batch size = 8
01/31/2025 08:32:34 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 08:32:37 - INFO - __main__ - Sample 125806 of the training set: {'input_ids': [101, 2054, 13519, 2515, 1996, 3422, 3578, 2554, 15454, 1029, 102, 1996, 3422, 3578, 2554, 19164, 13519, 2008, 2009, 2003, 1037, 6270, 12168, 1010, 5517, 2008, 2049, 12209, 2024, 2025, 4427, 2030, 1999, 13976, 7028, 1010, 1998, 2008, 2009, 2038, 2025, 3555, 2049, 20932, 2020, 1000, 1996, 2616, 1997, 15333, 6806, 21927, 1012, 1000, 2577, 1040, 1012, 10381, 24769, 7363, 2015, 2038, 4081, 2008, 2007, 1996, 6453, 1997, 8635, 2055, 4554, 1010, 4849, 1998, 3339, 1010, 1996, 5278, 5328, 1998, 5246, 1997, 1996, 15333, 6806, 21927, 1005, 1055, 9390, 2024, 4321, 2012, 18886, 8569, 10880, 2000, 2904, 4824, 2015, 1997, 10213, 17873, 2084, 2000, 3478, 20932, 1012, 10381, 24769, 7363, 2015, 2582, 2163, 1010, 1000, 2009, 2003, 3568, 21934, 24759, 6553, 1998, 15743, 2000, 3193, 1996, 9390, 2004, 1037, 2177, 2008, 4247, 2000, 2275, 1037, 2309, 2203, 1011, 3058, 2008, 11896, 1998, 2059, 14386, 3366, 1037, 2047, 2028, 1010, 2004, 2116, 4675, 1011, 8754, 5130, 2079, 1012, 1000, 2174, 1010, 25106, 4080, 9988, 2163, 2008, 2144, 1996, 3192, 1997, 1996, 2929, 2105, 8574, 2086, 3283, 1010, 1000, 9390, 2031, 5224, 2008, 2057, 2024, 2542, 2006, 1996, 3653, 6895, 24330, 2063, 1997, 1996, 2203, 1997, 2051, 1012, 1000, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 17, 'end_positions': 22}.
01/31/2025 08:32:37 - INFO - __main__ - Sample 112979 of the training set: {'input_ids': [101, 2054, 2003, 1037, 3278, 3120, 1997, 6599, 2005, 1996, 4202, 3189, 1029, 102, 3346, 5270, 2015, 2024, 1037, 3278, 3120, 1997, 3180, 3318, 2005, 4239, 2223, 4184, 1012, 2005, 1996, 2268, 1516, 2184, 2161, 1010, 2779, 5270, 2015, 2408, 1996, 2223, 4184, 2020, 4090, 1010, 17405, 2005, 4239, 2223, 3503, 2007, 1037, 2561, 9572, 5270, 3275, 1997, 2410, 1010, 25604, 1010, 6079, 2575, 1012, 2023, 5836, 2019, 3623, 1997, 2410, 1010, 5511, 2683, 2013, 1996, 2779, 5270, 1997, 2538, 1010, 14010, 2680, 1999, 1996, 2223, 1005, 1055, 2034, 2161, 1006, 2826, 1516, 6109, 1007, 1012, 2174, 1010, 2076, 1996, 2826, 1516, 6109, 2161, 1996, 21157, 1997, 2087, 28244, 2020, 4359, 2004, 4184, 2999, 25633, 2007, 4272, 1999, 2344, 2000, 3113, 1996, 4202, 3189, 1005, 1055, 2807, 1516, 5345, 15117, 2005, 2035, 1011, 23392, 28244, 1012, 1996, 4239, 2223, 1005, 1055, 2501, 2779, 5270, 1997, 4029, 1010, 14748, 2001, 2275, 2076, 1996, 2289, 1516, 5511, 2161, 1012, 2023, 2501, 2001, 2059, 7854, 1999, 1996, 2286, 1516, 2403, 2161, 3405, 2019, 2779, 5270, 1997, 4029, 1010, 6353, 2629, 2007, 1037, 2561, 5270, 1997, 2074, 2104, 2403, 2454, 1010, 1996, 3284, 2779, 1999, 2563, 1005, 1055, 2327, 3462, 2144, 3925, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 08:32:37 - INFO - __main__ - Sample 67244 of the training set: {'input_ids': [101, 1999, 2054, 2103, 2106, 18318, 5498, 3926, 2010, 11316, 2913, 1029, 102, 1999, 4947, 1010, 2002, 3133, 1996, 8705, 2000, 2468, 1037, 3142, 3234, 5011, 1012, 2002, 2001, 9492, 5011, 2006, 2756, 2089, 4444, 1999, 7987, 2229, 7405, 1998, 6334, 2010, 2034, 4151, 3742, 1999, 7987, 2229, 7405, 1999, 1996, 13546, 1997, 4203, 3814, 24121, 26918, 2666, 1012, 18318, 5498, 5531, 2010, 2913, 1999, 6954, 2007, 1037, 8972, 1999, 9330, 2375, 1999, 1996, 2168, 2095, 1012, 5728, 2002, 3273, 2012, 1996, 25847, 2118, 1010, 1996, 2118, 1997, 4199, 2474, 20066, 9013, 4143, 1998, 1010, 2012, 1996, 5227, 1997, 12574, 10733, 20683, 2012, 1996, 16222, 9648, 10092, 14866, 2053, 14454, 2072, 14925, 18954, 7951, 4588, 2072, 1012, 2012, 1996, 2287, 1997, 3174, 1011, 2274, 1010, 2153, 2012, 1996, 5227, 1997, 12574, 10733, 20683, 1010, 18318, 5498, 3133, 1996, 15128, 1997, 2110, 1999, 4798, 1010, 2073, 2002, 2499, 2104, 10733, 20683, 2362, 2007, 11400, 28709, 5063, 5498, 1011, 26363, 2050, 1010, 19423, 27178, 2696, 18073, 2072, 1010, 9758, 12604, 2080, 1010, 18638, 16985, 26039, 1998, 4557, 6297, 2386, 1012, 8821, 1010, 2002, 2985, 2025, 1037, 2154, 2004, 1037, 3583, 5011, 1012, 1999, 4849, 2002, 3271, 2179, 1996, 4640, 2160, 22822, 29109, 15204, 2050, 1999, 7987, 2229, 7405, 1010, 4208, 2006, 7694, 1037, 1005, 3017, 4427, 3226, 1005, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 65, 'end_positions': 65}.
01/31/2025 08:32:38 - INFO - __main__ - ***** Running training *****
01/31/2025 08:32:38 - INFO - __main__ - Num examples = 131754
01/31/2025 08:32:38 - INFO - __main__ - Num Epochs = 3
01/31/2025 08:32:38 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 08:32:38 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 08:32:38 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 08:32:38 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 08:32:39 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 08:32:39 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'force_calib_once', 'tb_writer', 'mapping'}
01/31/2025 08:32:39 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 08:32:50 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 08:32:52 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:32:56 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 08:33:14 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 08:33:14 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:33:15 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 08:33:15 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 08:33:15 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 08:33:15 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 08:33:15 - INFO - __main__ - Num examples = 12134
01/31/2025 08:33:15 - INFO - __main__ - Batch size = 8
01/31/2025 08:42:15 - INFO - __main__ - Evaluation metrics: {'exact': 1.4991998652404614, 'f1': 4.966895595525744, 'total': 11873, 'HasAns_exact': 0.10121457489878542, 'HasAns_f1': 7.046550507030578, 'HasAns_total': 5928, 'NoAns_exact': 2.893187552565181, 'NoAns_f1': 2.893187552565181, 'NoAns_total': 5945, 'best_exact': 50.07159100480081, 'best_exact_thresh': 0.0, 'best_f1': 50.07299474999298, 'best_f1_thresh': 0.0}
01/31/2025 08:55:45 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 08:55:48 - INFO - __main__ - Sample 500 of the training set: {'input_ids': [101, 20773, 2038, 3264, 2029, 2034, 3203, 2007, 3038, 1000, 2016, 16481, 2017, 2064, 2079, 2009, 2035, 1000, 1029, 102, 20773, 2038, 3090, 2008, 2016, 2003, 7714, 4427, 2011, 2149, 2034, 3203, 9393, 8112, 1010, 3038, 1000, 2016, 16481, 2017, 2064, 2079, 2009, 2035, 1000, 1998, 2016, 2038, 2649, 6728, 10404, 2663, 22586, 2004, 1000, 1996, 6210, 1997, 7780, 1998, 1037, 2844, 2450, 1000, 1012, 2016, 2038, 2036, 6936, 2129, 6108, 1062, 2003, 1037, 5719, 7780, 2000, 2014, 1010, 2119, 2007, 2054, 2016, 5577, 2004, 2010, 16376, 11067, 1998, 1999, 1996, 15314, 2002, 2038, 9462, 1999, 2010, 2166, 1012, 20773, 2038, 5228, 17005, 2005, 1996, 3063, 3744, 1011, 8709, 19021, 15549, 4017, 1010, 14739, 1999, 1037, 3661, 1000, 2054, 1045, 2424, 1999, 1996, 2147, 1997, 3744, 1011, 8709, 19021, 15549, 4017, 1010, 1045, 3945, 2005, 1999, 2296, 2154, 1999, 2189, 1012, 1012, 1012, 2002, 2003, 16376, 1998, 6315, 1000, 1012, 1999, 2337, 2286, 1010, 20773, 2056, 2008, 11284, 4427, 2014, 2000, 2202, 2491, 1997, 2014, 2219, 2476, 1012, 2016, 7034, 1024, 1000, 1045, 2228, 2055, 11284, 1998, 2129, 2016, 2165, 2035, 1997, 1996, 2307, 2477, 2016, 4719, 1998, 2318, 1996, 3830, 1998, 2764, 2060, 3324, 1012, 2021, 2045, 2024, 2025, 2438, 1997, 2216, 2308, 1012, 1000, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 32, 'end_positions': 33}.
01/31/2025 08:55:48 - INFO - __main__ - Sample 110437 of the training set: {'input_ids': [101, 2129, 2116, 9646, 2106, 8923, 9036, 1998, 8856, 11463, 4143, 3600, 6235, 3255, 1999, 1029, 102, 1999, 3380, 8923, 11463, 4143, 3600, 1998, 8856, 9036, 2649, 3255, 1999, 3408, 1997, 2049, 2093, 9646, 1024, 1000, 16792, 1011, 5860, 20026, 3981, 6024, 1000, 1006, 3168, 1997, 1996, 8015, 1010, 3295, 1010, 3737, 1998, 9367, 1997, 1996, 3255, 1007, 1010, 1000, 7461, 3512, 1011, 14354, 2389, 1000, 1006, 16010, 2791, 1998, 9075, 2000, 4019, 1996, 16010, 2791, 1007, 1010, 1998, 1000, 10699, 1011, 9345, 7630, 8082, 1000, 1006, 26497, 2015, 2107, 2004, 10439, 14995, 12002, 1010, 3451, 5300, 1010, 14836, 1998, 28322, 10293, 1007, 1012, 2027, 14833, 18425, 2008, 3255, 8015, 1006, 1996, 16792, 5860, 20026, 3981, 6024, 9812, 1007, 1998, 16010, 2791, 1006, 1996, 7461, 3512, 1011, 14354, 2389, 9812, 1007, 2024, 2025, 3432, 4340, 2011, 1996, 10194, 1997, 1996, 9145, 19220, 1010, 2021, 1000, 3020, 1000, 10699, 3450, 2064, 3747, 8690, 8015, 1998, 16010, 2791, 1012, 10699, 3450, 1000, 2089, 7461, 2119, 16792, 1998, 7461, 3512, 3325, 2030, 2027, 2089, 19933, 3952, 1996, 7461, 3512, 1011, 14354, 2389, 9812, 1012, 2947, 1010, 8277, 1999, 2399, 2030, 2162, 3544, 2000, 3796, 2119, 9646, 1997, 3255, 1010, 2096, 10293, 1998, 2173, 15853, 2089, 16913, 9869, 1996, 7461, 3512, 1011, 14354, 2389, 9812, 1998, 2681, 1996, 16792, 1011, 5860, 20026, 3981, 6024, 9812, 4659, 6151, 2923, 29595, 1012, 1000, 1006, 1052, 1012, 4724, 2475, 1007, 1996, 3259, 4515, 2007, 1037, 2655, 2000, 2895, 1024, 1000, 3255, 2064, 2022, 5845, 2025, 2069, 2011, 2667, 2000, 3013, 2091, 1996, 16792, 7953, 2011, 2019, 4355, 20086, 3796, 1010, 11707, 8830, 1998, 1996, 2066, 1010, 2021, 2036, 2011, 25870, 1996, 14354, 2389, 1011, 7461, 3512, 1998, 10699, 5876, 2004, 2092, 1012, 1000, 1006, 1052, 1012, 24125, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 08:55:48 - INFO - __main__ - Sample 52899 of the training set: {'input_ids': [101, 2043, 2001, 1996, 2882, 2326, 2081, 1029, 102, 2110, 19032, 2015, 2036, 2202, 2173, 1999, 1996, 14307, 1025, 2122, 5337, 29236, 2024, 2218, 2006, 1996, 2034, 3944, 1997, 1037, 2110, 3942, 2011, 1037, 3097, 2132, 1997, 2110, 1012, 2006, 2122, 6642, 1010, 2005, 2039, 2000, 10894, 6368, 1999, 5337, 1000, 2317, 5495, 1998, 14529, 1000, 1010, 2164, 27339, 8180, 1010, 1996, 7759, 2795, 2003, 4201, 2007, 1996, 2882, 2326, 1010, 1037, 3074, 1997, 3165, 1011, 13097, 2102, 5127, 2081, 1999, 13086, 2005, 1996, 3159, 1997, 3575, 1010, 2101, 2577, 4921, 1012, 1996, 2922, 1998, 2087, 5337, 7684, 2012, 17836, 4186, 3138, 2173, 2296, 2281, 2043, 1996, 3035, 20432, 2015, 2372, 1997, 1996, 8041, 3650, 1012, 2006, 2023, 2882, 6686, 1010, 2035, 1996, 2110, 4734, 2024, 1999, 2224, 1010, 2004, 1996, 2548, 2155, 10838, 2083, 2068, 1010, 2927, 2012, 1996, 2307, 2167, 4303, 1997, 1996, 3861, 3916, 1012, 2004, 10594, 2018, 4372, 11365, 18655, 1010, 2035, 1996, 2312, 1010, 3313, 1011, 22243, 4303, 3233, 2330, 1010, 10842, 1996, 3365, 6121, 9212, 9247, 10136, 1998, 8040, 2239, 9623, 1010, 4526, 1037, 15063, 9380, 12492, 1997, 2686, 1998, 2422, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 81, 'end_positions': 81}.
01/31/2025 08:55:49 - INFO - __main__ - ***** Running training *****
01/31/2025 08:55:49 - INFO - __main__ - Num examples = 131754
01/31/2025 08:55:49 - INFO - __main__ - Num Epochs = 3
01/31/2025 08:55:49 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 08:55:49 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 08:55:49 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 08:55:49 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 08:55:50 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 08:55:50 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'mapping', 'force_calib_once', 'tb_writer'}
01/31/2025 08:55:50 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 08:56:00 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 08:56:02 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:56:06 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 08:56:24 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 08:56:24 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 08:56:24 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 08:56:24 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 08:56:24 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 08:56:24 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 08:56:24 - INFO - __main__ - Num examples = 12134
01/31/2025 08:56:24 - INFO - __main__ - Batch size = 8
01/31/2025 08:58:09 - INFO - __main__ - Evaluation metrics: {'exact': 72.54274404110166, 'f1': 75.95472124253871, 'total': 11873, 'HasAns_exact': 72.8744939271255, 'HasAns_f1': 79.70823301495648, 'HasAns_total': 5928, 'NoAns_exact': 72.21194280908327, 'NoAns_f1': 72.21194280908327, 'NoAns_total': 5945, 'best_exact': 72.54274404110166, 'best_exact_thresh': 0.0, 'best_f1': 75.95472124253871, 'best_f1_thresh': 0.0}
01/31/2025 09:05:26 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 09:05:29 - INFO - __main__ - Sample 92371 of the training set: {'input_ids': [101, 1999, 2054, 2086, 2106, 1996, 2394, 3548, 11494, 3000, 2076, 1996, 3634, 2086, 2162, 1029, 102, 2076, 1996, 3634, 2086, 1005, 2162, 1010, 1996, 2390, 1997, 1996, 3804, 1997, 18383, 1998, 1037, 2486, 1997, 2055, 2048, 3634, 2394, 3548, 4548, 3000, 2013, 2089, 16087, 2692, 2127, 16065, 2575, 1012, 2027, 16360, 21148, 2019, 3535, 2011, 7437, 1997, 8115, 2000, 5622, 5677, 3686, 1996, 2103, 1999, 16087, 2683, 1012, 1037, 2301, 2101, 1010, 2076, 1996, 2413, 5233, 1997, 4676, 1010, 3000, 2001, 1037, 16995, 1997, 1996, 3234, 2223, 1012, 2006, 2484, 2257, 17403, 2475, 1010, 3000, 2001, 1996, 2609, 1997, 1996, 2358, 1012, 19866, 1005, 1055, 2154, 9288, 1010, 2043, 5190, 1997, 2413, 19592, 2020, 2730, 1012, 1996, 2197, 1997, 2122, 5233, 1010, 1996, 5964, 2028, 1010, 3092, 1999, 18914, 2549, 1010, 2044, 8863, 4921, 2018, 4991, 2000, 16138, 1998, 2001, 2633, 2583, 2000, 4607, 3000, 2004, 2002, 10743, 4161, 3000, 12436, 4904, 29316, 16655, 6752, 2063, 1006, 1000, 3000, 2003, 2092, 4276, 1037, 3742, 1000, 1007, 1012, 1996, 2103, 2018, 2042, 15486, 2005, 5109, 1025, 2011, 1996, 2051, 1997, 2010, 10102, 1999, 25800, 1010, 2888, 4921, 2018, 7183, 1996, 21179, 11265, 16093, 1010, 1996, 2034, 3000, 2958, 2007, 28386, 1998, 2025, 7732, 2007, 3121, 1010, 5799, 2007, 1037, 2047, 3358, 1996, 25110, 2000, 1996, 10722, 9463, 5134, 4186, 1010, 1998, 2580, 1996, 2034, 3000, 5647, 2675, 1010, 1996, 2173, 24483, 1010, 2085, 2173, 4078, 29536, 28745, 2229, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 44, 'end_positions': 48}.
01/31/2025 09:05:29 - INFO - __main__ - Sample 2179 of the training set: {'input_ids': [101, 2054, 2112, 1997, 1996, 16568, 2003, 4846, 2000, 2224, 4255, 1029, 102, 1996, 2208, 10841, 4783, 1998, 16568, 4617, 3444, 2195, 3576, 5966, 1999, 2037, 7711, 1012, 1996, 16568, 2544, 1997, 1996, 2208, 3084, 2224, 1997, 1996, 4367, 13907, 1998, 2328, 1011, 1999, 5882, 1997, 1996, 16568, 6556, 1012, 1996, 5882, 12495, 3215, 1996, 4165, 1997, 1037, 21207, 18886, 3070, 2043, 5008, 2019, 8612, 1010, 3054, 2532, 1005, 1055, 4756, 2043, 2016, 3957, 6040, 2000, 4957, 1010, 1998, 1996, 2186, 1005, 11749, 1000, 9610, 4168, 1000, 2043, 13648, 7800, 1012, 1996, 2447, 7711, 4957, 1005, 1055, 4690, 2011, 11820, 1996, 16568, 6556, 1012, 2060, 4491, 2024, 13330, 2478, 2714, 18327, 2007, 1996, 16634, 26516, 1012, 4310, 2000, 1996, 2208, 10841, 4783, 2544, 2003, 1996, 3754, 2005, 1996, 2447, 2000, 2491, 1996, 4950, 10350, 1010, 2302, 5738, 1037, 2569, 1000, 2298, 24490, 1000, 5549, 3223, 2011, 1996, 16568, 1025, 2174, 1010, 1999, 1996, 2208, 10841, 4783, 2544, 1010, 2069, 2048, 1997, 4957, 1005, 1055, 3905, 4255, 2064, 2022, 6055, 2012, 1037, 2051, 1010, 2004, 4941, 2000, 2176, 1999, 1996, 16568, 2544, 1012, 1031, 1043, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 47, 'end_positions': 48}.
01/31/2025 09:05:29 - INFO - __main__ - Sample 16536 of the training set: {'input_ids': [101, 4998, 2013, 1996, 5338, 2326, 2000, 9272, 1010, 2054, 2003, 1996, 2060, 2504, 1997, 2326, 3024, 1029, 102, 2045, 2024, 2048, 3798, 1997, 2326, 3024, 1517, 1037, 2489, 2326, 2000, 9272, 1998, 7000, 2326, 2000, 1996, 2822, 2231, 1998, 2510, 1012, 1996, 2489, 6831, 2326, 2038, 1037, 2184, 1011, 8316, 3295, 1011, 9651, 10640, 1010, 26351, 8093, 10698, 11254, 20940, 2007, 2019, 10640, 1997, 2184, 28991, 3366, 8663, 5104, 1010, 1998, 5761, 10898, 2000, 2306, 1014, 1012, 1016, 1049, 1013, 1055, 1012, 1996, 7775, 2510, 2326, 2038, 1037, 3295, 10640, 1997, 2184, 13935, 1010, 2064, 2022, 2109, 2005, 4807, 1010, 1998, 2097, 4425, 2592, 2055, 1996, 2291, 3570, 2000, 1996, 5310, 1012, 2000, 3058, 1010, 1996, 2510, 2326, 2038, 2042, 4379, 2069, 2000, 1996, 2111, 1005, 1055, 7931, 2390, 1998, 2000, 1996, 2510, 1997, 4501, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 0, 'end_positions': 0}.
01/31/2025 09:05:30 - INFO - __main__ - ***** Running training *****
01/31/2025 09:05:30 - INFO - __main__ - Num examples = 131754
01/31/2025 09:05:30 - INFO - __main__ - Num Epochs = 3
01/31/2025 09:05:30 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 09:05:30 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 09:05:30 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 09:05:30 - INFO - __main__ - Total optimization steps = 49410
01/31/2025 09:05:31 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 09:05:31 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'force_calib_once', 'mapping', 'tb_writer'}
01/31/2025 09:05:31 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 09:05:41 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 09:05:43 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 09:05:47 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 09:06:04 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 09:06:04 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 09:06:05 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 09:06:05 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 09:06:05 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 09:06:05 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 09:06:05 - INFO - __main__ - Num examples = 12134
01/31/2025 09:06:05 - INFO - __main__ - Batch size = 8
01/31/2025 09:12:12 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 09:12:51 - INFO - __main__ - Sample 37803 of the training set: {'input_ids': [101, 2029, 1997, 3035, 3848, 1005, 1055, 2336, 2351, 1999, 2251, 1997, 2008, 2095, 1029, 102, 3848, 4716, 8240, 2885, 5570, 2005, 11938, 1012, 1999, 6478, 1010, 2076, 1037, 2994, 1999, 12170, 2906, 14778, 2480, 1010, 2016, 2150, 1996, 2034, 16323, 11590, 2013, 3725, 2000, 2275, 3329, 1999, 3577, 2043, 2016, 4625, 1996, 3675, 2005, 1037, 4766, 3942, 1012, 2011, 2258, 5141, 1010, 1996, 19945, 2162, 2001, 2061, 19657, 1999, 8240, 2885, 2008, 2014, 3296, 4440, 2000, 2605, 2790, 27118, 2094, 11365, 3085, 1012, 2612, 1010, 1996, 3035, 2253, 2000, 3163, 2005, 1996, 2034, 2051, 2144, 6863, 1010, 1999, 2112, 2000, 13399, 1996, 6691, 1997, 3493, 10435, 2000, 1996, 2148, 3060, 2162, 1012, 1999, 2251, 1010, 2014, 2117, 2365, 6152, 1006, 1000, 21358, 8873, 2063, 1000, 1007, 2351, 1025, 1000, 2821, 1010, 2643, 999, 2026, 3532, 9548, 21358, 8873, 2063, 2908, 2205, 1000, 1010, 2016, 2626, 1999, 2014, 3485, 1012, 1000, 2009, 2003, 1037, 9202, 2095, 1010, 2498, 2021, 12039, 1004, 22812, 1997, 2028, 2785, 1004, 2178, 1012, 1000, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 116, 'end_positions': 119}.
01/31/2025 09:12:51 - INFO - __main__ - Sample 79867 of the training set: {'input_ids': [101, 2054, 2451, 4472, 1996, 2181, 2013, 5116, 2624, 5277, 2000, 1996, 6000, 1029, 102, 5116, 2624, 5277, 2003, 2284, 2006, 2624, 5277, 3016, 1012, 28352, 5092, 2050, 2380, 13974, 2195, 15797, 2015, 1998, 8399, 2015, 2000, 1996, 4794, 1010, 5129, 2011, 3080, 1010, 9742, 3923, 4279, 2164, 2940, 25313, 1998, 2167, 2380, 1012, 2000, 1996, 2264, 1998, 4643, 4682, 2103, 7535, 1010, 1996, 2267, 2181, 1010, 1998, 4643, 2624, 5277, 1012, 2000, 1996, 2167, 3658, 3260, 3028, 1998, 7553, 1022, 1012, 1996, 4279, 2167, 1997, 1996, 3028, 1998, 10846, 1010, 1998, 2148, 1997, 3884, 3650, 2250, 2276, 18062, 7849, 1010, 2421, 6249, 9629, 1010, 17710, 2906, 4890, 15797, 1010, 7563, 8180, 26802, 1010, 1998, 22440, 1012, 10917, 2167, 2013, 18062, 7849, 2024, 1996, 2642, 9435, 1997, 18062, 15797, 1010, 8040, 29443, 4523, 8086, 1010, 18123, 19409, 2015, 15549, 13122, 1010, 1998, 18123, 21175, 1012, 1996, 2521, 4794, 4664, 1997, 1996, 2103, 13974, 2697, 19772, 1998, 1996, 2624, 14674, 26426, 3028, 1010, 2029, 4324, 2019, 4910, 7969, 1012, 19443, 3028, 1998, 3972, 9388, 7535, 11494, 1996, 4514, 3420, 1997, 1996, 2103, 1012, 2000, 2037, 2148, 2024, 22047, 2100, 17527, 2110, 3914, 1998, 1996, 2449, 2415, 1997, 1996, 3585, 9546, 1012, 2582, 2148, 2024, 1996, 3509, 1998, 5780, 4279, 1997, 2474, 8183, 4571, 1010, 3534, 3509, 1010, 3260, 3509, 1010, 1998, 4153, 3509, 1012, 2391, 8840, 2863, 14133, 1996, 6000, 2408, 2624, 5277, 3016, 2013, 5116, 1012, 1996, 4279, 1997, 2148, 2624, 5277, 1010, 2107, 2004, 2624, 1061, 5332, 22196, 1998, 27178, 4710, 15797, 1010, 2024, 2284, 2279, 2000, 1996, 3290, 1516, 2142, 2163, 3675, 1010, 1998, 2024, 8186, 5459, 2013, 1996, 2717, 1997, 1996, 2103, 2011, 1996, 3655, 1997, 2120, 2103, 1998, 14684, 2721, 13005, 1012, 1037, 4867, 6167, 1997, 2455, 2012, 1996, 3953, 1997, 2624, 5277, 3016, 8539, 2122, 2670, 11681, 2007, 1996, 2717, 1997, 1996, 2103, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 223, 'end_positions': 225}.
01/31/2025 09:12:51 - INFO - __main__ - Sample 48785 of the training set: {'input_ids': [101, 2007, 3183, 2001, 1037, 12087, 20451, 23294, 12510, 1997, 11594, 1999, 10640, 2044, 1996, 5043, 5994, 3035, 3870, 2462, 1029, 102, 2006, 1023, 2233, 2355, 1010, 1996, 3103, 1005, 1055, 2392, 3931, 10116, 2008, 3035, 3870, 2462, 2001, 5150, 1000, 7987, 10288, 4183, 1000, 1010, 1037, 2691, 2744, 2005, 1037, 2329, 10534, 2013, 1996, 2647, 2586, 1012, 2009, 3555, 2008, 1999, 2249, 2012, 10064, 3317, 1010, 2096, 2383, 6265, 2007, 4112, 3539, 2704, 4172, 18856, 13910, 2290, 1010, 1996, 11590, 10648, 1996, 2586, 1012, 18856, 13910, 2290, 6380, 2008, 1996, 3035, 2081, 2107, 1037, 4861, 1010, 1998, 1037, 17836, 4186, 15974, 4484, 2008, 1037, 12087, 2018, 2042, 2081, 2000, 1996, 2981, 2811, 4781, 5502, 2058, 1037, 12510, 1997, 11594, 8800, 2000, 10640, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 111, 'end_positions': 114}.
01/31/2025 09:12:52 - INFO - __main__ - ***** Running training *****
01/31/2025 09:12:52 - INFO - __main__ - Num examples = 88524
01/31/2025 09:12:52 - INFO - __main__ - Num Epochs = 3
01/31/2025 09:12:52 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 09:12:52 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 09:12:52 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 09:12:52 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 09:12:53 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 09:12:53 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 09:13:03 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 09:13:06 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 09:13:10 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 09:13:28 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 09:13:28 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 09:13:28 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 09:13:29 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 09:13:29 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 09:13:29 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 09:13:29 - INFO - __main__ - Num examples = 10784
01/31/2025 09:13:29 - INFO - __main__ - Batch size = 8
01/31/2025 09:19:46 - INFO - __main__ - Evaluation metrics: {'exact_match': 0.3973509933774834, 'f1': 1.96930824924695}
01/31/2025 09:26:45 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 09:26:51 - INFO - __main__ - Sample 35071 of the training set: {'input_ids': [101, 2054, 2001, 1037, 4126, 1999, 1996, 12965, 2301, 2008, 2071, 3426, 5729, 7750, 2021, 2001, 3262, 6439, 1029, 102, 2138, 4424, 10296, 2003, 3375, 1998, 4800, 1011, 8789, 1010, 2070, 15032, 1998, 6950, 1010, 2926, 1999, 19483, 2913, 1010, 2031, 5275, 2008, 2009, 2003, 1037, 3439, 1998, 2591, 2810, 1012, 1999, 3299, 1010, 9667, 1998, 5272, 8709, 1042, 7140, 3540, 11314, 5275, 1999, 1996, 2381, 1997, 13798, 2008, 15949, 2004, 2019, 4767, 2106, 2025, 4839, 1999, 1996, 12965, 2301, 1025, 2008, 2111, 2612, 3764, 1997, 1000, 2061, 9527, 2100, 1010, 1000, 2029, 3615, 2000, 4424, 4490, 1012, 2061, 9527, 2100, 2001, 1037, 4126, 2008, 2001, 2411, 6439, 1010, 2021, 2823, 14248, 8949, 1006, 2156, 2061, 9527, 2100, 2375, 1007, 1012, 2002, 2626, 1010, 1000, 1005, 13798, 1005, 2003, 2019, 11028, 1997, 1996, 2715, 2110, 1010, 1996, 3919, 4329, 1010, 1998, 16498, 1012, 1000, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 98, 'end_positions': 100}.
01/31/2025 09:26:51 - INFO - __main__ - Sample 75633 of the training set: {'input_ids': [101, 2054, 2515, 1000, 19734, 3900, 3690, 1000, 102, 2105, 1996, 2203, 1997, 1996, 3708, 2301, 1998, 2046, 1996, 3983, 2301, 1010, 1996, 23214, 3690, 2001, 4417, 2011, 1996, 5853, 1997, 1996, 23214, 3750, 1012, 2076, 2023, 2051, 1010, 2900, 2318, 2049, 20181, 1998, 3123, 2000, 2088, 2373, 3570, 1012, 2023, 3690, 2171, 2965, 1000, 4372, 7138, 6675, 3627, 1000, 1012, 1999, 2900, 1010, 1996, 23214, 6418, 2318, 1999, 1996, 15914, 1010, 10060, 1996, 5915, 20181, 2011, 1996, 2887, 3209, 2247, 2647, 3210, 1012, 2172, 2470, 2038, 4208, 2006, 1996, 3314, 1997, 12532, 16778, 11231, 3012, 6431, 13717, 2007, 1996, 3025, 22065, 2558, 1012, 1999, 1996, 4120, 3920, 2887, 5784, 2419, 2011, 20868, 12352, 4213, 18765, 29493, 4048, 1010, 14831, 2114, 1996, 4879, 17510, 3565, 9153, 2618, 1010, 1998, 2211, 6575, 2005, 1996, 3181, 2535, 1997, 1996, 2691, 2111, 1012, 2027, 9511, 1996, 7069, 1010, 1998, 4208, 2025, 2006, 2576, 2824, 2021, 2006, 2591, 2749, 1998, 13818, 1012, 2027, 5837, 2119, 27255, 1998, 20181, 3399, 2004, 7344, 1998, 9530, 16294, 2075, 1012, 2027, 13233, 1996, 5197, 1997, 2759, 19320, 1999, 1996, 2458, 1997, 2715, 2900, 1012, 2027, 11792, 2381, 2011, 2478, 1996, 4725, 1997, 2591, 2381, 1012, 2009, 2001, 2025, 2127, 1996, 2927, 1997, 1996, 23214, 3690, 2008, 1996, 2887, 2231, 2211, 2635, 20181, 5667, 1012, 2900, 4423, 2049, 2510, 2537, 2918, 2011, 3098, 9433, 2015, 1999, 2536, 5269, 1012, 1996, 1044, 7677, 22427, 2080, 1006, 2162, 2436, 1007, 2001, 2999, 2007, 1037, 2162, 2533, 1998, 1037, 3987, 2533, 1012, 1996, 16352, 2465, 4265, 2307, 10520, 1996, 2206, 2086, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 55, 'end_positions': 58}.
01/31/2025 09:26:51 - INFO - __main__ - Sample 87516 of the training set: {'input_ids': [101, 2043, 2001, 1996, 4956, 2610, 2552, 2979, 1029, 102, 1996, 11523, 4956, 2610, 2552, 2580, 1037, 2715, 2610, 2486, 2011, 14879, 1996, 16405, 2099, 8584, 1997, 1996, 2486, 1998, 2049, 4204, 1010, 1998, 4372, 17084, 2075, 2009, 2004, 6414, 2019, 5812, 1997, 1996, 8268, 2291, 1012, 2037, 3105, 2001, 9706, 10893, 14656, 1025, 2000, 5441, 1996, 3521, 1998, 10439, 2890, 22342, 12290, 2005, 1996, 5434, 2000, 2832, 2429, 2000, 1996, 2375, 1012, 2023, 2001, 2200, 2367, 2000, 1996, 1005, 6803, 2944, 1005, 1997, 1996, 2610, 2486, 2008, 2018, 2042, 2764, 1999, 2605, 1010, 2073, 1996, 2610, 2486, 2499, 2306, 1996, 11709, 1997, 1996, 14689, 4747, 21823, 3367, 2110, 2004, 2019, 5331, 1997, 1996, 3691, 1997, 1996, 11590, 1998, 20903, 2004, 2112, 1997, 1996, 8677, 2110, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 11, 'end_positions': 11}.
01/31/2025 09:26:52 - INFO - __main__ - ***** Running training *****
01/31/2025 09:26:52 - INFO - __main__ - Num examples = 88524
01/31/2025 09:26:52 - INFO - __main__ - Num Epochs = 3
01/31/2025 09:26:52 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 09:26:52 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 09:26:52 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 09:26:52 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 09:26:54 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 09:26:54 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 09:27:04 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 09:27:06 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 09:27:10 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 09:27:28 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 09:27:28 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 09:27:28 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 09:27:29 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 09:27:29 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 09:27:29 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 09:27:29 - INFO - __main__ - Num examples = 10784
01/31/2025 09:27:29 - INFO - __main__ - Batch size = 8
01/31/2025 09:29:59 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 09:30:32 - INFO - __main__ - Sample 35472 of the training set: {'input_ids': [101, 2040, 2001, 6472, 2006, 2254, 2459, 1010, 3355, 1029, 102, 22679, 7943, 2006, 2254, 2459, 1010, 3355, 1010, 2043, 5639, 22999, 2253, 2077, 1037, 7493, 4686, 1999, 6646, 1012, 2021, 1996, 6393, 2001, 3243, 4030, 2349, 2000, 1996, 2224, 1997, 15382, 9887, 2029, 2920, 15242, 5567, 25697, 2015, 1997, 5292, 4783, 3022, 13931, 1010, 2029, 4594, 2005, 2116, 1999, 29391, 2037, 5025, 7781, 2005, 2116, 2086, 1012, 2348, 5606, 1997, 3633, 2020, 7331, 2000, 2331, 1999, 1996, 2142, 2163, 2076, 1996, 3955, 1998, 2220, 3865, 1010, 2069, 2702, 2111, 4661, 22999, 1006, 2040, 2018, 16301, 2035, 1997, 2010, 5574, 2916, 1007, 2020, 2941, 6472, 3188, 2000, 3118, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 20, 'end_positions': 21}.
01/31/2025 09:30:32 - INFO - __main__ - Sample 18376 of the training set: {'input_ids': [101, 4998, 2013, 2555, 1010, 2054, 2003, 2019, 2742, 1997, 1037, 5387, 2008, 2089, 3747, 9329, 1029, 102, 2028, 8192, 1517, 1996, 2062, 2691, 2426, 22978, 2015, 1517, 5218, 2000, 1037, 3528, 1997, 1037, 2653, 2008, 2003, 1037, 8281, 1997, 1037, 3327, 2177, 1997, 1996, 2653, 1005, 1055, 7492, 1012, 1996, 2744, 2003, 4162, 2087, 2411, 2000, 3164, 4613, 7060, 1010, 2021, 1037, 9329, 2089, 2036, 2022, 4225, 2011, 2060, 5876, 1010, 2107, 2004, 2591, 2465, 1012, 1037, 9329, 2008, 2003, 3378, 2007, 1037, 3327, 2591, 2465, 2064, 2022, 12061, 1037, 17522, 2571, 6593, 1010, 1037, 9329, 2008, 2003, 3378, 2007, 1037, 3327, 5636, 2177, 2064, 2022, 12061, 2004, 3802, 7295, 9890, 6593, 1010, 1998, 1037, 3164, 9329, 2089, 2022, 12061, 1037, 19723, 20282, 22471, 1012, 2429, 2000, 2023, 6210, 1010, 2151, 3528, 1997, 1037, 2653, 17367, 1000, 1037, 9329, 1000, 1010, 2164, 2151, 3115, 9903, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 74, 'end_positions': 75}.
01/31/2025 09:30:32 - INFO - __main__ - Sample 40254 of the training set: {'input_ids': [101, 2054, 2003, 1996, 8940, 2173, 1997, 10424, 14194, 8525, 19137, 14043, 1029, 102, 2036, 1997, 2350, 3602, 1999, 20759, 20098, 3900, 2003, 1996, 8232, 2139, 2474, 9530, 16643, 8525, 10446, 1006, 2030, 8232, 13523, 21885, 1007, 1012, 2076, 1996, 2034, 5109, 1997, 23464, 4336, 2023, 2675, 2001, 1996, 2364, 9594, 1997, 2103, 2166, 1012, 2006, 1996, 2675, 2024, 1996, 9298, 4014, 3527, 1517, 1996, 2835, 1997, 5336, 2231, 1517, 1998, 1996, 22460, 4956, 5040, 1012, 1996, 5040, 2003, 1996, 8940, 2173, 1997, 10424, 14194, 8525, 19137, 14043, 1010, 5348, 4980, 26205, 2571, 3900, 1998, 2310, 7229, 9793, 17343, 1012, 2178, 3862, 2675, 2003, 8232, 23564, 25060, 2007, 1996, 19191, 6231, 1997, 10391, 5003, 9496, 9793, 2139, 23564, 25060, 1012, 2006, 2049, 2148, 2217, 1010, 14412, 21361, 10225, 15305, 1010, 2320, 5039, 1997, 1996, 19439, 10225, 15305, 3428, 1010, 2003, 2085, 1996, 2688, 1997, 11584, 2840, 1012, 1037, 2261, 5991, 4514, 1997, 8232, 23564, 25060, 2003, 1996, 21442, 3540, 3527, 3972, 5984, 1010, 2178, 2350, 7538, 7688, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 72, 'end_positions': 74}.
01/31/2025 09:30:33 - INFO - __main__ - ***** Running training *****
01/31/2025 09:30:33 - INFO - __main__ - Num examples = 88524
01/31/2025 09:30:33 - INFO - __main__ - Num Epochs = 3
01/31/2025 09:30:33 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 09:30:33 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 09:30:33 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 09:30:33 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 09:30:35 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 09:30:35 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 09:30:45 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 09:30:47 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 09:30:51 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 09:31:09 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 09:31:09 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 09:31:09 - INFO - fms_mo.custom_ext_kernels.utils - Found CUTLASS include path under /home/cliu22/cutlass.
01/31/2025 09:31:09 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS gemm functions have been loaded and registered to torch.ops.cutlass_gemm.
01/31/2025 09:31:09 - INFO - fms_mo.custom_ext_kernels.utils - A few CUTLASS conv2d functions have been loaded and registered to torch.ops.cutlass.
01/31/2025 09:31:10 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 09:31:10 - INFO - __main__ - Num examples = 10784
01/31/2025 09:31:10 - INFO - __main__ - Batch size = 8
01/31/2025 09:37:24 - INFO - __main__ - Evaluation metrics: {'exact_match': 0.40681173131504256, 'f1': 2.0039608614142876}
01/31/2025 10:09:20 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:09:27 - INFO - __main__ - Sample 63811 of the training set: {'input_ids': [101, 2054, 2079, 4776, 21273, 2015, 2302, 17419, 2050, 2031, 2000, 2224, 2005, 9141, 1029, 102, 1996, 8331, 1999, 1996, 24873, 21297, 6790, 3397, 24873, 21297, 10085, 17250, 4442, 2008, 6985, 1996, 4176, 2114, 23996, 1998, 15245, 1012, 1999, 2070, 2427, 24873, 21297, 28788, 2089, 2036, 5383, 1037, 16464, 28815, 1516, 2417, 19610, 8649, 4135, 8428, 1999, 2070, 2427, 1010, 2665, 10381, 10626, 10085, 6820, 28741, 1999, 2500, 1006, 8314, 1999, 1996, 12123, 1007, 1516, 1998, 3073, 7722, 3665, 2306, 2037, 9214, 1012, 16464, 28815, 2003, 2036, 8314, 1999, 1996, 2668, 12123, 1012, 2427, 2007, 2092, 1011, 2764, 17419, 2050, 3227, 2036, 2031, 2668, 6470, 2770, 2035, 2146, 2037, 4230, 2682, 1998, 2917, 1996, 9535, 1010, 1996, 3356, 2028, 4755, 2668, 19390, 2096, 1996, 2896, 2028, 7883, 2009, 11043, 1012, 6125, 1997, 6178, 9386, 5134, 1999, 1996, 2303, 2813, 1998, 2105, 1996, 9535, 4651, 2668, 2090, 1996, 2364, 2668, 6470, 1998, 2000, 3033, 1997, 1996, 6903, 2008, 2342, 7722, 1998, 20435, 1012, 2119, 1997, 1996, 2350, 6470, 1010, 2926, 1996, 3356, 2028, 1010, 2064, 10216, 2668, 2011, 21012, 1012, 1999, 2070, 4776, 21273, 2015, 1996, 2830, 2203, 1997, 1996, 3356, 2668, 6258, 2003, 11792, 2007, 6650, 2000, 2433, 1037, 2540, 1010, 2096, 1999, 1996, 2830, 4515, 1997, 2116, 3011, 22769, 2015, 2070, 1997, 1996, 6470, 2008, 7532, 1996, 3356, 1998, 2896, 2364, 6470, 3853, 2004, 8072, 1012, 2427, 2007, 9996, 2764, 2030, 2053, 17419, 2050, 3227, 2031, 2053, 2668, 6470, 1998, 11160, 2006, 1996, 9141, 2306, 1996, 24873, 21297, 2005, 12771, 20435, 1998, 7722, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 244, 'end_positions': 248}.
01/31/2025 10:09:27 - INFO - __main__ - Sample 35502 of the training set: {'input_ids': [101, 2429, 2000, 21628, 4817, 1010, 2054, 7017, 1997, 5292, 4783, 3022, 13931, 24320, 2020, 4379, 2000, 2331, 5216, 13187, 2090, 3299, 1998, 2889, 1029, 102, 2508, 4682, 25526, 2319, 1010, 1037, 2934, 1997, 2375, 2012, 3996, 2375, 2082, 1010, 3090, 1999, 2727, 2008, 2010, 2817, 2179, 2008, 2043, 5292, 4783, 3022, 13931, 24320, 1999, 2331, 6531, 3572, 2020, 9551, 2013, 10652, 2000, 6503, 1997, 1996, 2553, 2008, 2045, 2001, 1000, 1037, 2871, 3867, 3112, 3446, 1999, 2035, 3007, 3572, 2013, 3301, 2000, 2786, 1012, 1000, 6660, 1010, 1037, 2817, 2011, 8923, 21628, 4817, 1999, 1037, 2375, 3319, 3720, 8509, 1996, 3112, 3446, 1999, 5292, 4783, 3022, 13931, 3572, 5994, 2331, 5216, 13187, 2130, 3020, 1010, 4531, 2008, 2090, 1000, 3299, 1998, 2889, 1010, 3155, 4700, 3867, 1997, 1996, 5292, 4783, 3022, 24320, 6406, 2011, 2331, 5216, 13187, 2020, 4379, 1012, 1000, 1996, 2367, 3616, 2024, 4321, 6210, 2389, 1010, 2738, 2084, 27737, 1012, 10650, 3286, 1005, 1055, 6747, 3504, 2012, 1996, 7017, 1997, 2035, 2331, 6531, 3572, 11674, 1010, 2096, 1996, 2500, 2298, 2069, 2012, 3572, 2025, 11674, 3188, 2000, 5292, 4783, 3022, 13931, 3319, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 125, 'end_positions': 125}.
01/31/2025 10:09:27 - INFO - __main__ - Sample 29827 of the training set: {'input_ids': [101, 2054, 2744, 2106, 4401, 1997, 1996, 5148, 2162, 2224, 2000, 6235, 2009, 1029, 102, 5758, 1998, 8220, 2013, 1996, 2162, 2024, 2145, 1037, 2350, 5387, 1999, 2137, 4331, 1012, 2028, 2217, 5328, 1996, 2162, 2004, 1037, 4072, 2112, 1997, 1996, 29174, 3343, 1010, 2029, 3039, 1996, 4099, 2000, 5454, 1996, 2051, 1998, 2173, 1997, 8309, 1012, 2500, 3602, 1996, 1057, 1012, 1055, 1012, 2081, 2350, 6143, 12154, 2004, 1996, 13009, 2020, 3249, 1999, 6239, 1010, 1998, 2011, 3285, 2119, 4924, 1998, 7211, 2020, 6637, 2005, 2137, 2490, 1010, 2012, 1996, 10961, 1997, 2037, 6956, 1999, 24809, 1012, 4401, 2156, 1996, 4736, 2004, 1037, 1000, 24209, 8490, 14503, 2063, 1000, 1517, 2019, 10866, 5949, 1997, 2137, 2668, 1998, 8813, 1999, 1037, 4736, 2008, 2106, 2025, 5142, 2149, 5426, 1012, 10069, 1997, 2178, 24209, 8490, 14503, 2063, 2031, 2042, 2350, 5876, 1999, 3097, 3343, 14379, 2412, 2144, 1012, 1996, 4433, 2150, 5186, 19657, 1010, 1998, 2343, 11296, 3092, 2009, 1999, 3381, 1010, 6932, 1996, 2510, 1006, 1996, 2390, 2926, 1007, 2000, 11160, 4498, 2588, 7314, 1012, 2008, 2992, 1996, 3277, 1997, 2129, 2092, 1996, 2658, 2510, 7686, 3452, 2137, 2554, 1998, 5300, 1025, 1996, 3548, 4050, 2165, 1996, 2597, 2008, 2037, 2326, 3421, 1996, 3284, 1998, 2190, 2137, 5300, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 103, 'end_positions': 109}.
01/31/2025 10:09:28 - INFO - __main__ - ***** Running training *****
01/31/2025 10:09:28 - INFO - __main__ - Num examples = 88524
01/31/2025 10:09:28 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:09:28 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:09:28 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:09:28 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:09:28 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:09:29 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:09:29 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:09:39 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 10:09:42 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:09:46 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 10:10:03 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 10:10:03 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 10:10:04 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 10:10:04 - INFO - __main__ - Num examples = 10784
01/31/2025 10:10:04 - INFO - __main__ - Batch size = 8
01/31/2025 10:13:07 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:13:10 - INFO - __main__ - Sample 72036 of the training set: {'input_ids': [101, 2054, 2003, 1996, 2171, 1997, 1996, 2149, 1011, 2241, 3813, 1999, 1037, 2537, 1011, 6631, 3820, 2007, 9719, 1029, 102, 1999, 3522, 2086, 3278, 12450, 1997, 12195, 3019, 3806, 2031, 2042, 3603, 1999, 1996, 2181, 2124, 2004, 9706, 8093, 7716, 4221, 1999, 9719, 1005, 7262, 3171, 4224, 1006, 25212, 2480, 1007, 1010, 2055, 12862, 3717, 1006, 11518, 2661, 1007, 2148, 1997, 12967, 24137, 2140, 2012, 3943, 7737, 2629, 1531, 2871, 1532, 1050, 1998, 3590, 7737, 28154, 1531, 1014, 1532, 1041, 1012, 2174, 1010, 4977, 1005, 1055, 12195, 15827, 3316, 2031, 11570, 2119, 3019, 3806, 1998, 3514, 4219, 2144, 2286, 1012, 9719, 17183, 2906, 12921, 2049, 7803, 3675, 2007, 5279, 1999, 2494, 1010, 1998, 2007, 8341, 1999, 2289, 1012, 9719, 1998, 3956, 17183, 2906, 12921, 2037, 7803, 3675, 1999, 2230, 1010, 1998, 1999, 2257, 2249, 1010, 1996, 2149, 1011, 2241, 3813, 7015, 2943, 3133, 2046, 1037, 2537, 1011, 6631, 3820, 2007, 1996, 18543, 2231, 4953, 1996, 3796, 1005, 1055, 3293, 2458, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 141, 'end_positions': 142}.
01/31/2025 10:13:10 - INFO - __main__ - Sample 15985 of the training set: {'input_ids': [101, 2054, 2515, 1996, 2312, 2193, 1997, 18168, 14643, 8496, 2191, 1996, 28283, 22311, 6024, 2015, 1029, 102, 1996, 1999, 21031, 7542, 1997, 28283, 22311, 6024, 2015, 2003, 3375, 1010, 11974, 2138, 1997, 1996, 2152, 2193, 1997, 12005, 27466, 1010, 2021, 2003, 2714, 2000, 1996, 8581, 4155, 1012, 13973, 2038, 2062, 21963, 2015, 1997, 17463, 19234, 1009, 3720, 2084, 3009, 1010, 2066, 3972, 2015, 1006, 1000, 1997, 1009, 1996, 1031, 13994, 1033, 1000, 1007, 1010, 2021, 2025, 2004, 2116, 2004, 3059, 1006, 2029, 2038, 21396, 1010, 8902, 1010, 11265, 2140, 1010, 4385, 1012, 1007, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 28, 'end_positions': 28}.
01/31/2025 10:13:10 - INFO - __main__ - Sample 73559 of the training set: {'input_ids': [101, 2005, 2129, 2146, 2106, 1996, 3000, 5715, 4839, 1029, 102, 1999, 6940, 1010, 2206, 1996, 14726, 4154, 1997, 1996, 2413, 2390, 2011, 1996, 7074, 1999, 1996, 9341, 1011, 10734, 2162, 1010, 2413, 3667, 1998, 6102, 24517, 8243, 3000, 1998, 2580, 1996, 3000, 5715, 1012, 1996, 5715, 6354, 2005, 2048, 2706, 2077, 2009, 2001, 10560, 2011, 1996, 2413, 2390, 1010, 2007, 2172, 2668, 14740, 1012, 1996, 2434, 2417, 23562, 1997, 1996, 5715, 2150, 18407, 1997, 1996, 6102, 4329, 1025, 1999, 4885, 2372, 1997, 1996, 2413, 4750, 2283, 2234, 2000, 4924, 1998, 3591, 1996, 2047, 3354, 2231, 2007, 2028, 1997, 1996, 2434, 5715, 23562, 1025, 2009, 2001, 2872, 1006, 1998, 2003, 2145, 1999, 2173, 1007, 1999, 1996, 8136, 1997, 8748, 17497, 1010, 2279, 2000, 2010, 2330, 13123, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 49, 'end_positions': 50}.
01/31/2025 10:13:11 - INFO - __main__ - ***** Running training *****
01/31/2025 10:13:11 - INFO - __main__ - Num examples = 88524
01/31/2025 10:13:11 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:13:11 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:13:11 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:13:11 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:13:11 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:13:12 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:13:12 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:13:23 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 10:13:25 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:13:29 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 10:13:46 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 10:13:46 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 10:13:47 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 10:13:47 - INFO - __main__ - Num examples = 10784
01/31/2025 10:13:47 - INFO - __main__ - Batch size = 8
01/31/2025 10:21:57 - INFO - __main__ - Evaluation metrics: {'exact_match': 79.8864711447493, 'f1': 87.90196963597948}
01/31/2025 10:23:32 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:24:05 - INFO - __main__ - Sample 35250 of the training set: {'input_ids': [101, 2054, 11768, 2106, 12418, 29453, 2000, 5547, 1029, 102, 12418, 5462, 2000, 5547, 16635, 3806, 11768, 2013, 2049, 3795, 3450, 2011, 2871, 1003, 2011, 2325, 1010, 2007, 2263, 10807, 2095, 2004, 1996, 26163, 2095, 1012, 2009, 2003, 3205, 1999, 2665, 5051, 10732, 1521, 1055, 5009, 2000, 11006, 2099, 8139, 2008, 7644, 2877, 8139, 8712, 2429, 2000, 2037, 6043, 2006, 15169, 1010, 4785, 1998, 2943, 1998, 2129, 2665, 2037, 3688, 2024, 1012, 1999, 2281, 2249, 1010, 12418, 4396, 3416, 2041, 1997, 2321, 3205, 8139, 11153, 1006, 4852, 2049, 3556, 2000, 1019, 1012, 1015, 2013, 1018, 1012, 1023, 1010, 2029, 2009, 4227, 1999, 1996, 3025, 5464, 2013, 2255, 2230, 1007, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 14, 'end_positions': 15}.
01/31/2025 10:24:05 - INFO - __main__ - Sample 33131 of the training set: {'input_ids': [101, 2040, 2001, 1996, 7786, 1997, 13083, 1999, 15028, 4647, 1029, 102, 2055, 2048, 4595, 3142, 15526, 2020, 3876, 2045, 1999, 15028, 4647, 2076, 1996, 3627, 1997, 11801, 11703, 19315, 12022, 4173, 7987, 23998, 16122, 11261, 1012, 1996, 3142, 5272, 13109, 21694, 2758, 2008, 7987, 23998, 4015, 1996, 3548, 2040, 2018, 4061, 2104, 2032, 2000, 2008, 2874, 1012, 2023, 2001, 1037, 5171, 3142, 2103, 1999, 2049, 13120, 1010, 2004, 2009, 2001, 2284, 1999, 1037, 6143, 3295, 2379, 1996, 2712, 2006, 1037, 2314, 2479, 4625, 2011, 1996, 3081, 13635, 1010, 1996, 4461, 2346, 2008, 4198, 1996, 2874, 2000, 4199, 1010, 1996, 3007, 1997, 1996, 3400, 1012, 1996, 2803, 1997, 1996, 2103, 2001, 2284, 1999, 1996, 2556, 1011, 2154, 10971, 1997, 1996, 8232, 2139, 2474, 6819, 25892, 1012, 2182, 2001, 1996, 7057, 1998, 1996, 5153, 1997, 1996, 4003, 2080, 21692, 1998, 1996, 11703, 19042, 2271, 21692, 1010, 2029, 3961, 1996, 2048, 2364, 19589, 1997, 1996, 2103, 1012, 1996, 4003, 2080, 14788, 2000, 1996, 4493, 2655, 2063, 2139, 10582, 1010, 2632, 5302, 3981, 1010, 1998, 1996, 11703, 19042, 2271, 14788, 2000, 2655, 2063, 2139, 3050, 9298, 24164, 7352, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 28, 'end_positions': 35}.
01/31/2025 10:24:05 - INFO - __main__ - Sample 14094 of the training set: {'input_ids': [101, 2043, 2020, 1996, 2034, 3293, 2003, 4523, 2511, 1029, 102, 1999, 2960, 1010, 1996, 2034, 2003, 4523, 2020, 2511, 1999, 2660, 1998, 1996, 2142, 2163, 1012, 1999, 9566, 4179, 1010, 4404, 1010, 1996, 2088, 2150, 1996, 2034, 3293, 2003, 2361, 1999, 1996, 2149, 1012, 2049, 2034, 8013, 2001, 2366, 1999, 2281, 2960, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 12, 'end_positions': 12}.
01/31/2025 10:24:07 - INFO - __main__ - ***** Running training *****
01/31/2025 10:24:07 - INFO - __main__ - Num examples = 88524
01/31/2025 10:24:07 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:24:07 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:24:07 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:24:07 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:24:07 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:24:08 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:24:08 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:24:19 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 10:24:21 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:24:25 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 10:24:43 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 10:24:43 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 10:24:44 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 10:24:44 - INFO - __main__ - Num examples = 10784
01/31/2025 10:24:44 - INFO - __main__ - Batch size = 8
01/31/2025 10:27:01 - INFO - __main__ - Evaluation metrics: {'exact_match': 0.1986754966887417, 'f1': 6.873674093201651}
01/31/2025 10:28:05 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:28:08 - INFO - __main__ - Sample 84547 of the training set: {'input_ids': [101, 2054, 2020, 1996, 4513, 2040, 13588, 2493, 1999, 1996, 2118, 1997, 3000, 2170, 1029, 102, 2348, 1996, 8332, 2944, 3024, 2011, 1996, 2118, 1997, 3000, 1010, 2073, 3076, 2372, 2024, 4758, 2011, 4513, 1000, 5972, 1010, 1000, 3024, 1037, 3115, 2005, 5534, 1010, 1996, 4646, 1997, 2023, 2944, 2165, 2012, 2560, 2093, 2367, 3596, 1012, 2045, 2020, 5534, 2008, 2018, 1037, 2291, 1997, 18658, 3005, 4252, 8280, 1037, 2200, 3563, 8882, 1025, 2023, 2944, 11121, 2000, 3345, 15744, 1012, 2045, 2001, 1037, 9234, 2030, 14924, 4818, 2944, 2241, 2006, 1996, 2291, 2012, 2118, 1997, 4345, 2073, 4252, 1998, 3029, 2001, 11519, 7941, 3550, 1998, 3716, 2001, 2062, 1997, 1037, 2236, 2923, 3267, 1012, 2045, 2020, 2036, 5534, 2008, 4117, 2122, 4275, 1010, 2478, 1996, 9234, 2944, 2021, 2383, 1037, 22493, 3029, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 35, 'end_positions': 35}.
01/31/2025 10:28:08 - INFO - __main__ - Sample 27993 of the training set: {'input_ids': [101, 2129, 2172, 2106, 1996, 12256, 12928, 2099, 2918, 3643, 3623, 2000, 1999, 2286, 1029, 102, 1996, 12194, 2918, 3774, 1997, 7824, 1998, 2976, 3914, 3964, 1999, 9141, 2648, 1996, 2976, 3914, 5085, 1998, 1996, 1057, 1012, 1055, 1012, 9837, 1010, 4606, 10042, 2218, 2011, 12816, 10253, 4896, 2012, 2976, 3914, 5085, 1012, 1996, 10426, 12194, 2918, 2038, 3445, 2013, 3155, 4278, 4551, 6363, 1999, 2807, 1010, 2000, 5385, 4551, 1999, 2384, 1010, 1998, 2058, 11910, 4551, 1999, 2286, 1012, 1996, 3815, 1997, 5356, 1999, 9141, 2003, 3445, 1006, 2030, 10548, 1007, 2011, 1996, 4506, 1997, 1996, 2976, 3914, 2291, 1012, 2809, 2335, 1037, 2095, 1010, 1996, 2260, 1011, 2711, 2976, 2330, 3006, 2837, 3113, 2000, 5646, 1057, 1012, 1055, 1012, 12194, 3343, 1012, 2296, 2449, 2154, 1010, 1996, 2976, 3914, 2291, 24255, 1999, 2330, 3006, 3136, 2000, 4287, 2041, 2008, 12194, 3343, 1012, 2065, 1996, 2976, 3914, 14714, 2000, 3623, 1996, 2769, 4425, 1010, 2009, 2097, 4965, 12012, 1006, 2107, 2004, 1057, 1012, 1055, 1012, 9837, 9547, 1007, 10812, 2135, 2013, 5085, 1999, 3863, 2005, 6363, 1012, 18868, 1010, 2009, 2097, 5271, 12012, 2000, 1996, 5085, 1999, 3863, 2005, 6363, 1010, 2000, 2202, 6363, 2041, 1997, 9141, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 74, 'end_positions': 76}.
01/31/2025 10:28:08 - INFO - __main__ - Sample 33042 of the training set: {'input_ids': [101, 2043, 2106, 13083, 7806, 1029, 102, 1999, 13138, 2620, 1010, 2332, 2508, 1045, 1997, 16146, 1010, 2007, 2019, 2390, 3605, 1997, 16146, 6810, 1010, 13973, 2015, 1010, 21260, 3366, 1998, 18831, 2013, 1996, 2344, 1997, 10250, 4017, 2527, 3567, 1010, 4201, 6859, 2000, 13083, 1998, 2006, 2654, 2244, 4663, 1037, 7806, 1012, 5595, 4595, 24812, 2020, 3140, 2000, 2681, 1012, 9736, 2107, 2004, 7839, 2632, 1011, 11113, 8237, 1998, 7839, 18904, 2050, 9587, 21737, 2094, 2023, 8340, 2013, 2037, 11419, 13083, 1012, 2044, 1996, 3017, 3377, 1998, 1996, 18272, 1997, 1996, 5152, 2313, 1996, 2103, 2001, 4055, 2090, 2216, 2040, 2018, 4194, 1999, 1996, 9187, 1010, 2429, 2000, 1996, 10896, 1999, 1996, 2222, 12322, 2890, 3972, 16360, 8445, 14428, 3372, 1006, 2338, 1997, 4353, 1007, 1012, 2508, 1045, 4379, 1996, 2103, 2047, 23010, 1997, 2375, 1010, 1996, 6519, 2015, 1997, 13083, 1010, 2029, 2101, 2020, 3668, 2000, 1996, 2878, 2983, 1997, 13083, 1012, 23166, 15628, 1996, 2103, 3133, 1037, 2047, 3439, 2754, 1999, 2029, 1037, 2047, 2554, 1998, 1037, 2047, 2653, 2764, 1010, 5716, 1996, 3978, 1997, 1996, 2839, 1997, 1996, 13083, 2078, 2111, 2004, 2027, 2024, 2124, 2651, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 47, 'end_positions': 48}.
01/31/2025 10:28:09 - INFO - __main__ - ***** Running training *****
01/31/2025 10:28:09 - INFO - __main__ - Num examples = 88524
01/31/2025 10:28:09 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:28:09 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:28:09 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:28:09 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:28:09 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:28:10 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:28:10 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:28:21 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 10:28:24 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:28:28 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 10:28:45 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 10:28:45 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 10:28:46 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 10:28:46 - INFO - __main__ - Num examples = 10784
01/31/2025 10:28:46 - INFO - __main__ - Batch size = 8
01/31/2025 10:30:57 - INFO - __main__ - Evaluation metrics: {'exact_match': 73.64238410596026, 'f1': 83.64928236252847}
01/31/2025 10:32:52 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:32:54 - INFO - __main__ - Sample 30626 of the training set: {'input_ids': [101, 2073, 2024, 18423, 1998, 16122, 6593, 9232, 9063, 1029, 102, 18749, 18260, 2003, 2019, 9007, 2008, 7807, 2091, 1996, 4487, 3736, 9468, 18428, 3207, 18749, 22282, 2000, 2049, 6922, 3033, 1010, 18423, 1998, 16122, 6593, 9232, 1012, 18423, 1998, 16122, 6593, 9232, 2064, 2022, 9063, 2011, 1996, 2235, 20014, 4355, 3170, 1012, 3155, 3515, 3867, 1997, 1996, 4639, 2313, 3965, 2069, 2235, 8310, 1997, 18749, 18260, 1998, 2024, 4039, 2000, 4521, 4895, 7512, 3672, 2098, 6501, 1011, 2241, 9440, 1012, 2023, 2003, 4141, 2124, 2004, 18749, 22282, 2046, 3917, 6651, 1012, 18749, 22282, 2046, 3917, 6651, 9783, 4235, 2011, 5636, 4348, 1025, 2062, 2084, 3938, 3867, 1997, 7243, 1997, 2264, 4004, 6934, 2024, 18749, 22282, 2046, 3917, 4630, 1010, 1999, 5688, 2000, 2055, 1019, 3867, 1997, 2111, 1997, 2642, 2647, 6934, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 48, 'end_positions': 51}.
01/31/2025 10:32:54 - INFO - __main__ - Sample 31813 of the training set: {'input_ids': [101, 10043, 2000, 2060, 4175, 7442, 2015, 1010, 2024, 2270, 3665, 1010, 4026, 2491, 1010, 21107, 1010, 1998, 2495, 1996, 5368, 1997, 2334, 2030, 2110, 2231, 1029, 102, 1996, 2334, 10784, 2024, 3625, 2005, 4346, 1996, 4972, 2275, 2041, 1999, 1996, 2334, 2231, 2552, 2960, 2107, 2004, 3923, 4041, 1998, 5949, 2968, 1012, 2087, 2060, 2231, 2578, 2024, 3024, 2030, 12222, 2011, 1996, 6652, 2110, 2231, 1010, 2029, 21208, 2015, 2013, 3323, 2160, 1999, 3500, 2395, 1012, 2122, 2421, 2578, 2029, 2024, 3378, 2007, 2334, 2231, 1999, 2060, 3032, 1998, 2421, 2270, 3665, 1010, 2364, 4925, 1010, 4026, 2491, 1010, 21107, 1010, 2495, 2682, 23655, 2504, 1010, 2740, 1998, 4041, 1997, 2350, 6502, 3934, 1012, 1996, 2110, 2231, 14567, 1996, 2157, 2000, 2058, 15637, 3056, 2334, 2231, 6567, 1010, 2164, 3923, 4041, 1010, 1998, 11463, 8022, 2937, 3314, 2411, 3444, 14500, 1999, 2110, 2602, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 64, 'end_positions': 65}.
01/31/2025 10:32:54 - INFO - __main__ - Sample 18662 of the training set: {'input_ids': [101, 5006, 8963, 1998, 19127, 2198, 2031, 2119, 2018, 2054, 2828, 1997, 2731, 1029, 102, 2070, 1000, 2759, 1000, 6907, 5389, 2031, 2018, 3278, 4556, 2731, 1010, 2107, 2004, 5006, 8963, 1010, 19127, 2198, 1010, 1996, 3158, 13084, 2078, 3428, 1010, 9744, 1054, 6806, 19303, 1998, 20404, 2304, 5974, 1012, 9308, 1010, 5337, 2731, 2003, 2025, 4310, 2000, 1996, 4556, 6907, 1012, 2116, 2600, 1998, 3769, 5389, 2031, 2949, 5445, 1999, 3293, 2189, 3454, 2107, 2004, 2216, 3253, 2011, 1996, 2022, 8024, 10559, 2267, 1997, 2189, 1998, 2116, 4166, 5389, 2031, 2949, 5445, 1999, 2189, 2013, 5534, 2007, 4166, 3454, 1010, 2107, 2004, 1996, 7128, 2082, 1997, 2189, 1998, 17919, 2118, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 24, 'end_positions': 24}.
01/31/2025 10:32:55 - INFO - __main__ - ***** Running training *****
01/31/2025 10:32:55 - INFO - __main__ - Num examples = 88524
01/31/2025 10:32:55 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:32:55 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:32:55 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:32:55 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:32:55 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:32:56 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:32:56 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:33:06 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 10:33:08 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:33:12 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 10:33:29 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 10:33:29 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 10:33:30 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 10:33:30 - INFO - __main__ - Num examples = 10784
01/31/2025 10:33:30 - INFO - __main__ - Batch size = 8
01/31/2025 10:35:29 - INFO - __main__ - Evaluation metrics: {'exact_match': 73.64238410596026, 'f1': 83.64928236252847}
01/31/2025 10:36:37 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:37:13 - INFO - __main__ - Sample 3841 of the training set: {'input_ids': [101, 2054, 2038, 3047, 2000, 2070, 2040, 10214, 2000, 5993, 2000, 2025, 6186, 1029, 102, 1996, 2047, 2259, 2335, 2988, 2008, 1000, 2231, 4584, 1999, 7211, 1998, 20980, 2031, 2056, 2027, 2024, 11538, 1996, 25938, 1012, 1999, 2019, 9353, 2243, 19779, 3709, 21693, 4765, 1997, 1996, 11251, 1997, 2311, 9537, 1999, 1996, 10833, 1010, 1996, 2120, 2458, 1998, 5290, 3222, 2056, 2006, 2089, 2676, 2008, 2009, 2018, 7462, 2019, 7450, 2000, 5335, 2810, 4781, 2005, 3078, 1998, 2690, 2816, 1999, 3541, 2752, 1012, 8519, 2024, 15252, 1996, 4433, 1010, 1996, 3222, 2056, 1012, 1000, 2000, 5787, 8090, 1010, 4584, 3724, 3008, 2000, 3696, 1037, 6254, 1010, 2029, 27315, 2068, 2013, 3173, 8090, 1010, 1999, 3863, 1997, 2769, 1010, 2021, 2070, 2040, 4188, 2000, 3696, 2020, 5561, 1012, 1996, 7909, 8310, 9426, 2013, 2082, 2000, 2082, 2021, 2020, 3155, 1996, 2168, 1012, 1999, 7658, 16600, 1010, 3008, 2020, 3253, 1037, 7427, 11126, 2012, 1022, 1010, 5385, 13751, 1999, 5356, 1998, 1037, 2566, 1011, 6687, 11550, 1997, 3053, 1019, 1010, 5174, 13751, 1012, 7297, 1010, 4584, 2109, 2060, 4725, 1997, 9033, 7770, 6129, 1024, 11421, 2610, 3738, 3631, 2039, 8090, 2011, 3008, 1025, 1996, 4614, 2275, 2039, 11601, 5644, 2105, 1996, 2816, 1025, 1998, 4584, 3641, 1996, 2822, 2739, 2865, 2000, 2644, 7316, 2006, 2082, 25938, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 124, 'end_positions': 125}.
01/31/2025 10:37:13 - INFO - __main__ - Sample 69327 of the training set: {'input_ids': [101, 2171, 1020, 2590, 4407, 8160, 1029, 102, 1999, 1996, 2034, 5109, 1997, 1996, 3708, 2301, 1010, 2976, 4294, 1998, 3306, 6308, 4294, 2020, 6817, 2011, 4407, 8160, 2107, 2004, 6425, 2474, 13181, 4783, 1010, 2520, 2358, 11285, 3122, 1010, 2198, 5292, 14762, 5685, 1010, 2198, 2025, 2386, 1010, 2726, 1057, 1012, 4787, 1010, 1998, 5212, 19024, 1012, 3581, 6519, 2791, 2003, 2641, 4407, 1005, 1055, 4602, 4944, 1997, 1996, 2117, 2431, 1997, 1996, 3708, 2301, 1010, 2021, 2010, 16682, 2443, 2198, 22432, 15265, 3126, 1010, 3781, 1012, 1010, 18403, 20408, 1010, 4267, 26975, 1010, 1996, 4267, 3428, 1010, 1998, 12757, 19817, 25438, 21126, 1012, 1999, 7428, 1010, 2810, 2211, 2006, 1996, 2117, 3400, 1011, 2806, 4407, 2103, 2534, 1012, 1996, 4407, 3439, 3222, 2001, 2580, 1999, 3982, 2000, 7969, 1996, 3451, 1998, 6549, 2381, 1997, 1996, 2103, 1012, 1996, 3222, 9319, 1996, 4407, 4236, 1997, 3181, 3182, 1010, 5815, 3181, 3121, 1010, 5090, 1010, 4573, 1010, 5200, 1998, 4733, 2004, 2009, 5927, 4906, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 30, 'end_positions': 56}.
01/31/2025 10:37:13 - INFO - __main__ - Sample 15255 of the training set: {'input_ids': [101, 2054, 2212, 2003, 1996, 2167, 1017, 1013, 1018, 1997, 1996, 2103, 2284, 1999, 1029, 102, 20892, 1010, 3731, 2003, 3975, 2090, 2048, 7740, 4733, 1012, 1996, 2642, 2093, 1011, 2959, 2015, 1997, 1996, 2103, 2003, 1999, 1996, 5504, 2212, 1010, 3421, 2011, 3505, 6178, 13860, 2080, 2144, 2687, 1012, 1996, 2670, 2959, 2003, 1999, 1996, 5893, 2212, 1010, 3421, 2011, 4459, 11404, 1012, 2119, 2024, 8037, 1025, 1037, 3951, 2038, 2025, 3421, 1037, 3278, 4664, 1997, 3731, 1999, 2058, 1037, 2301, 1012, 1996, 2110, 1005, 1055, 3026, 2266, 1997, 1996, 2142, 2163, 4001, 2003, 7672, 3870, 6031, 1010, 2034, 2700, 1999, 2262, 1012, 1996, 2110, 1005, 1055, 3502, 2266, 1997, 1996, 2142, 2163, 4001, 2003, 7672, 3968, 2928, 3240, 1010, 2040, 2001, 2700, 1999, 2286, 2000, 9510, 2198, 11260, 2044, 11260, 1005, 1055, 6098, 1998, 13964, 2004, 1996, 2142, 2163, 3187, 1997, 2110, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 37, 'end_positions': 39}.
01/31/2025 10:37:14 - INFO - __main__ - ***** Running training *****
01/31/2025 10:37:14 - INFO - __main__ - Num examples = 88524
01/31/2025 10:37:14 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:37:14 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:37:14 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:37:14 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:37:14 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:37:15 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:37:15 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'mapping', 'tb_writer', 'force_calib_once'}
01/31/2025 10:37:15 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 10:37:25 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 10:37:27 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:37:32 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 10:37:49 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 10:37:49 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:37:50 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 10:37:50 - INFO - __main__ - Num examples = 10784
01/31/2025 10:37:50 - INFO - __main__ - Batch size = 8
01/31/2025 10:39:44 - INFO - __main__ - Evaluation metrics: {'exact': 74.15326395458845, 'f1': 80.60357290095014, 'total': 10570, 'HasAns_exact': 74.15326395458845, 'HasAns_f1': 80.60357290095014, 'HasAns_total': 10570, 'best_exact': 74.15326395458845, 'best_exact_thresh': 0.0, 'best_f1': 80.60357290095014, 'best_f1_thresh': 0.0}
01/31/2025 10:43:40 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:44:16 - INFO - __main__ - Sample 85368 of the training set: {'input_ids': [101, 2054, 2038, 1996, 4259, 2457, 5451, 2055, 12961, 14866, 6491, 1029, 102, 8363, 27462, 3513, 1998, 2569, 5581, 14310, 2005, 5231, 1010, 1996, 4171, 1011, 2489, 3570, 1997, 2277, 3200, 1010, 1996, 2755, 2008, 4234, 2003, 1037, 2976, 6209, 1010, 4385, 1012, 1010, 2031, 2036, 2042, 8781, 1010, 2021, 2031, 2042, 2641, 4973, 1997, 1996, 10605, 3653, 3217, 26792, 1999, 10561, 6742, 1998, 15189, 7565, 2005, 1996, 2554, 1012, 1996, 2120, 12652, 1000, 1999, 2643, 2057, 3404, 1000, 2038, 2042, 8315, 2004, 1037, 11371, 1010, 2021, 1996, 4259, 2457, 2038, 5451, 2008, 12961, 14866, 6491, 2003, 2025, 3412, 1999, 3267, 1012, 1037, 4984, 2457, 6996, 19768, 4058, 1005, 1055, 2157, 2000, 2224, 2004, 2049, 12652, 1037, 6019, 2013, 1996, 6331, 1010, 1000, 2007, 2643, 1010, 2035, 2477, 2024, 2825, 1000, 1010, 2138, 2009, 6913, 2053, 12157, 2005, 1037, 3327, 4676, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 97, 'end_positions': 100}.
01/31/2025 10:44:16 - INFO - __main__ - Sample 55970 of the training set: {'input_ids': [101, 28103, 2626, 1037, 12859, 4159, 2054, 1029, 102, 1999, 28058, 1010, 2096, 2145, 2045, 1010, 28103, 2626, 1037, 12859, 4159, 1000, 1996, 2839, 1997, 1996, 6841, 1010, 1000, 2030, 1000, 1996, 6270, 4552, 1997, 1996, 2277, 1012, 1000, 1999, 2009, 2002, 5228, 2048, 14848, 2015, 1024, 2034, 1010, 16725, 2024, 2025, 2000, 2022, 19775, 1025, 1998, 2117, 1010, 1000, 3424, 26654, 7066, 4991, 2024, 2000, 2022, 4914, 2046, 1996, 2995, 2277, 2011, 18336, 1012, 1000, 6516, 1010, 2010, 10652, 2001, 2008, 1037, 5896, 11137, 2277, 2323, 8676, 2069, 1997, 19723, 24454, 3686, 20373, 2040, 2031, 2042, 19775, 2006, 1037, 3167, 12633, 1997, 4752, 1012, 2002, 5837, 1996, 19802, 25879, 2923, 2929, 1005, 1055, 8998, 1997, 10527, 18336, 1006, 6643, 26010, 3676, 13876, 2964, 1007, 1012, 3859, 6920, 1010, 28103, 2187, 1996, 2177, 1010, 1998, 3913, 2386, 2726, 2002, 2140, 18418, 2015, 2165, 2058, 1996, 4105, 1010, 2877, 1996, 2277, 2067, 2000, 2563, 1999, 28769, 1012, 4821, 1010, 28103, 2150, 5462, 2000, 20373, 1005, 18336, 2004, 1996, 2069, 10213, 18336, 1012, 2002, 2001, 6427, 2006, 1996, 3978, 1997, 2010, 7613, 1997, 18919, 2008, 16725, 2052, 2025, 2022, 9636, 2323, 2027, 3280, 1999, 22813, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 21, 'end_positions': 38}.
01/31/2025 10:44:16 - INFO - __main__ - Sample 6065 of the training set: {'input_ids': [101, 1999, 2054, 2095, 2106, 21326, 1011, 1037, 1011, 3062, 2050, 2191, 29270, 2225, 2124, 1029, 102, 2225, 2288, 2010, 2502, 3338, 1999, 1996, 2095, 2456, 1010, 2043, 2002, 2211, 2000, 3965, 2005, 3324, 2006, 21326, 1011, 1037, 1011, 3062, 2050, 2636, 1012, 2225, 2234, 2000, 6162, 5038, 1998, 2003, 2411, 5827, 2007, 7065, 18400, 6026, 6108, 1011, 1062, 1005, 1055, 2476, 2007, 2010, 5857, 2000, 1996, 9680, 9587, 24848, 1005, 1055, 6383, 2541, 2201, 1996, 2630, 16550, 1012, 1996, 2630, 16550, 2003, 10862, 4396, 2426, 1996, 4602, 5099, 1011, 6154, 4042, 1010, 1998, 1996, 4187, 1998, 3361, 3112, 1997, 1996, 2201, 7013, 6937, 3037, 1999, 2225, 2004, 1037, 3135, 1012, 3529, 2004, 2019, 1999, 1011, 2160, 3135, 2005, 21326, 1011, 1037, 1011, 3062, 2050, 2636, 1010, 2225, 2550, 2636, 2005, 2060, 3324, 2013, 1996, 3830, 1010, 2164, 14068, 2666, 9033, 12439, 1010, 10846, 1010, 1998, 11503, 1005, 6902, 1012, 2002, 2036, 19275, 2718, 2774, 2005, 11320, 2850, 26775, 2483, 1010, 15935, 6309, 1010, 1998, 9965, 4027, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 25, 'end_positions': 25}.
01/31/2025 10:44:17 - INFO - __main__ - ***** Running training *****
01/31/2025 10:44:17 - INFO - __main__ - Num examples = 88524
01/31/2025 10:44:17 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:44:17 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:44:17 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:44:17 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:44:17 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:44:18 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:44:18 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:46:10 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:46:13 - INFO - __main__ - Sample 18502 of the training set: {'input_ids': [101, 2040, 2106, 1996, 5969, 28755, 16152, 1996, 5924, 2013, 1029, 102, 1996, 2446, 3400, 11438, 5924, 2076, 2088, 2162, 1045, 1998, 2001, 4041, 2006, 2593, 17827, 2075, 2009, 2030, 23658, 1037, 13997, 2332, 1010, 2021, 2001, 3249, 2011, 1996, 4372, 6528, 2618, 1010, 2007, 2350, 6624, 2011, 1996, 5969, 28755, 1012, 2044, 16152, 2075, 1996, 2717, 1997, 5924, 2013, 1996, 12461, 1010, 5924, 2587, 1996, 10331, 1998, 2001, 11792, 1006, 8550, 21516, 1998, 2059, 2789, 19017, 1007, 1010, 2043, 3401, 1037, 2832, 1997, 2866, 21166, 3989, 2001, 5625, 1010, 2007, 15846, 2013, 4924, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 61, 'end_positions': 61}.
01/31/2025 10:46:13 - INFO - __main__ - Sample 60534 of the training set: {'input_ids': [101, 2029, 2048, 17586, 22085, 1000, 3226, 1000, 2007, 1996, 2110, 1997, 3267, 1029, 102, 5487, 7779, 22085, 1000, 3226, 1000, 2007, 26395, 1025, 2060, 13481, 1010, 2206, 17586, 2726, 7570, 19473, 2015, 1998, 3744, 1011, 7445, 26868, 1010, 22085, 1000, 3226, 1000, 2007, 1000, 1996, 2110, 1997, 3267, 1000, 1012, 2429, 2000, 7570, 19473, 2015, 1998, 26868, 1010, 1996, 3128, 4841, 2040, 2020, 2108, 11438, 2011, 13481, 2013, 1996, 5767, 4693, 2006, 2020, 2542, 1999, 1037, 2110, 1997, 3267, 1025, 2023, 4559, 2001, 5228, 2083, 1996, 5688, 2090, 1000, 25825, 1000, 1998, 1000, 4895, 6895, 21661, 5422, 1012, 1000, 2429, 2000, 2023, 2126, 1997, 3241, 1010, 2028, 2071, 26268, 2070, 3032, 1998, 3741, 2004, 2062, 25825, 2084, 2500, 1998, 2070, 2111, 2004, 2062, 3226, 2094, 2084, 2500, 1012, 2023, 5688, 2419, 2000, 7253, 7084, 1005, 1055, 3399, 1997, 2591, 11534, 2964, 1998, 4572, 2888, 5253, 1005, 1055, 3399, 1997, 3451, 6622, 1012, 2074, 2004, 2070, 4401, 2031, 5275, 2008, 1996, 7835, 2090, 2152, 1998, 2659, 8578, 2003, 2428, 2019, 3670, 1997, 1996, 4736, 2090, 2647, 27021, 1998, 2512, 1011, 27021, 1010, 2070, 4401, 2031, 5275, 2008, 1996, 7835, 2090, 25825, 1998, 4895, 6895, 21661, 5422, 2111, 2003, 2428, 2019, 3670, 1997, 1996, 4736, 2090, 2647, 5336, 4204, 1998, 2037, 5336, 5739, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 29, 'end_positions': 37}.
01/31/2025 10:46:13 - INFO - __main__ - Sample 24981 of the training set: {'input_ids': [101, 2040, 2515, 1996, 21288, 2360, 2001, 2036, 2013, 1037, 25020, 2155, 1029, 102, 2009, 2003, 10386, 2005, 1996, 7746, 3003, 2025, 2000, 2022, 2013, 1996, 2155, 1997, 7187, 1012, 1031, 11091, 2734, 1033, 2429, 2000, 4862, 2632, 1011, 9436, 3270, 1010, 2144, 2009, 2003, 26471, 2000, 15470, 2032, 1010, 2045, 2323, 2022, 1037, 3696, 2000, 4415, 5769, 1996, 7746, 3003, 1012, 2008, 3696, 2003, 2010, 2092, 1011, 2124, 7208, 1997, 27866, 2007, 7187, 1998, 2010, 3154, 6098, 2061, 2008, 1996, 2111, 2071, 10782, 2032, 2013, 2500, 1010, 1998, 2022, 4415, 8546, 2646, 2032, 1012, 4728, 2500, 2024, 7015, 2099, 2084, 7187, 1005, 1055, 13195, 1998, 2027, 2024, 2000, 2022, 2628, 1998, 22665, 1025, 1998, 1996, 13195, 1997, 7187, 2024, 15578, 10265, 3372, 1998, 3395, 2000, 1996, 13195, 1997, 7187, 1521, 1055, 6716, 2107, 2004, 11113, 2072, 14855, 7317, 2030, 7839, 11113, 2072, 5003, 1521, 25212, 2102, 1012, 1031, 2434, 2470, 1029, 1033, 2174, 1010, 7187, 2003, 2172, 7015, 2099, 2084, 2500, 2000, 2022, 1999, 3715, 1998, 2000, 2022, 22665, 1012, 9308, 1010, 2320, 1996, 12168, 9021, 1997, 7187, 2003, 14914, 2027, 2052, 15470, 2032, 1010, 2053, 2028, 2052, 16390, 2000, 3582, 2010, 13195, 1998, 2023, 2052, 2025, 2022, 2524, 2005, 3087, 1012, 2096, 2000, 3582, 1996, 13195, 1997, 1996, 27279, 2945, 2003, 3697, 1012, 1031, 2434, 2470, 1029, 1033, 1998, 2008, 2003, 2672, 2339, 1996, 3937, 8281, 1997, 7187, 1998, 2060, 23172, 2001, 2037, 11760, 1012, 1031, 2434, 2470, 1029, 1033, 2005, 3904, 1997, 2068, 1010, 2009, 2003, 2056, 1010, 2020, 7940, 2013, 1037, 29591, 2094, 2155, 1012, 1031, 11091, 2734, 1033, 2009, 2003, 3373, 2008, 2035, 7187, 1005, 1055, 10748, 2039, 2000, 4205, 2020, 2995, 7486, 1012, 1031, 1037, 1033, 1031, 11091, 2734, 1033, 4441, 2001, 2036, 2013, 1037, 25020, 2155, 1010, 2004, 2009, 2003, 3855, 1999, 21288, 2008, 2044, 2010, 4182, 1010, 2111, 2056, 2000, 2984, 1024, 1051, 2905, 1997, 7158, 1010, 2115, 2269, 2001, 2025, 1037, 2158, 1997, 4763, 1010, 4496, 2001, 2115, 2388, 4895, 7507, 13473, 1012, 1000, 1031, 1038, 1033, 1031, 24156, 10752, 1029, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 286, 'end_positions': 286}.
01/31/2025 10:46:14 - INFO - __main__ - ***** Running training *****
01/31/2025 10:46:14 - INFO - __main__ - Num examples = 88524
01/31/2025 10:46:14 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:46:14 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:46:14 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:46:14 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:46:14 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:46:15 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:46:15 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:47:13 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:47:16 - INFO - __main__ - Sample 73616 of the training set: {'input_ids': [101, 2040, 2106, 8042, 2031, 1037, 4736, 2058, 2770, 1996, 2250, 2486, 1029, 102, 1037, 2350, 3291, 1999, 1996, 6605, 1997, 1996, 17740, 2001, 12224, 2175, 4892, 1012, 8042, 3373, 1996, 17740, 2001, 1000, 1996, 2087, 4621, 6143, 5195, 1000, 1010, 1998, 1999, 7514, 2000, 5567, 11186, 2013, 1996, 1047, 7373, 5620, 21966, 2005, 2491, 2058, 2948, 7278, 1010, 1000, 2057, 2323, 2196, 2031, 2042, 2583, 2000, 2907, 2256, 2219, 1999, 2023, 2162, 2065, 2057, 2018, 2025, 2018, 2019, 6151, 12848, 14097, 17740, 1000, 1012, 2107, 6481, 2081, 2009, 2172, 6211, 2000, 17409, 1996, 2250, 2486, 2046, 1996, 3452, 5656, 1998, 2550, 1999, 2175, 4892, 1037, 9981, 1998, 15011, 4721, 1997, 2010, 1000, 3400, 1000, 2096, 9268, 8042, 17912, 2013, 1996, 11778, 3257, 1997, 1996, 17740, 2012, 2593, 1996, 6143, 2030, 6515, 2504, 1012, 2043, 8042, 2699, 2000, 18793, 2062, 1999, 1996, 2770, 1997, 1996, 2250, 2486, 2101, 1999, 1996, 2162, 1010, 2002, 2001, 4320, 2007, 1037, 2576, 4736, 1997, 2010, 2219, 2437, 2090, 2370, 1998, 2175, 4892, 1010, 2029, 2001, 2025, 3929, 10395, 2127, 1996, 2162, 2001, 2471, 2058, 1012, 1999, 3878, 1998, 3874, 1010, 2175, 4892, 1005, 1055, 13948, 2000, 17654, 2007, 1996, 1047, 7373, 5620, 21966, 6380, 1996, 2972, 23443, 2510, 2749, 1997, 1996, 14365, 1996, 3382, 2000, 2358, 21476, 2329, 2712, 4806, 1010, 2029, 2453, 2031, 2018, 6143, 2030, 13079, 3466, 1999, 1996, 2162, 2114, 1996, 2329, 3400, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 24, 'end_positions': 26}.
01/31/2025 10:47:16 - INFO - __main__ - Sample 65664 of the training set: {'input_ids': [101, 2339, 2003, 2478, 5762, 4937, 20265, 26910, 2005, 11268, 16281, 2411, 6367, 1029, 102, 1999, 2070, 3032, 1010, 2375, 7285, 3594, 2679, 2000, 6337, 13172, 1012, 2023, 2224, 1997, 5762, 7236, 2003, 4703, 6367, 2005, 2566, 22327, 24133, 2019, 2041, 5302, 5732, 4824, 1997, 2529, 6897, 8386, 1010, 1998, 7694, 22807, 1012, 2138, 1999, 2070, 8384, 5762, 19765, 2015, 17254, 4876, 2007, 7060, 1997, 2591, 2358, 8609, 9031, 1010, 2005, 2591, 6529, 5702, 2591, 16440, 1010, 2679, 2064, 2022, 1037, 3278, 8023, 1012, 2004, 24846, 5876, 1010, 5762, 7236, 2089, 1999, 2112, 8339, 20714, 2012, 18886, 29446, 2015, 1010, 2969, 1011, 15702, 1010, 1998, 2591, 4896, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 36, 'end_positions': 47}.
01/31/2025 10:47:16 - INFO - __main__ - Sample 43424 of the training set: {'input_ids': [101, 2129, 2003, 2053, 5049, 3141, 2000, 1037, 4009, 1010, 2065, 2012, 2035, 1029, 102, 1037, 4843, 2674, 2089, 2022, 4161, 1037, 2053, 5049, 2065, 1996, 3045, 3785, 2024, 4039, 2000, 5258, 1012, 2023, 2064, 2022, 2349, 2000, 11664, 11099, 1010, 3279, 1997, 5330, 1005, 1055, 2491, 2058, 1996, 2674, 1010, 2028, 2030, 2062, 6818, 22663, 2139, 14454, 16518, 4544, 2025, 3303, 2011, 1996, 7116, 1010, 2030, 1996, 13720, 1997, 1037, 5115, 2674, 2000, 2130, 4088, 1012, 1037, 2053, 5049, 2003, 1037, 2110, 3584, 1998, 5664, 2013, 1037, 4009, 1517, 1037, 4009, 7127, 3045, 3785, 2020, 2777, 1012, 2348, 1996, 3408, 2024, 2823, 2109, 8989, 8231, 1999, 3218, 1010, 2023, 8192, 2003, 10892, 16542, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 82, 'end_positions': 88}.
01/31/2025 10:47:17 - INFO - __main__ - ***** Running training *****
01/31/2025 10:47:17 - INFO - __main__ - Num examples = 88524
01/31/2025 10:47:17 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:47:17 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:47:17 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:47:17 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:47:17 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:47:18 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:47:18 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:48:00 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 10:48:02 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:48:06 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 10:48:24 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 10:48:24 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=32,32,QntzerW,A=)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (key): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (value): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=32,32,QntzerW,A=)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=32,32,QntzerW,A=)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): QLinear(768,2, Nbits_W,A=32,32,QntzerW,A=)
)

01/31/2025 10:48:24 - WARNING - fms_mo.utils.aiu_utils - Zero_shift may have overflow issue in bert.encoder.layer.2.output.dense!
01/31/2025 10:48:24 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 10:48:24 - INFO - __main__ - Num examples = 10784
01/31/2025 10:48:24 - INFO - __main__ - Batch size = 8
01/31/2025 10:50:37 - INFO - __main__ - Evaluation metrics: {'exact': 65.09933774834437, 'f1': 72.41930292315119, 'total': 10570, 'HasAns_exact': 65.09933774834437, 'HasAns_f1': 72.41930292315119, 'HasAns_total': 10570, 'best_exact': 65.09933774834437, 'best_exact_thresh': 0.0, 'best_f1': 72.41930292315119, 'best_f1_thresh': 0.0}
01/31/2025 10:53:30 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

01/31/2025 10:54:04 - INFO - __main__ - Sample 85240 of the training set: {'input_ids': [101, 2073, 2106, 2119, 7625, 1998, 7063, 5463, 3412, 2578, 1029, 102, 7625, 1998, 2508, 7063, 1005, 1055, 13120, 2015, 1997, 8745, 2031, 2146, 2042, 15268, 1012, 7625, 4188, 2000, 3277, 16413, 2015, 1997, 15060, 2741, 2000, 2032, 2011, 3519, 2076, 2010, 8798, 1010, 2295, 2002, 2106, 3277, 1037, 15060, 1998, 7083, 16413, 2004, 3099, 1997, 3448, 1012, 7063, 3843, 2176, 3412, 16413, 2015, 2096, 2343, 1010, 2021, 22102, 2098, 2048, 8236, 2006, 1996, 5286, 2027, 14424, 1996, 2034, 7450, 1012, 2006, 1996, 2060, 2192, 1010, 2119, 7625, 1998, 7063, 3230, 3412, 2578, 2012, 1996, 9424, 1012, 2086, 2077, 1996, 27369, 1997, 1996, 4552, 1010, 7063, 27481, 2098, 1000, 2138, 2065, 4676, 2022, 11819, 2013, 1996, 3691, 1997, 1996, 2554, 2012, 2312, 1010, 2145, 2625, 2064, 2009, 2022, 3395, 2000, 2008, 1997, 1996, 4884, 2303, 1012, 1000, 2044, 9150, 2013, 1996, 8798, 1010, 7063, 2626, 1997, 1000, 2561, 8745, 1997, 1996, 2277, 2013, 1996, 2110, 1012, 1000, 1000, 1000, 6118, 13802, 2004, 2003, 1996, 8745, 2090, 4676, 1004, 22410, 1999, 1996, 4552, 1997, 1996, 2142, 2163, 1010, 1000, 7063, 2626, 1010, 1998, 2002, 4161, 1010, 1000, 6742, 7835, 2090, 4676, 1998, 2942, 2231, 2003, 6827, 2000, 1996, 18433, 1997, 2119, 1010, 1998, 2004, 12361, 2011, 1996, 4552, 1997, 1996, 2142, 2163, 1012, 1000, 1999, 1037, 3661, 2000, 3487, 14109, 7063, 2582, 4423, 1010, 1000, 2057, 2024, 4252, 1996, 2088, 1996, 2307, 3606, 2008, 22410, 2015, 1012, 2079, 2488, 2302, 5465, 1004, 13969, 2084, 2007, 2068, 1012, 1996, 7857, 2097, 2022, 11515, 2011, 1996, 2060, 10800, 2008, 4676, 24299, 2229, 1999, 3618, 18433, 1010, 2302, 2084, 2007, 1996, 4681, 1997, 22410, 1012, 1000, 7063, 1005, 1055, 2434, 4433, 1997, 1996, 3021, 1997, 2916, 2018, 2443, 8910, 8031, 1996, 2163, 1010, 2004, 2092, 2004, 1996, 2976, 2231, 1010, 2013, 2019, 5069, 1997, 4676, 1010, 2021, 1996, 2160, 2106, 2025, 3413, 2068, 1012, 1031, 11091, 2734, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 93, 'end_positions': 95}.
01/31/2025 10:54:04 - INFO - __main__ - Sample 48638 of the training set: {'input_ids': [101, 2054, 2001, 1996, 2466, 2241, 2006, 1029, 102, 2104, 1037, 2392, 3931, 17653, 1000, 1996, 3606, 1000, 1010, 1996, 3259, 6267, 9989, 3024, 2000, 2068, 2008, 2070, 4599, 3856, 1996, 10306, 1997, 10560, 5694, 1010, 2008, 2500, 24471, 15833, 2006, 2372, 1997, 1996, 5057, 2578, 2004, 2027, 2699, 2000, 2393, 1998, 2008, 2070, 2130, 17536, 1037, 2610, 12294, 1000, 5819, 2002, 2001, 28965, 1996, 3610, 1997, 2166, 2000, 1037, 5776, 1012, 1000, 2750, 1996, 17653, 1010, 2517, 2011, 24810, 11407, 1010, 1996, 2466, 2001, 2241, 2006, 9989, 2593, 2011, 13294, 1998, 14477, 4779, 3089, 8569, 10880, 4216, 1010, 2030, 14994, 4710, 6115, 1997, 2054, 2315, 3633, 2018, 2056, 1516, 1037, 2755, 2081, 3154, 2000, 11407, 2011, 4302, 7779, 1010, 1996, 6398, 2040, 2626, 1996, 2466, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 87, 'end_positions': 108}.
01/31/2025 10:54:04 - INFO - __main__ - Sample 26391 of the training set: {'input_ids': [101, 1996, 9187, 1997, 1996, 3700, 6354, 3053, 2129, 2146, 1029, 102, 24536, 26619, 3540, 3148, 1006, 2047, 20377, 3540, 2100, 1007, 2001, 1996, 2034, 2874, 1997, 2642, 2047, 3577, 2000, 2022, 10641, 1998, 3876, 2011, 1996, 3009, 1012, 2105, 15017, 2620, 1010, 1037, 2177, 1997, 8487, 14619, 19264, 1010, 2419, 2011, 2632, 10755, 28454, 9298, 28640, 2139, 12436, 3540, 1010, 2034, 3133, 1996, 5025, 3700, 1997, 2054, 2003, 2085, 28480, 1012, 1996, 9187, 1997, 1996, 3700, 6354, 3053, 2028, 2301, 1010, 1998, 8567, 9205, 5012, 2013, 1996, 9530, 9905, 2015, 5917, 1010, 2021, 1996, 4792, 1997, 1996, 3009, 4410, 2000, 10938, 1996, 2555, 2046, 1037, 13950, 2989, 5471, 2415, 2419, 2000, 1037, 2844, 5656, 2000, 2491, 1996, 2181, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 79, 'end_positions': 80}.
01/31/2025 10:54:05 - INFO - __main__ - ***** Running training *****
01/31/2025 10:54:05 - INFO - __main__ - Num examples = 88524
01/31/2025 10:54:05 - INFO - __main__ - Num Epochs = 3
01/31/2025 10:54:05 - INFO - __main__ - Instantaneous batch size per device = 8
01/31/2025 10:54:05 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 8
01/31/2025 10:54:05 - INFO - __main__ - Gradient Accumulation steps = 1
01/31/2025 10:54:05 - INFO - __main__ - Total optimization steps = 33198
01/31/2025 10:54:06 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda - CUDA extension not installed.
01/31/2025 10:54:06 - WARNING - auto_gptq.nn_modules.qlinear.qlinear_cuda_old - CUDA extension not installed.
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - A qcfg dict is provided thru args but the ckpt folder also has a qcfg.json. Will use the json's value if both exist!!
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_w] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[nbits_a] = 32 (user provided) and 8 (loaded from file.)
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[qmodel_calibration] = 0 (user provided) and 10 (loaded from file.)
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[clip_val_asst_percentile] = (0.1, 99.9) (user provided) and [0.1, 99.9] (loaded from file.)
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[qskip_layer_name] = [] (user provided) and ['qa_outputs', 'bert.embeddings.word_embeddings', 'bert.embeddings.token_type_embeddings'] (loaded from file.)
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[params2optim] = {'W': [[]], 'cvs': [[]]} (user provided) and {'W': [[], [], [], [], [], [], [], []], 'cvs': [[], [], [], [], [], [], [], []]} (loaded from file.)
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - qcfg[world_size] = 1 (user provided) and 8 (loaded from file.)
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - Missing keys in qcfg file and user provided qcfg {'mapping', 'force_calib_once', 'tb_writer'}
01/31/2025 10:54:06 - WARNING - fms_mo.utils.aiu_utils - qcfg['qmodel_calibration_new'] was not 0 and has been set to 0 now. We do not want to run calibration during verification!
01/31/2025 10:54:16 - INFO - fms_mo.utils.aiu_utils - Mapping for aten_node_name to original_module_name is done!
01/31/2025 10:54:18 - INFO - fms_mo.prep - --- Before model quantization --- 
 BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:54:22 - WARNING - fms_mo.fx.dynamo_utils - No bmm and matmul are found. Likely SDPA is enabled. Will patch nothing.
01/31/2025 10:54:40 - INFO - fms_mo.prep - Please provide a valid quantized checkpoint or run calibration for best results.
01/31/2025 10:54:40 - INFO - fms_mo.prep - --- Quantized model --- 
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-10): 11 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(3072,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (key): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (value): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(768,768, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(768,3072, Nbits_W,A=8,8,QntzerW,A=SAWB,PACT2)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)

01/31/2025 10:54:41 - INFO - __main__ - ***** Running Evaluation *****
01/31/2025 10:54:41 - INFO - __main__ - Num examples = 10784
01/31/2025 10:54:41 - INFO - __main__ - Batch size = 8
01/31/2025 10:56:36 - INFO - __main__ - Evaluation metrics: {'exact': 72.42194891201514, 'f1': 78.80925715238682, 'total': 10570, 'HasAns_exact': 72.42194891201514, 'HasAns_f1': 78.80925715238682, 'HasAns_total': 10570, 'best_exact': 72.42194891201514, 'best_exact_thresh': 0.0, 'best_f1': 78.80925715238682, 'best_f1_thresh': 0.0}
